{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This is the documentation website for the LGSVL Simulator. You can subscribe to our email newsletter here . Visit our website here: https://www.lgsvlsimulator.com Visit our Github here: https://github.com/lgsvl/simulator Quick Start # Getting Started Keyboard Shortcuts Release notes User Interface Maps Vehicles Sensor Parameters Clusters Simulations Configuration file and command line parameters Integration with AD # Running with Autoware.AI Instructions Sample sensor configuration Running with Autoware.Auto Instructions Sample sensor configuration Running with Apollo 5.0 Instructions Sample sensor configuration Running with Apollo 3.0 Instructions Sample sensor configuration Ground truth obstacles Viewing and subscribing to ground truth data Sample sensor configuration for data collection Python API # Python API Guide Python API Quickstart Examples Python API Use Case Examples How to run a scenario Tutorials # Reinforcement Learning with OpenAI Gym Deep Learning Lane Following Model How to create a simple ROS2-based AD stack with LGSVL Simulator Advanced # Map Annotation Build Instructions Adding Assets How to add a new ego vehicle NPC Map Navigation Sensor Plugins Controllable Plugins Support # Frequently Asked Questions Contributing Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Home"},{"location":"#quick-start","text":"Getting Started Keyboard Shortcuts Release notes User Interface Maps Vehicles Sensor Parameters Clusters Simulations Configuration file and command line parameters","title":"Quick Start"},{"location":"#integration-with-ad","text":"Running with Autoware.AI Instructions Sample sensor configuration Running with Autoware.Auto Instructions Sample sensor configuration Running with Apollo 5.0 Instructions Sample sensor configuration Running with Apollo 3.0 Instructions Sample sensor configuration Ground truth obstacles Viewing and subscribing to ground truth data Sample sensor configuration for data collection","title":"Integration with AD"},{"location":"#python-api","text":"Python API Guide Python API Quickstart Examples Python API Use Case Examples How to run a scenario","title":"Python API"},{"location":"#tutorials","text":"Reinforcement Learning with OpenAI Gym Deep Learning Lane Following Model How to create a simple ROS2-based AD stack with LGSVL Simulator","title":"Tutorials"},{"location":"#advanced","text":"Map Annotation Build Instructions Adding Assets How to add a new ego vehicle NPC Map Navigation Sensor Plugins Controllable Plugins","title":"Advanced"},{"location":"#support","text":"Frequently Asked Questions Contributing","title":"Support"},{"location":"#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"add-new-ego-vehicle/","text":"How to Add a New Ego Vehicle This tutorial works with Simulator Release 2019.05 This document will describe how to create a new ego vehicle in the LGSVL Simulator. Video # ( Link ) Getting Started # The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive . Create and Name Your Vehicle # Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later. Add Child Components # Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab. Apply the Correct References # We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references. Vehicle Controller script # For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center Car HeadLights script # For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot Car Input Controller script # For the Car Input Controller script, reference the DriverCamera. Force Feedback script # For the Force Feedback script, reference the FL and FR WheelColliders. Vehicle Animation Manager script # For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator) Vehicle Position Resetter script # For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray. Agent Setup script # For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes Final Steps # Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle! Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"How to add a new ego vehicle"},{"location":"add-new-ego-vehicle/#video","text":"( Link )","title":"Video"},{"location":"add-new-ego-vehicle/#getting-started","text":"The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive .","title":"Getting Started"},{"location":"add-new-ego-vehicle/#create-and-name-your-vehicle","text":"Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later.","title":"Create and Name Your Vehicle"},{"location":"add-new-ego-vehicle/#add-child-components","text":"Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab.","title":"Add Child Components"},{"location":"add-new-ego-vehicle/#apply-the-correct-references","text":"We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references.","title":"Apply the Correct References"},{"location":"add-new-ego-vehicle/#vehicle-controller-script","text":"For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center","title":"Vehicle Controller script"},{"location":"add-new-ego-vehicle/#car-headlights-script","text":"For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot","title":"Car HeadLights script"},{"location":"add-new-ego-vehicle/#car-input-controller-script","text":"For the Car Input Controller script, reference the DriverCamera.","title":"Car Input Controller script"},{"location":"add-new-ego-vehicle/#force-feedback-script","text":"For the Force Feedback script, reference the FL and FR WheelColliders.","title":"Force Feedback script"},{"location":"add-new-ego-vehicle/#vehicle-animation-manager-script","text":"For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator)","title":"Vehicle Animation Manager script"},{"location":"add-new-ego-vehicle/#vehicle-position-resetter-script","text":"For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray.","title":"Vehicle Position Resetter script"},{"location":"add-new-ego-vehicle/#agent-setup-script","text":"For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes","title":"Agent Setup script"},{"location":"add-new-ego-vehicle/#final-steps","text":"Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle!","title":"Final Steps"},{"location":"add-new-ego-vehicle/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"api-example-descriptions/","text":"Python API Use Case Examples The LGSVL Simulator teams has created sample Python scripts that use the LGSVL Simulator Python API to test specific scenarios or perform certain tasks. These example scripts can be found on our Github here . Please contact us if you would like to contribute examples that you are using, or submit a pull request . Scenarios top # We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started. Vehicle Following top # Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane. Encroaching Oncoming Vehicle top # Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts. Other Uses top # Collecting data in KITTI format top # Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip Automated Driving System Test Cases top # The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Python API Use Case Examples"},{"location":"api-example-descriptions/#scenarios","text":"We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started.","title":"Scenarios"},{"location":"api-example-descriptions/#vehicle-following","text":"Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane.","title":"Vehicle Following"},{"location":"api-example-descriptions/#encroaching-oncoming-vehicle","text":"Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts.","title":"Encroaching Oncoming Vehicle"},{"location":"api-example-descriptions/#other-uses","text":"","title":"Other Uses"},{"location":"api-example-descriptions/#collecting-data-in-kitti-format","text":"Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip","title":"Collecting data in KITTI format"},{"location":"api-example-descriptions/#automated-driving-system-test-cases","text":"The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Automated Driving System Test Cases"},{"location":"api-how-to-run-scenario/","text":"How To Run a Scenario or Test Case The following steps detail how to run the Vehicle Following scenario. This scenario and other example scenarios can be found on our examples page . Install Simulator Python API by navigating to the Api submodule directory under the simulator repository: pip3 install --user . Start the simulator. Set environment variables SIMULATOR_HOST and BRIDGE_HOST SIMULATOR_HOST is where the simulator will be run. The default value for this is \"localhost\" and does not need to be set if the simulator is running on the same machine that the python script will be run from. BRIDGE_HOST is where the AD stack will be run. This is relative to where the simulator is run. The default value is \"localhost\" which is the same machine as the simulator. For example, if computer A will run the simulator and computer B will run the AD stack SIMULATOR_HOST should be set to the IP of computer A BRIDGE_HOST should be set to the IP of computer B To set the variables for the current terminal window use export SIMULATOR_HOST=192.168.1.100 Start your AD stack. The example scripts are written for Apollo 5.0. See below for how to edit the scripts to work with other AD stacks. Select the MKZ as the vehicle SingleLaneRoad for the map Start all modules and the bridge (if relevant) Run the script ./VF_S_25.py Set the destination for the AD stack. For this scenario, the destination is the end of the current lane. The AV should start driving forward towards the NPC. It should avoid crashing into the NPC. How to Edit the EGO vehicle # In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to Run a Scenario"},{"location":"api-how-to-run-scenario/#how-to-edit-the-ego-vehicle","text":"In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to Edit the EGO vehicle"},{"location":"api-quickstart-descriptions/","text":"Python API Quickstart Script Descriptions This document describes the example Python scripts that use the LGSVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LIDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints with fixed wait times and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the npc is printed 24-ego-drive-straight-non-realtime.py : How to run the simulator at non-realtime. 25-waypoint-flying-npc.py : How to use waypoints to define customized motion for npc. 26-npc-trigger-waypoints.py : How to use trigger waypoints that pause npc motion until an ego vehicle approaches. 27-control-traffic-lights.py : How to get and set the control policy of a controllable object (e.g., changing a traffic light signal) 28-control-traffic-cone.py : How to add and move a controllable object (e.g. a traffic cone) 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API Quickstart Examples"},{"location":"apollo-instructions/","text":"Running Apollo 3.0 with LGSVL Simulator This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and ROSbridge Launching Apollo alongside the simulator Adding a Vehicle Adding an HD Map Copyright and License Getting Started top # The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Prerequisites top # Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing Nvidia Docker top Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image top LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo Cloning the Repository top # This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git Building Apollo and ROSbridge top # Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make Launching Apollo alongside the simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap.sh Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Create a Simulation with the BorregasAve map and the Jaguar2015XE (Apollo 3.0) vehicle. Enter localhost:9090 for the Bridge Connection String Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Jaguar2015XE vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # The default vehicles have their calibration files included in the LGSVL Branch of Apollo 3.0 . Adding an HD Map top # The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 . Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"apollo-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle.","title":"Getting Started"},{"location":"apollo-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo-instructions/#setup","text":"","title":"Setup"},{"location":"apollo-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Cloning the Repository"},{"location":"apollo-instructions/#building-apollo-and-rosbridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make","title":"Building Apollo and ROSbridge"},{"location":"apollo-instructions/#launching-apollo-alongside-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap.sh Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Create a Simulation with the BorregasAve map and the Jaguar2015XE (Apollo 3.0) vehicle. Enter localhost:9090 for the Bridge Connection String Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Jaguar2015XE vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongside the simulator"},{"location":"apollo-instructions/#adding-a-vehicle","text":"The default vehicles have their calibration files included in the LGSVL Branch of Apollo 3.0 .","title":"Adding a Vehicle"},{"location":"apollo-instructions/#adding-an-hd-map","text":"The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding an HD Map"},{"location":"apollo-instructions/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"apollo-json-example/","text":"Example JSON Configuration for an Apollo 3.0 Vehicle Bridge Type top # ROS Apollo Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 Lidar /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Sample sensor configuration"},{"location":"apollo-json-example/#bridge-type","text":"ROS Apollo","title":"Bridge Type"},{"location":"apollo-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 Lidar /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera","title":"Published Topics"},{"location":"apollo-json-example/#subscribed-topcs","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"apollo-json-example/#complete-json-configuration","text":"[ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"apollo5-0-instructions/","text":"Running Apollo 5.0 with LGSVL Simulator This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and bridge Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Copyright and License Getting Started top # The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . Prerequisites top # Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing Nvidia Docker top Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image top LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo-5.0 Cloning the Repository top # This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git Building Apollo and bridge top # Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. bootstrap.sh Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # The default vehicles have their calibration files included in the LGSVL Branch of Apollo 5.0 . Adding an HD Map top # The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 . Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"apollo5-0-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here .","title":"Getting Started"},{"location":"apollo5-0-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo5-0-instructions/#setup","text":"","title":"Setup"},{"location":"apollo5-0-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo5-0-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git","title":"Cloning the Repository"},{"location":"apollo5-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu","title":"Building Apollo and bridge"},{"location":"apollo5-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. bootstrap.sh Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"apollo5-0-instructions/#adding-a-vehicle","text":"The default vehicles have their calibration files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding a Vehicle"},{"location":"apollo5-0-instructions/#adding-an-hd-map","text":"The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding an HD Map"},{"location":"apollo5-0-instructions/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"apollo5-0-json-example/","text":"Example JSON Configuration for an Apollo 5.0 Vehicle Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Sample sensor configuration"},{"location":"apollo5-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"apollo5-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera","title":"Published Topics"},{"location":"apollo5-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"apollo5-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"assets/","text":"Adding Assets The main repository for the LGSVL Simulator does not contain and environments or vehicles. Currently there are several open-source examples. Environments: CubeTown SingleLaneRoad Shalun SanFrancisco Vehicles: Jaguar2015XE Table of Contents Adding an Asset Building an Asset Check Asset Consistency Adding an Asset top # Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab Building an Asset top # Assets are built using the same build script as the simulator. Follow the build instructions through step 17. IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles Check Asset Consistency top # There is a tool to check if there are any inconsistencies in assets. Run Check... in Unity : Simulator -> Check... The script checks if the project structure is correct: assets are named correctly, assets are in the correct location, etc. This will generate a list of warnings and errors.","title":"Adding Assets"},{"location":"assets/#adding-an-asset","text":"Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab","title":"Adding an Asset"},{"location":"assets/#building-an-asset","text":"Assets are built using the same build script as the simulator. Follow the build instructions through step 17. IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles","title":"Building an Asset"},{"location":"assets/#check-asset-consistency","text":"There is a tool to check if there are any inconsistencies in assets. Run Check... in Unity : Simulator -> Check... The script checks if the project structure is correct: assets are named correctly, assets are in the correct location, etc. This will generate a list of warnings and errors.","title":"Check Asset Consistency"},{"location":"autoware-auto-instructions/","text":"Autoware.Auto with LGSVL Simulator Table of Contents Overview Setup Requirements Installing Autoware.auto Simulator Installation Install Ros2 dashing Install Ros2 Web Bridge Run Simulator alongside Autoware.Auto Copyright and License Overview top # This guide describes setting up and using Autoware.Auto with the LGSVL simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented. Setup top # Requirements top # Linux operating system Nvidia graphics card Installing Docker CE top To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing Nvidia Docker top Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Install nvidia docker . Note: For docker 19.03 and newer nvidia GPUs are natively supported as devices in docker runtime, and nvidia-docker2 is deprecated, however, because Autoware.auto uses the --runtime nvidia argument nvidia-docker2 will need to be installed even for newer docker versions. Installing Autoware.auto top # Follow the installation and development setup guide for Autoware.auto. Simulator installation top # Download and extract the latest simulator release (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user . Install ROS2 dashing top # Follow these steps . Install the ROS2 Web Bridge top # Clone the ROS2 web bridge cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge ade enter git clone -b 0.2.7 https://github.com/RobotWebTools/ros2-web-bridge.git Install nodejs v10 curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - sudo apt-get install -y nodejs cd ros2-web-bridge npm install # If node.js packages are not installed, run this. Run Simulator alongside Autoware.Auto top # The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers: cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build # If you want to use autoware_auto_msgs, ros2-web-bridge needs compiled them. export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64/ source ~/AutowareAuto/install/local_setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the LGSVL Simulator by launching the executable and click on the button to open the web UI. In the Vehicles tab look for Lexus2016RXHybrid . If not available download it from here and follow these instructions to add it. Click on the wrench icon for the Lexus vehicle: Change the bridge type to ROS2 Use the following JSON configuration Autoware Auto JSON Example Switch to the Simulations tab and click the Add new button: Enter a name and switch to the Map & Vehicles tab Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu. In the bridge connection box to the right enter the bridge address (default: localhost:9090 ) Click submit Select the simulation and press the play button in the bottom right corner of the screen Launch ROS2 web bridge in a new terminal: NOTE Node.js will need to be reinstalled in the container every time it is started ade enter # ros2 web bridge should be run in ade environment. cd ros2-web-bridge source ~/AutowareAuto/install/local_setup.bash node bin/rosbridge.js You should now be able to see the lidar point cloud in rviz (see image below). If the pointcloud is not visible make sure the fixed frame is set to velodyne_front and that a PointCloud2 message is added which listens on the /points_raw topic. Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"autoware-auto-instructions/#general","text":"This guide describes setting up and using Autoware.Auto with the LGSVL simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented.","title":"Overview"},{"location":"autoware-auto-instructions/#setup","text":"","title":"Setup"},{"location":"autoware-auto-instructions/#requirements","text":"Linux operating system Nvidia graphics card","title":"Requirements"},{"location":"autoware-auto-instructions/#installing-autoware-auto","text":"Follow the installation and development setup guide for Autoware.auto.","title":"Installing Autoware.auto"},{"location":"autoware-auto-instructions/#simulator-installation","text":"Download and extract the latest simulator release (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user .","title":"Simulator Installation"},{"location":"autoware-auto-instructions/#install-ros2-dashing","text":"Follow these steps .","title":"Install Ros2 dashing"},{"location":"autoware-auto-instructions/#install-ros2-web-bridge","text":"","title":"Install Ros2 Web Bridge"},{"location":"autoware-auto-instructions/#run-simulator-alongside-autoware-auto","text":"The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers: cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build # If you want to use autoware_auto_msgs, ros2-web-bridge needs compiled them. export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64/ source ~/AutowareAuto/install/local_setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the LGSVL Simulator by launching the executable and click on the button to open the web UI. In the Vehicles tab look for Lexus2016RXHybrid . If not available download it from here and follow these instructions to add it. Click on the wrench icon for the Lexus vehicle: Change the bridge type to ROS2 Use the following JSON configuration Autoware Auto JSON Example Switch to the Simulations tab and click the Add new button: Enter a name and switch to the Map & Vehicles tab Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu. In the bridge connection box to the right enter the bridge address (default: localhost:9090 ) Click submit Select the simulation and press the play button in the bottom right corner of the screen Launch ROS2 web bridge in a new terminal: NOTE Node.js will need to be reinstalled in the container every time it is started ade enter # ros2 web bridge should be run in ade environment. cd ros2-web-bridge source ~/AutowareAuto/install/local_setup.bash node bin/rosbridge.js You should now be able to see the lidar point cloud in rviz (see image below). If the pointcloud is not visible make sure the fixed frame is set to velodyne_front and that a PointCloud2 message is added which listens on the /points_raw topic.","title":"Run Simulator alongside Autoware.Auto"},{"location":"autoware-auto-instructions/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"autoware-auto-json-example/","text":"Example JSON Configuration for an Autoware Auto Vehicle Bridge Type top # ROS2 Published Topics top # Topic Sensor Name /autoware_auto_msgs/VehicleStateReport CAN Bus /nmea_sentence GPS /autoware_auto_msgs/VehicleOdometry GPS Odometry /imu_raw IMU /points_raw LidarFront /points_raw_rear LidarRear /simulator/camera_node/image/compressed Main Camera Subscribed Topics top # Topic Sensor Name /autoware_auto_msgs/RawControlCommand Autoware Car Control /autoware_auto_msgs/VehicleStateCommand Autoware Auto Vehicle State Complete JSON Configuration top # [ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/vehicle_state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/vehicle_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne_front\" }, \"transform\": { \"x\": 0, \"y\": 1.8, \"z\": 0.02, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw_rear\", \"Frame\": \"velodyne_rear\" }, \"transform\": { \"x\": 0, \"y\": 1.8, \"z\": -1.17, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_control_command\" } }, { \"type\": \"Vehicle State\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/vehicle_state_command\" } } ]","title":"Sample sensor configuration"},{"location":"autoware-auto-json-example/#bridge-type","text":"ROS2","title":"Bridge Type"},{"location":"autoware-auto-json-example/#published-topics","text":"Topic Sensor Name /autoware_auto_msgs/VehicleStateReport CAN Bus /nmea_sentence GPS /autoware_auto_msgs/VehicleOdometry GPS Odometry /imu_raw IMU /points_raw LidarFront /points_raw_rear LidarRear /simulator/camera_node/image/compressed Main Camera","title":"Published Topics"},{"location":"autoware-auto-json-example/#subscribed-topics","text":"Topic Sensor Name /autoware_auto_msgs/RawControlCommand Autoware Car Control /autoware_auto_msgs/VehicleStateCommand Autoware Auto Vehicle State","title":"Subscribed Topics"},{"location":"autoware-auto-json-example/#complete-json-configuration","text":"[ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/vehicle_state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/vehicle_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne_front\" }, \"transform\": { \"x\": 0, \"y\": 1.8, \"z\": 0.02, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw_rear\", \"Frame\": \"velodyne_rear\" }, \"transform\": { \"x\": 0, \"y\": 1.8, \"z\": -1.17, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_control_command\" } }, { \"type\": \"Vehicle State\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/vehicle_state_command\" } } ]","title":"Complete JSON Configuration"},{"location":"autoware-instructions/","text":"Autoware.AI 1.12.0 with LGSVL Simulator The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents General Setup Requirements Simulator Installation Launching Autoware alongside LGSVL Simulator Driving by following vector map: Driving by following prerecorded waypoints: Adding a Vehicle Adding an HD Map Copyright and License General top # This guide goes through how to run Autoware.AI with the LG SVL Simulator. In order to run Autoware with the LGSVL simulator, it is easiest to pull an official Autoware docker image (see official guide ), but it is also possible to build autoware from source . Autoware communicates with the simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official autoware docker containers have rosbridge_suite included. Setup top # Requirements top # Linux operating system Nvidia graphics card Installing Docker CE top To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing Nvidia Docker top Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Simulator installation top # Follow the instructions on our simulator Github page here . Launching Autoware alongside LGSVL Simulator top # Before launching, you need to create a directory called shared_dir in the home directory to hold maps and launch files for the simulator. The autoware docker container will mount this folder: mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git To launch Autoware, first bring up the Docker container following these steps (see official guide for more details): Clone the docker repository from autoware.ai : git clone https://gitlab.com/autowarefoundation/autoware.ai/docker.git Navigate to: cd docker/generic NOTE With the latest Docker and Nvidia-docker versions, the docker option --runtime=nvidia has been deprecated. If you have Docker CE version 19.03 and Nvidia-docker release v2.2.2 please run the following to check if docker containers will have access to your GPU. In a terminal run type nvidia-docker . If you get the ouput similar to this: nvidia-docker is /usr/bin/nvidia-docker , the run script will work fine. If you get the following output bash: type: nvidia-docker: not found , you need to modify the run script as shown below. In run.sh find the following at line 139: if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" RUNTIME=\"--runtime=nvidia\" fi Replace them with: DOCKER_VERSION=$(docker version --format '{{.Client.Version}}' | cut -d'.' -f1) if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" if [[ $DOCKER_VERSION -ge \"19\" ]] && ! type nvidia-docker; then RUNTIME=\"--gpus all\" else RUNTIME=\"--runtime=nvidia\" fi fi Pull the image and run (for release 1.12.0): ./run.sh -t 1.12.0 Once inside the container, launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the LG SVL simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an inital pose or for localization but the current Autoware release (1.12.0) does not support GNSS coordinates outside of Japan. Fix for this is available in following pull requests: utilities#27 , common#20 , core_perception#26 These are not yet merged in Autoware master. Driving by following vector map: # To drive following the HD map follow these steps: - in rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. To follow the selected route launch these nodes: - Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. - Enable astar_avoid and velocity_set . - Enable pure_pursuit and twist_filter to start driving. Driving by following prerecorded waypoints: # A basic functionality of Autoware is to follow a prerecorded map while obeying traffic rules. To do this you will need to record a route first. Switch to the Computing tab and check the box for waypoint_saver . Make sure to select an appropriate location and file name by clicking on the app button. Now you can drive around the map using the keyboard. Once you are satisfied with your route, uncheck the box for waypoint_saver to end the route. To drive the route using autoware: Enable waypoint_loader while making sure the correct route file is selected in the app settings. Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. Enable astar_avoid and velocity_set . Enable pure_pursuit and twist_filter to start driving. The ego vehicle should try to follow the waypoints at the velocity which they were originally recorded at. You can modify this velocity by manually editing the values csv file. Adding a Vehicle top # The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository. Adding an HD Map top # The default maps have the Vector map files included in the LGSVL Autoware Data Github repository. Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"autoware-instructions/#general","text":"This guide goes through how to run Autoware.AI with the LG SVL Simulator. In order to run Autoware with the LGSVL simulator, it is easiest to pull an official Autoware docker image (see official guide ), but it is also possible to build autoware from source . Autoware communicates with the simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official autoware docker containers have rosbridge_suite included.","title":"General"},{"location":"autoware-instructions/#setup","text":"","title":"Setup"},{"location":"autoware-instructions/#requirements","text":"Linux operating system Nvidia graphics card","title":"Requirements"},{"location":"autoware-instructions/#simulator-installation","text":"Follow the instructions on our simulator Github page here .","title":"Simulator Installation"},{"location":"autoware-instructions/#launching-autoware-alongside-lgsvl-simulator","text":"Before launching, you need to create a directory called shared_dir in the home directory to hold maps and launch files for the simulator. The autoware docker container will mount this folder: mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git To launch Autoware, first bring up the Docker container following these steps (see official guide for more details): Clone the docker repository from autoware.ai : git clone https://gitlab.com/autowarefoundation/autoware.ai/docker.git Navigate to: cd docker/generic NOTE With the latest Docker and Nvidia-docker versions, the docker option --runtime=nvidia has been deprecated. If you have Docker CE version 19.03 and Nvidia-docker release v2.2.2 please run the following to check if docker containers will have access to your GPU. In a terminal run type nvidia-docker . If you get the ouput similar to this: nvidia-docker is /usr/bin/nvidia-docker , the run script will work fine. If you get the following output bash: type: nvidia-docker: not found , you need to modify the run script as shown below. In run.sh find the following at line 139: if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" RUNTIME=\"--runtime=nvidia\" fi Replace them with: DOCKER_VERSION=$(docker version --format '{{.Client.Version}}' | cut -d'.' -f1) if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" if [[ $DOCKER_VERSION -ge \"19\" ]] && ! type nvidia-docker; then RUNTIME=\"--gpus all\" else RUNTIME=\"--runtime=nvidia\" fi fi Pull the image and run (for release 1.12.0): ./run.sh -t 1.12.0 Once inside the container, launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the LG SVL simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an inital pose or for localization but the current Autoware release (1.12.0) does not support GNSS coordinates outside of Japan. Fix for this is available in following pull requests: utilities#27 , common#20 , core_perception#26 These are not yet merged in Autoware master.","title":"Launching Autoware alongside LGSVL Simulator"},{"location":"autoware-instructions/#driving-by-following-vector-map","text":"To drive following the HD map follow these steps: - in rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. To follow the selected route launch these nodes: - Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. - Enable astar_avoid and velocity_set . - Enable pure_pursuit and twist_filter to start driving.","title":"Driving by following vector map:"},{"location":"autoware-instructions/#driving-by-following-prerecorded-waypoints","text":"A basic functionality of Autoware is to follow a prerecorded map while obeying traffic rules. To do this you will need to record a route first. Switch to the Computing tab and check the box for waypoint_saver . Make sure to select an appropriate location and file name by clicking on the app button. Now you can drive around the map using the keyboard. Once you are satisfied with your route, uncheck the box for waypoint_saver to end the route. To drive the route using autoware: Enable waypoint_loader while making sure the correct route file is selected in the app settings. Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. Enable astar_avoid and velocity_set . Enable pure_pursuit and twist_filter to start driving. The ego vehicle should try to follow the waypoints at the velocity which they were originally recorded at. You can modify this velocity by manually editing the values csv file.","title":"Driving by following prerecorded waypoints:"},{"location":"autoware-instructions/#adding-a-vehicle","text":"The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository.","title":"Adding a Vehicle"},{"location":"autoware-instructions/#adding-an-hd-map","text":"The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"Adding an HD Map"},{"location":"autoware-instructions/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"autoware-json-example/","text":"Example JSON Configuration for an Autoware Vehicle Bridge Type top # ROS Published Topics top # Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera Subscribed Topics top # Topic Sensor Name /vehicle_cmd Autoware Car Control Complete JSON Configuration top # [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Sample sensor configuration"},{"location":"autoware-json-example/#bridge-type","text":"ROS","title":"Bridge Type"},{"location":"autoware-json-example/#published-topics","text":"Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera","title":"Published Topics"},{"location":"autoware-json-example/#subscribed-topics","text":"Topic Sensor Name /vehicle_cmd Autoware Car Control","title":"Subscribed Topics"},{"location":"autoware-json-example/#complete-json-configuration","text":"[ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Complete JSON Configuration"},{"location":"bridge-connection-ui/","text":"Bridge Connection UI When in a non-Headless Simulation, a list of published and subscribed topics can be found in the Simulator menu (plug icon). At the top of the menu is the selected vehicle. The bridge status can be: Disconnected , Connecting , or Connected The bridge address is the same that was entered as the Bridge Connection String when creating the Simulation. Each topic is then listed in the following format: PUB or SUB : indicates if the Simulator publishes or subscribes to messages on this topic Topic : is the topic that the messages are published/subscribed to Type : is the message type on this topic Count : is the total number of messages published/received when the bridge was connected","title":"Bridge Topics"},{"location":"build-instructions/","text":"Instructions to build standalone executable Download and Install Unity Hub: Ubuntu: https://forum.unity.com/threads/unity-hub-v2-0-0-release.677485/ Windows: https://unity3d.com/get-unity/download Download and Install Unity 2019.1.10f1: IMPORTANT include support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Ubuntu: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 Windows: https://unity3d.com/get-unity/download/archive Download and Install Node.js Version 12.13.0 LTS is fine Make sure you have git-lfs installed before cloning this repository . Instructions for installation are here Verify installation with: git lfs install > Git LFS initialized. Clone simulator from GitHub: git clone --single-branch https://github.com/lgsvl/simulator.git Run Unity Hub In the Projects tab, click Add and select the folder that the Simulator was cloned to In the Installs tab, click Locate and choose the Unity launcher in the Unity2019.1.10f1 folder In the Projects tab, verify that the Simulator is using Unity Version 2019.1.10f1 from the dropdown Double-click the name of the project to launch Unity Editor Open a terminal window cmd.exe on Windows Terminal on Linux Navigate to the WebUI folder of the Simulator project Window ex. C:\\Users\\XXX\\Documents\\Simulator\\WebUI Linux ex. /home/XXX/Projects/Simulator/WebUI Where XXX is the user profile Run npm install to install dependencies, do this only once or if dependencies change inside packages.json file Run Build WebUi... in Unity : Simulator -> Build WebUI... Open Build... in Unity : Simulator -> Build... Check the Environments and Vehicles that should be generated as AssetBundles See assets documentation for information on how to add Environments and Vehicles They will be located in a folder called AssetBundles in the folder selected as the build location These may also be built separately from the Simulator. In this case they will be put into the AssetBundles folder of the project (Optional) Click Build to only build the assetbundles. Load the LoaderScene.unity and click the Play button at the top of the editor to start the simulator. Select the Target OS for the build This is only used when building the Simulator. Assetbundles are built for Linux and Windows automatically Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build Click Build NOTE You will get an error when building assetbundles if either Windows or Linux support is not installed in Unity Test Simulator top # Ubuntu - Install Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Select graphics options then press Ok Click Open Browser In the Maps tab, Add new map with the URL to an environment assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\environment_borregasave In the Vehicles tab, Add new vehicle with the URL to a vehicle assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\vehicle_jaguar2015xe (Optional) Add a manual control \"sensor\" to the vehicle to enable driving Click the wrench icon next to the vehicle name In the text box insert [{\"type\": \"Manual Control\", \"name\": \"Manual Car Control\"}] In the Simulations tab, Add new simulation with the added map and vehicle Press the Play button The Unity window should now show a vehicle in the built environment","title":"Build instructions"},{"location":"build-instructions/#test-simulator","text":"Ubuntu - Install Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Select graphics options then press Ok Click Open Browser In the Maps tab, Add new map with the URL to an environment assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\environment_borregasave In the Vehicles tab, Add new vehicle with the URL to a vehicle assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\vehicle_jaguar2015xe (Optional) Add a manual control \"sensor\" to the vehicle to enable driving Click the wrench icon next to the vehicle name In the text box insert [{\"type\": \"Manual Control\", \"name\": \"Manual Car Control\"}] In the Simulations tab, Add new simulation with the added map and vehicle Press the Play button The Unity window should now show a vehicle in the built environment","title":"Test Simulator"},{"location":"changelog/","text":"Changelog All notable changes and release notes for LGSVL Simulator will be documented in this file. [2020.01] - 2020-01-31 # Added # Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LIDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto Changed # Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS [2019.12] - 2020-01-21 # Added # Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map. Changed # Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh. [2019.11] - 2019-11-19 # Added # OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins. Changed # Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps. [2019.10] - 2019-10-28 # Added # Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor. Changed # Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start. [2019.09] - 2019-09-06 # Added # Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle Changed # Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations [2019.07] - 2019-08-09 # Added # Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization Changed # User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor Removed # Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps [2019.05 and older] # Please see release notes for previous versions on our Github releases page.","title":"Release notes"},{"location":"changelog/#202001-2020-01-31","text":"","title":"[2020.01] - 2020-01-31"},{"location":"changelog/#added","text":"Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LIDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto","title":"Added"},{"location":"changelog/#changed","text":"Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS","title":"Changed"},{"location":"changelog/#201912-2020-01-21","text":"","title":"[2019.12] - 2020-01-21"},{"location":"changelog/#added_1","text":"Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map.","title":"Added"},{"location":"changelog/#changed_1","text":"Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh.","title":"Changed"},{"location":"changelog/#201911-2019-11-19","text":"","title":"[2019.11] - 2019-11-19"},{"location":"changelog/#added_2","text":"OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins.","title":"Added"},{"location":"changelog/#changed_2","text":"Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps.","title":"Changed"},{"location":"changelog/#201910-2019-10-28","text":"","title":"[2019.10] - 2019-10-28"},{"location":"changelog/#added_3","text":"Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor.","title":"Added"},{"location":"changelog/#changed_3","text":"Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start.","title":"Changed"},{"location":"changelog/#201909-2019-09-06","text":"","title":"[2019.09] - 2019-09-06"},{"location":"changelog/#added_4","text":"Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle","title":"Added"},{"location":"changelog/#changed_4","text":"Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations","title":"Changed"},{"location":"changelog/#201907-2019-08-09","text":"","title":"[2019.07] - 2019-08-09"},{"location":"changelog/#added_5","text":"Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization","title":"Added"},{"location":"changelog/#changed_5","text":"User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor","title":"Changed"},{"location":"changelog/#removed","text":"Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps","title":"Removed"},{"location":"changelog/#201905-and-older","text":"Please see release notes for previous versions on our Github releases page.","title":"[2019.05 and older]"},{"location":"clusters-tab/","text":"Web UI Clusters Tab Explanation Clusters have not been implemented yet. What a Cluster is top # A Cluster is a single unit of simulation. By default there is always local machine available as a cluster. When a user launches the simulator, there is a command line option available to select between simulation master and slave. When Simulator is started as master (default mode) user can access Web UI and perform simulation tasks. When Simulator is started as a slave (using command line argument) simulator is not rendering anything, but expects connection from master. For a simulation to start, Master and Slave should share the same maps, vehicles and simulation configuration. Simulation master is responsible to provide all required configuration and slave is responsible to download all asset bundles required to start simulation. Simulation master and other slaves are waiting for all nodes to download and cache required asset bundles. Master is responsible for assigning sensors to each Slave for simulation (including itself). After sensors are assigned each slave starts the simulation while synchronizing position of vehicles. See Configuration File and Command Line Parameters for details on how to setup the Simulator on each machine in a cluster. Why use a Cluster top # Clusters allow for better performance when generating data from multiple sensors. Unity does not have multiple-gpu support so multiple instances of the simulator are required. This also allows for large-scale simulations with multiple vehicles in the same map. How to Add a Cluster top # Click the Add new button In the dialogue that opens, enter the name of the cluster and IPv4 of each machine that will be in the cluster. A blank list of IPs will default to the localhost. How to Edit a Cluster top # Click the pencil icon In the dialogue that opens, the name of the cluster can be changed and the list of IPs can be modified. Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Clusters"},{"location":"clusters-tab/#what-a-cluster-is","text":"A Cluster is a single unit of simulation. By default there is always local machine available as a cluster. When a user launches the simulator, there is a command line option available to select between simulation master and slave. When Simulator is started as master (default mode) user can access Web UI and perform simulation tasks. When Simulator is started as a slave (using command line argument) simulator is not rendering anything, but expects connection from master. For a simulation to start, Master and Slave should share the same maps, vehicles and simulation configuration. Simulation master is responsible to provide all required configuration and slave is responsible to download all asset bundles required to start simulation. Simulation master and other slaves are waiting for all nodes to download and cache required asset bundles. Master is responsible for assigning sensors to each Slave for simulation (including itself). After sensors are assigned each slave starts the simulation while synchronizing position of vehicles. See Configuration File and Command Line Parameters for details on how to setup the Simulator on each machine in a cluster.","title":"What a Cluster is"},{"location":"clusters-tab/#why-use-a-cluster","text":"Clusters allow for better performance when generating data from multiple sensors. Unity does not have multiple-gpu support so multiple instances of the simulator are required. This also allows for large-scale simulations with multiple vehicles in the same map.","title":"Why use a Cluster"},{"location":"clusters-tab/#how-to-add-a-cluster","text":"Click the Add new button In the dialogue that opens, enter the name of the cluster and IPv4 of each machine that will be in the cluster. A blank list of IPs will default to the localhost.","title":"How to Add a Cluster"},{"location":"clusters-tab/#how-to-edit-a-cluster","text":"Click the pencil icon In the dialogue that opens, the name of the cluster can be changed and the list of IPs can be modified.","title":"How to Edit a Cluster"},{"location":"clusters-tab/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"config-and-cmd-line-params/","text":"Configuration File and Command Line Parameters Configuration File top # To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. Simulator configuration file includes parameters shared between different users and allows administrator to setup deployment specific parameters. Simulator uses YAML format for storing data in configuration file. See a table below to find out all supported parameters: Parameter Name Type Default Value Description hostname string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. port integer 8080 Port number used by HTTP server to host WebUI. headless bool false Whether or not simulator should work in headless mode only. If parameter is set to true - non headless simulation should fail to start, WebUI should not allow to create non-headless simulations. slave bool false Whether or not simulator should work in slave mode only. If parameter is set to true - HTTP server does not start and user only can run this instance as a part of cluster. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. api_hostname string localhost Name of the Python API host. By default it equal to hostname. Python API should respond to queries only related to the api_hostname provided. Star (*) can be used as a wildcard to match any domain. api_port integer 8181 Port number used by Python API to connect. cloud_url string TBD Cloud URL points to a simulator API endpoint in the cloud and is responsible for user authentication, new user registration and sharing content. Simulator uses our public API endpoint by default, but that could be changed for private on-premise deployment. Command Line Parameters top # Simulator accepts provided command line parameters during start. Command line parameters overrides values from Configuration File. Only most important parameters can be provided via command line, see more details about Simulator Configuration File in the section related to configuration file. List of supported command line parameters can be found below: Parameter Name Type Default Value Description --hostname or -h string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. --port or -p integer 8080 Port number used by HTTP server to host WebUI. --slave or -s none Whether or not simulator should work in slave mode only. If parameter is present - HTTP server does not start and user only can run this instance as a part of cluster. --master or -m none Whether or not simulator should work in master mode. If parameter is present - HTTP server starts and user can use WebUI regardless of what was specified in Simulator Configuration File. --username or -u string Provides username for authentication with the cloud. --password or -w string Provides password for authentication with the cloud. --agree none Accepts the license agreement and forces to skip it in Web UI for specified user. This parameter is optional and can only be used only when username has been provided.","title":"Configuration File and Command Line Parameters"},{"location":"config-and-cmd-line-params/#configuration-file","text":"To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. Simulator configuration file includes parameters shared between different users and allows administrator to setup deployment specific parameters. Simulator uses YAML format for storing data in configuration file. See a table below to find out all supported parameters: Parameter Name Type Default Value Description hostname string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. port integer 8080 Port number used by HTTP server to host WebUI. headless bool false Whether or not simulator should work in headless mode only. If parameter is set to true - non headless simulation should fail to start, WebUI should not allow to create non-headless simulations. slave bool false Whether or not simulator should work in slave mode only. If parameter is set to true - HTTP server does not start and user only can run this instance as a part of cluster. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. api_hostname string localhost Name of the Python API host. By default it equal to hostname. Python API should respond to queries only related to the api_hostname provided. Star (*) can be used as a wildcard to match any domain. api_port integer 8181 Port number used by Python API to connect. cloud_url string TBD Cloud URL points to a simulator API endpoint in the cloud and is responsible for user authentication, new user registration and sharing content. Simulator uses our public API endpoint by default, but that could be changed for private on-premise deployment.","title":"Configuration File"},{"location":"config-and-cmd-line-params/#command-line-parameters","text":"Simulator accepts provided command line parameters during start. Command line parameters overrides values from Configuration File. Only most important parameters can be provided via command line, see more details about Simulator Configuration File in the section related to configuration file. List of supported command line parameters can be found below: Parameter Name Type Default Value Description --hostname or -h string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. --port or -p integer 8080 Port number used by HTTP server to host WebUI. --slave or -s none Whether or not simulator should work in slave mode only. If parameter is present - HTTP server does not start and user only can run this instance as a part of cluster. --master or -m none Whether or not simulator should work in master mode. If parameter is present - HTTP server starts and user can use WebUI regardless of what was specified in Simulator Configuration File. --username or -u string Provides username for authentication with the cloud. --password or -w string Provides password for authentication with the cloud. --agree none Accepts the license agreement and forces to skip it in Web UI for specified user. This parameter is optional and can only be used only when username has been provided.","title":"Command Line Parameters"},{"location":"contributing/","text":"Contributing to LGSVL Simulator As an open project and community, we welcome and highly encourage contributions back to LGSVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of LGSVL Simulator, you can help contribute to the LGSVL Simulator project in several different ways. Feedback, questions, bug reports, and issues # The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com . Submitting a Pull Request # We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms. Documentation # We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website. Demonstrations # If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application. Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Contributing"},{"location":"contributing/#feedback-questions-bug-reports-and-issues","text":"The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com .","title":"Feedback, questions, bug reports, and issues"},{"location":"contributing/#submitting-a-pull-request","text":"We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms.","title":"Submitting a Pull Request"},{"location":"contributing/#documentation","text":"We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website.","title":"Documentation"},{"location":"contributing/#demonstrations","text":"If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application.","title":"Demonstrations"},{"location":"contributing/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"control-calibration/","text":"How to Collect Data with Control Calibration Sensor Control Calibration Sensor is for collecting control data to generate control calibration table which can be referred by control module to decide throttle, brake and steering command. Setup # Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map Instructions # Add WideFlatMap into WebUI. The assetbundle is available on the content website Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"How to Collect Data with Control Calibration Sensor"},{"location":"control-calibration/#setup","text":"Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map","title":"Setup"},{"location":"control-calibration/#instructions","text":"Add WideFlatMap into WebUI. The assetbundle is available on the content website Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"Instructions"},{"location":"controllable-plugins/","text":"Controllable Plugins Controllable plugins are custom controllables that can be added to a scene at runtime with the API. Before running the simulator (running the executable or pressing Play in the Editor) controllable plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/Controllables folder. Open-source example: TrafficCone Table of Contents Building Controllable Plugins Creating Controllable Plugins Controllable Logic Building Controllable Plugins top # Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details. Creating Controllable Plugins top # Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel Controllable Logic top # Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable, { public bool Spawned { get; set; } public string UID { get; set; } public string Key => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public string DefaultControlPolicy { get; set; } = \"\"; public string CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; CurrentState = \"\"; } public void Control(List<ControlAction> controlActions) { for (int i = 0; i < controlActions.Count; i++) { var action = controlActions[i].Action; var value = controlActions[i].Value; switch (action) { case = \"state\": // set in policy parse CurrentState = value; // switch (CurrentState) // set logic break; default: Debug.LogError($\"'{action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable Plugins"},{"location":"controllable-plugins/#building-controllable-plugins","text":"Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details.","title":"Building Controllable Plugins"},{"location":"controllable-plugins/#creating-controllable-plugins","text":"Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel","title":"Creating Controllable Plugins"},{"location":"controllable-plugins/#controllable-logic","text":"Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable, { public bool Spawned { get; set; } public string UID { get; set; } public string Key => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public string DefaultControlPolicy { get; set; } = \"\"; public string CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; CurrentState = \"\"; } public void Control(List<ControlAction> controlActions) { for (int i = 0; i < controlActions.Count; i++) { var action = controlActions[i].Action; var value = controlActions[i].Value; switch (action) { case = \"state\": // set in policy parse CurrentState = value; // switch (CurrentState) // set logic break; default: Debug.LogError($\"'{action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable Logic"},{"location":"create-ros2-ad-stack/","text":"How to create a ROS2-based AD stack with LGSVL Simulator This tutorial works with Simulator Release 2019.05 This documentation describes how to develop ROS2 nodes to receive sensor data from LGSVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with LGSVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously. Table of Contents Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package setup.py package.xml Building a ROS2 Package Running Rosbridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to LGSVL Simulator Running ROS2 Node References Copyright and License Requirements # Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator Setup # Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away. Installing Docker CE # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook Creating a ROS2 Package # A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml setup.py # from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, ) package.xml # <?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package> Building a ROS2 Package # Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install Running Rosbridge # Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge Writing ROS2 Subscriber Node # ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously. Subscribe to a single topic # import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Subscribe to multiple topics simultaneously # In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main() Writing ROS2 Publisher Node # The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge. Publish command back to LGSVL Simulator # import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Running ROS2 Node # Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node} References # Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"How to create a simple ROS2-based AD stack"},{"location":"create-ros2-ad-stack/#requirements","text":"Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator","title":"Requirements"},{"location":"create-ros2-ad-stack/#setup","text":"Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away.","title":"Setup"},{"location":"create-ros2-ad-stack/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"create-ros2-ad-stack/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"create-ros2-ad-stack/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"create-ros2-ad-stack/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"create-ros2-ad-stack/#creating-a-ros2-package","text":"A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml","title":"Creating a ROS2 Package"},{"location":"create-ros2-ad-stack/#setuppy","text":"from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, )","title":"setup.py"},{"location":"create-ros2-ad-stack/#packagexml","text":"<?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package>","title":"package.xml"},{"location":"create-ros2-ad-stack/#building-a-ros2-package","text":"Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install","title":"Building a ROS2 Package"},{"location":"create-ros2-ad-stack/#running-rosbridge","text":"Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge","title":"Running Rosbridge"},{"location":"create-ros2-ad-stack/#writing-ros2-subscriber-node","text":"ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously.","title":"Writing ROS2 Subscriber Node"},{"location":"create-ros2-ad-stack/#subscribe-to-a-single-topic","text":"import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Subscribe to a single topic"},{"location":"create-ros2-ad-stack/#subscribe-to-multiple-topics-simultaneously","text":"In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main()","title":"Subscribe to multiple topics simultaneously"},{"location":"create-ros2-ad-stack/#writing-ros2-publisher-node","text":"The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge.","title":"Writing ROS2 Publisher Node"},{"location":"create-ros2-ad-stack/#publish-command-back-to-lgsvl-simulator","text":"import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Publish command back to LGSVL Simulator"},{"location":"create-ros2-ad-stack/#running-ros2-node","text":"Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node}","title":"Running ROS2 Node"},{"location":"create-ros2-ad-stack/#references","text":"Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters","title":"References"},{"location":"create-ros2-ad-stack/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"faq/","text":"LGSVL Simulator FAQ What are the recommended system specs? What are the minimum REQUIRED system specs? top For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, Nvidia GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, Nvidia graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and Nvidia drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended. Does the simulator run on Windows/Mac/Linux? top Officially, you can run LGSVL Simulator on Windows 10 and Ubuntu 16.04 (or later). We do not support macOS at this time. Why does the simulator not open on Linux? top The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1 Which Unity version is required and how do I get it? top LGSVL Simulator is currently on Unity version 2019.1.10f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (20191.10f1) here: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 We are constantly working to ensure that LGSVL Simulator runs on the latest version of Unity which supports all of our required functionality. How do I setup development environment for Unity on Ubuntu? top Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2019.1.10f1: curl -fLo UnitySetup https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 16.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 16.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code Where are Unity log files located? top Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LG Silicon Valley Lab\\LGSVL Simulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/Unity/Editor/Player.log Linux Editor ~/.config/unity3d/Editor.log Why are assets/scenes missing/empty after cloning from git? top We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? top If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git ROS Bridge won't connect? top First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports. How do I control the ego vehicle (my vehicle) spawn position? top Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component. How can I add a custom ego vehicle to LGSVL Simulator? top Please see our tutorial on how to add a new ego vehicle to LGSVL Simulator here . How can I add extra sensors to vehicles in LGSVL Simulator? top Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the availble sensors. How can I add a custom map to LGSVL Simulator? top See Maps for details. How can I create or edit map annotations? top Please see our tutorial on how to add map annotations in LGSVL Simulator here . Why are pedestrians not spawning when annotated correctly? top LGSVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh. Why can't I find catkin_make command when building Apollo? top Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo. Why is Apollo perception module turning on and off all the time? top This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo Why does the Apollo vehicle stop at stop line and not cross intersections? top Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED). Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" top This is expected behavior. LGSVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it. Why does Rviz not load the Autoware vector map? top Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516 Why are there no maps when I make a local build? top See Build Instructions . It is not required to build the whole simulator using this tool. Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool ? top Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added. Why does the simulator start and then say the simulation is \"Invalid\"? top If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified. Why are there no assets when building the simulator from Unity Editor? top Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project. Other questions? top See our Github issues page, or email us at contact@lgsvlsimulator.com .","title":"FAQ"},{"location":"getting-started/","text":"LGSVL Simulator: An Autonomous Vehicle Simulator Website | Documentation | Download Table of Contents Introduction Getting Started Downloading and starting simulator Building and running from source Simulator Instructions Guide to simulator functionality Contact Copyright and License Introduction top # LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the TierIV's Autoware and Baidu's Apollo 5.0 and Apollo 3.0 platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo 5.0 fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork . Getting Started top # You can find complete and the most up-to-date guides on our documentation website . Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Quad core CPU Nvidia GTX 1080 (8GB memory) Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux. If running Apollo or Autoware on the same system as the Simulator, it is recommended to upgrade to a GPU with at least 10GB memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required). Downloading and starting simulator top # Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Unzip the downloaded folder and run the executable. Building and running from source # Check out our instructions for getting started with building from source here . Simulator Instructions top # After starting the simulator, you should see a button \"Open Browser...\" to open the UI in the browser. Click the button. Go to the Simulations tab and select the appropriate map and vehicle. For a standard setup, select \"BorregasAve\" for map and \"Jaguar2015XE (Apollo 5.0)\" for vehicle. Click \"Run\" to begin. The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo 5.0 repositories for instructions on running the platforms with the simulator. Guide to simulator functionality top # Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator. Contact top # Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at contact@lgsvlsimulator.com . Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Getting started"},{"location":"getting-started/#introduction","text":"LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the TierIV's Autoware and Baidu's Apollo 5.0 and Apollo 3.0 platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo 5.0 fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork .","title":"Introduction"},{"location":"getting-started/#getting-started","text":"You can find complete and the most up-to-date guides on our documentation website . Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Quad core CPU Nvidia GTX 1080 (8GB memory) Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux. If running Apollo or Autoware on the same system as the Simulator, it is recommended to upgrade to a GPU with at least 10GB memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required).","title":"Getting Started"},{"location":"getting-started/#downloading-and-starting-simulator","text":"Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Unzip the downloaded folder and run the executable.","title":"Downloading and starting simulator"},{"location":"getting-started/#building-and-running-from-source","text":"Check out our instructions for getting started with building from source here .","title":"Building and running from source"},{"location":"getting-started/#simulator-instructions","text":"After starting the simulator, you should see a button \"Open Browser...\" to open the UI in the browser. Click the button. Go to the Simulations tab and select the appropriate map and vehicle. For a standard setup, select \"BorregasAve\" for map and \"Jaguar2015XE (Apollo 5.0)\" for vehicle. Click \"Run\" to begin. The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo 5.0 repositories for instructions on running the platforms with the simulator.","title":"Simulator Instructions"},{"location":"getting-started/#guide-to-simulator-functionality","text":"Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator.","title":"Guide to simulator functionality"},{"location":"getting-started/#contact","text":"Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at contact@lgsvlsimulator.com .","title":"Contact"},{"location":"getting-started/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"ground-truth-json-example/","text":"Example JSON Configuration For Data Collection Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/sensor/camera/front_6mm/image/compressed Main Camera /simulator/ground_truth/3d_detections 3D Ground Truth /simulator/ground_truth/2d_detections 2D Ground Truth /simulator/depth_camera Depth Camera /simulator/semantic_camera Semantic Camera Subscribed Topics top # Topic Sensor Name /simulator/ground_truth/3d_visualize 3D Ground Truth Visualizer /simulator/ground_truth/2d_visualize 2D Ground Truth Visualizer JSON Configuration top # [ { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"DetectionRange\": 100, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } ]","title":"Sample sensor configuration for data collection"},{"location":"ground-truth-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"ground-truth-json-example/#published-topics","text":"Topic Sensor Name /apollo/sensor/camera/front_6mm/image/compressed Main Camera /simulator/ground_truth/3d_detections 3D Ground Truth /simulator/ground_truth/2d_detections 2D Ground Truth /simulator/depth_camera Depth Camera /simulator/semantic_camera Semantic Camera","title":"Published Topics"},{"location":"ground-truth-json-example/#subscribed-topics","text":"Topic Sensor Name /simulator/ground_truth/3d_visualize 3D Ground Truth Visualizer /simulator/ground_truth/2d_visualize 2D Ground Truth Visualizer","title":"Subscribed Topics"},{"location":"ground-truth-json-example/#json-configuration","text":"[ { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"DetectionRange\": 100, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } ]","title":"JSON Configuration"},{"location":"keyboard-shortcuts/","text":"Simulator Controls Key Bindings # Officially supported: F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam Camera Controls Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down For developer use: N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location Miscellaneous H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker Logitech G920 Wheel Inputs # Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Keyboard shortcuts"},{"location":"keyboard-shortcuts/#key-bindings","text":"","title":"Key Bindings"},{"location":"keyboard-shortcuts/#logitech-g920-wheel-inputs","text":"Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Logitech G920 Wheel Inputs"},{"location":"lane-following/","text":"ROS2 End-to-End Lane Following Model with LGSVL Simulator This tutorial works with Simulator Release 2019.05 This documentation describes applying a deep learning neural network for lane following in LGSVL Simulator . In this project, we use LGSVL Simulator for customizing sensors (one main camera and two side cameras) for a car, collect data for training, and deploying and testing a trained model. This project was inspired by NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Video # Table of Contents # Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with LGSVL Simulator Collect data from LGSVL Simulator Data preprocessing Train a model Drive with your trained model in LGSVL Simulator Future Works and Contributing References Getting Started # First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for LGSVL Simulator to connect. Prerequisites # Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU) Setup # Installing Docker CE # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Crystal + rosbridge Jupyter Notebook Features # Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend Training Details # Network Architecture # The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11 Hyperparameters # Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2 Dataset # Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images Center Image Left Image Right Image Original Image Cropped Image Data Distribution How to Collect Data and Train Your Own Model with LGSVL Simulator # Collect data from LGSVL Simulator # To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (geometry_msgs/TwistStamped) To launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect To drive a car and publish messages over rosbridge in training mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera , Left Camera , Right Camera , and check Publish Control Command The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files. Data preprocessing # Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training. Train a model # We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously. Drive with your trained model in LGSVL Simulator # Now, it's time to deploy your trained model and test drive with it using LGSVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual To drive a car in autonomous mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera (we don't need side cameras for inference) Your car will start driving autonomously and try to mimic your driving behavior when training the model. Future Works and Contributing # Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network) References # Lane Following Github Repository LGSVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Deep Learning Lane Following Model"},{"location":"lane-following/#video","text":"","title":"Video"},{"location":"lane-following/#table-of-contents","text":"Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with LGSVL Simulator Collect data from LGSVL Simulator Data preprocessing Train a model Drive with your trained model in LGSVL Simulator Future Works and Contributing References","title":"Table of Contents"},{"location":"lane-following/#getting-started","text":"First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for LGSVL Simulator to connect.","title":"Getting Started"},{"location":"lane-following/#prerequisites","text":"Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU)","title":"Prerequisites"},{"location":"lane-following/#setup","text":"","title":"Setup"},{"location":"lane-following/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"lane-following/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"lane-following/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"lane-following/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Crystal + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"lane-following/#features","text":"Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend","title":"Features"},{"location":"lane-following/#training-details","text":"","title":"Training Details"},{"location":"lane-following/#network-architecture","text":"The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11","title":"Network Architecture"},{"location":"lane-following/#hyperparameters","text":"Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2","title":"Hyperparameters"},{"location":"lane-following/#dataset","text":"Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images","title":"Dataset"},{"location":"lane-following/#how-to-collect-data-and-train-your-own-model-with-lgsvl-simulator","text":"","title":"How to Collect Data and Train Your Own Model with LGSVL Simulator"},{"location":"lane-following/#collect-data-from-lgsvl-simulator","text":"To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (geometry_msgs/TwistStamped) To launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect To drive a car and publish messages over rosbridge in training mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera , Left Camera , Right Camera , and check Publish Control Command The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files.","title":"Collect data from LGSVL Simulator"},{"location":"lane-following/#data-preprocessing","text":"Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training.","title":"Data preprocessing"},{"location":"lane-following/#train-a-model","text":"We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously.","title":"Train a model"},{"location":"lane-following/#drive-with-your-trained-model-in-lgsvl-simulator","text":"Now, it's time to deploy your trained model and test drive with it using LGSVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual To drive a car in autonomous mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera (we don't need side cameras for inference) Your car will start driving autonomously and try to mimic your driving behavior when training the model.","title":"Drive with your trained model in LGSVL Simulator"},{"location":"lane-following/#future-works-and-contributing","text":"Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network)","title":"Future Works and Contributing"},{"location":"lane-following/#references","text":"Lane Following Github Repository LGSVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"References"},{"location":"lane-following/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"map-annotation/","text":"Map Annotation The LGSVL Simulator supports creating, editing, and exporting of HD maps of existing 3D environments (Unity scenes). The maps can be saved in the currently supported Apollo, Autoware or Lanelet2 formats. Currently, annotating map is only recommended while running the simulator as a Unity project in a Windows environment. Table of Contents Creating a New Map Annotate Lanes Annotate Intersections Annotate Self-Reversing Lanes Annotate Other Features Export Map Annotations Import Map Annotations Map Formats Creating a New Map top # Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Make sure your roads has added Mesh Collider . Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show existing map annotation. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All Annotate Lanes top # Create Parent Object top In the Map prefab (if you don't have one, you can create an empty GameObject and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes section 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane Make Lanes top Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to deterin how many waypoints to add in between Verify Lane is the selected Map Object Type Verify NO_TURN is the selected Lane Turn Type Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted Make Boundary Lines top Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible If you are annotating map for Lanelet2 format, you also need to annotate boundary lines for every lane and drag them into the corresponding field in the lane object. Annotate Intersections top # Create Parent Object top In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated. Adjust the Trigger Bounds of the MapIntersection so that the box covers the center of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection Create Intersection Lanes top Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating for Lanelet2, you also need to annotate boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line For NPCs to properly navigate an intersection, the Yield To Lanes list must be manually filled in. With View Selected toggled in the Map Annotation Tool, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list Create Traffic Signals top Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The annotation directions must match the directions of the mesh. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Create Traffic Signs top Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box Create Stop Lines top Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stopline and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should be in the same direction as the lanes. A StopLine needs to with the lanes that approch it. The last waypoint of approaching lanes should be past the line. Create Pole top Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights Annotate Self-Reversing Lanes top # These types of lanes are only supported on Apollo 5.0 Annotation Information top There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane. Annotate Self-Reversing Lanes under Traffic Lanes top Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Self-Reversing Lanes under Intersections top Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane1_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane1_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane1_forward, MapLane1_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Other Features top # Other annotation features may be included in the TrafficLane or Intersection objects or they may be sorted into other parent objects. Create Pedestrian Path top This annotation controls where pedestrians will pass with the highest priority. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object Create Junction top Junction annotations can be used by the AD stack if needed. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction Create Crosswalk top Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk Create Clear Area top Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea Create Parking Space top Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space. Create Speed Bump top Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints. Export Map Annotations top # HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Import Map Annotations top # The simulator can import a variety of formats of annotated map. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly. Map Formats top # For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map Annotation"},{"location":"map-annotation/#creating-a-new-map","text":"Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Make sure your roads has added Mesh Collider . Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show existing map annotation. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All","title":"Creating a New Map"},{"location":"map-annotation/#annotate-lanes","text":"","title":"Annotate Lanes"},{"location":"map-annotation/#annotate-intersections","text":"","title":"Annotate Intersections"},{"location":"map-annotation/#annotate-self-reversing-lanes","text":"These types of lanes are only supported on Apollo 5.0","title":"Annotate Self-Reversing Lanes"},{"location":"map-annotation/#annotate-other-features","text":"Other annotation features may be included in the TrafficLane or Intersection objects or they may be sorted into other parent objects.","title":"Annotate Other Features"},{"location":"map-annotation/#export-map-annotations","text":"HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map","title":"Export Map Annotations"},{"location":"map-annotation/#import-map-annotations","text":"The simulator can import a variety of formats of annotated map. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly.","title":"Import Map Annotations"},{"location":"map-annotation/#map-formats","text":"For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map Formats"},{"location":"maps-tab/","text":"Web UI Maps Tab Explanation A Map can be in the following states. A Map with a local URL or if it has already been downloaded will have a Valid status. If the URL to the Map assetbundle is not local and the assetbundle is not in the local database, then the vehicle needs to be downloaded. Currently only 1 assetbundle is downloaded at a time. If an assetbundle is downloading, the Map will show a GREY dot and the status will be Downloading with the download percentage. If another assetbundle is downloading, the icon will be ORANGE and the status will be Downloading without a percentage. A downloading Map can be interrupted by pressing the stop button. If the Map is not usable in a Simulation it will have an Invalid status. This can be because the local assetbundle is not usable or the download was interrupted. Where to find Maps top # Map AssetBundles and HD maps are available on our content website . When adding a map, the link to the appropriate AssetBundle can be entered as the URL or the AssetBundle can be downloaded manually and the local path can be entered. The HD maps for maps are available in the same page. Please see the relevant doc for instructions on how to add an HD map to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware How to Add a Map top # Click the Add new button. In the dialogue that opens, enter the name of the map and the URL to the AssetBundle. This can be a URL to a location in the cloud or to a location on a local drive. If the URL is not local, the AssetBundle will be downloaded to the local database. How to Edit a Map top # Click the pencil icon In the dialogue that opens, the name of the map can be changed and the URL to the AssetBundle. If the URL is changed, the AssetBundle in the database will be updated (downloaded if necessary) How to Annotate a Map top # Please see Map Annotation for more information on how to annotate a map in Unity. This also details how to export or import and HD map. How to Migrate a Map top # If you have created maps in our old simulator (before HDRP), you can reuse those maps in current HDRP simulator by following steps: In old simulator You need to make some changes in header inside HDMapTool.cs (line 148-154): Convert your MapOrigin Northing/Easting to Longitude/Latitude. Replace values for left and right with Latitude. Repalce values for top and bottom with Longitude. You also need to change zone in header. If you have intersections annotated, you need to annotate junctions for each intersection in the old simulator since we are importing intersections based on junction in HDRP simulator. Otherwise, signals/signs will not be grouped correctly, every signal/sign will be created as an intersection object. Export map to Apollo Map file: base_map.bin . In current HDRP simulator Select Simulator->Import HD Map , and set Import Format as Apollo 5 HD Map . Select the exported map and import. Note, you need to check intersections, some signals/signs may be grouped incorrectly.","title":"Maps"},{"location":"maps-tab/#where-to-find-maps","text":"Map AssetBundles and HD maps are available on our content website . When adding a map, the link to the appropriate AssetBundle can be entered as the URL or the AssetBundle can be downloaded manually and the local path can be entered. The HD maps for maps are available in the same page. Please see the relevant doc for instructions on how to add an HD map to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware","title":"Where to find Maps"},{"location":"maps-tab/#how-to-add-a-map","text":"Click the Add new button. In the dialogue that opens, enter the name of the map and the URL to the AssetBundle. This can be a URL to a location in the cloud or to a location on a local drive. If the URL is not local, the AssetBundle will be downloaded to the local database.","title":"How to Add a Map"},{"location":"maps-tab/#how-to-edit-a-map","text":"Click the pencil icon In the dialogue that opens, the name of the map can be changed and the URL to the AssetBundle. If the URL is changed, the AssetBundle in the database will be updated (downloaded if necessary)","title":"How to Edit a Map"},{"location":"maps-tab/#how-to-annotate-a-map","text":"Please see Map Annotation for more information on how to annotate a map in Unity. This also details how to export or import and HD map.","title":"How to Annotate a Map"},{"location":"maps-tab/#how-to-annotate-a-map","text":"If you have created maps in our old simulator (before HDRP), you can reuse those maps in current HDRP simulator by following steps: In old simulator You need to make some changes in header inside HDMapTool.cs (line 148-154): Convert your MapOrigin Northing/Easting to Longitude/Latitude. Replace values for left and right with Latitude. Repalce values for top and bottom with Longitude. You also need to change zone in header. If you have intersections annotated, you need to annotate junctions for each intersection in the old simulator since we are importing intersections based on junction in HDRP simulator. Otherwise, signals/signs will not be grouped correctly, every signal/sign will be created as an intersection object. Export map to Apollo Map file: base_map.bin . In current HDRP simulator Select Simulator->Import HD Map , and set Import Format as Apollo 5 HD Map . Select the exported map and import. Note, you need to check intersections, some signals/signs may be grouped incorrectly.","title":"How to Annotate a Map"},{"location":"npc-map-navigation/","text":"NPC Map Navigation In the latest Simulator Release, NPC vehicles follow the annotated HD map. No extra steps are required. The following tutorial works with Simulator Release 2019.05 Non-player character (NPC) vehicles now use the MapSegmentBuilder classes to navigate annotated maps. Map Manager Use SanFrancisco.scene as a template to build map data for NPCs. Remove SFTraffic_New.prefab from scene and any associated scripts, e.g., TrafPerformanceManager.cs, TrafInfoManager.cs and TrafSystem.cs. Create a new GameObject at position Vector3.zero and Quaternion.identity, named Map . Add MapManager.cs component to this object and save as a prefab in project assets. The public field, SpawnLanesHolder, of MapManager.cs requires the MapLaneSegmentBuilder holder transform. The public field, IntersectionsHolder, of MapManager.cs requires the TrafficLights holder transform. This has intersection meshes and scripts for lights. The public fields Green, Yellow, and Red are materials for the segmentation camera system. If using the Traffic meshes from SanFrancisco.scene, these need to be added here. If not, IntersectionComponent.cs and associated scripts will need to be edited. Map Lane and Intersection Grouping Create TrafficLanes holder object as a child of Map.prefab . Place all MapLaneSegmentBuilder objects into TrafficLanes holder object for all non intersection lanes. Create IntersectionLanes holder object as a child of Map.prefab . Create a new Intersection holder object as a child of IntersectionLanes transform for each intersection annotation. Be sure its world position is in the center of each intersection. Place MapLaneSegmentBuilder and MapStopLineSegmentBuilder objects into Intersection holder object for each intersection. Map Intersection Builder For each Intersection holder, add the MapIntersectionBuilder.cs component. Traffic Lights Create a TrafficLights holder object to hold all traffic light meshes or place all traffic meshes under the map annotation Intersections . Just be sure to have the root holder be in MapManager.cs IntersectionHolder public reference. Create a Intersection holder object. Be sure its world position is in the center of each intersection. Add IntersectionComponent.cs to each Intersection holder object. Place TrafficLightPole facing it's corresponding StopLineSegmentBuilder object. The transfom needs to be Z axis or gizmo arrow forward, parallel to the StopLineSegmentBuilder object Z axis or gizmo arrow forward. Add IntersectionTrafficLightSetComponent.cs. Place as a child of the Intersection holder object. For opposite facing TrafficLightPoles and StopLineSegmentBuilders , be sure to orient transforms in Z axis or gizmo arrow forward but perpendicular to other facing light poles and stoplines. Add TrafficLight meshes as children of the TrafficLightPole . Add IntersectionTrafficLightSetComponent.cs to each TrafficLight . StopLine and MapLaneSegmentBuilder overlap MapLaneSegmentBuilders final waypoint needs to be slightly overlapping the MapStopLineBuilder","title":"NPC Map Navigation"},{"location":"openai-gym/","text":"Reinforcement Learning with OpenAI Gym OpenAI Gym is a toolkit for developing reinforcement learning algorithms. Gym provides a collection of test problems called environments which can be used to train an agent using a reinforcement learning. Each environment defines the reinforcement learnign problem the agent will try to solve. To facilitate developing reinforcement learning algorithms with the LGSVL Simulator , we have developed gym-lgsvl , a custom environment that using the openai gym interface. gym-lgsvl can be used with general reinforcement learning algorithms implementations that are compatible with openai gym. Developers can modify the environment to define the specific reinforcement learning problem they are trying to solve. Requirements # Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv Setup # Clone the LGSVL Simulator repository: git clone https://github.com/lgsvl/simulator.git Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone this repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e . Getting Started # The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5 Customizing the environment # The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py . CONFIG # Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation. Reward calculation # The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode. Sensors # By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called. NPC Behavior # The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around. Copyright and License # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Reinforcement Learning with OpenAI Gym"},{"location":"openai-gym/#requirements","text":"Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv","title":"Requirements"},{"location":"openai-gym/#setup","text":"Clone the LGSVL Simulator repository: git clone https://github.com/lgsvl/simulator.git Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone this repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e .","title":"Setup"},{"location":"openai-gym/#getting-started","text":"The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5","title":"Getting Started"},{"location":"openai-gym/#customizing-the-environment","text":"The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py .","title":"Customizing the environment"},{"location":"openai-gym/#config","text":"Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation.","title":"CONFIG"},{"location":"openai-gym/#reward-calculation","text":"The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode.","title":"Reward calculation"},{"location":"openai-gym/#sensors","text":"By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called.","title":"Sensors"},{"location":"openai-gym/#npc-behavior","text":"The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"NPC Behavior"},{"location":"openai-gym/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"perception-ground-truth/","text":"Ground Truth Obstacles This guide is for Simulator Release 2019.05 . To visualize ground truth sensor data in version 2019.07, please activate the ground truth sensors according to the example sensor configuration file. Overview # You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles. View ground truth obstacles in Simulator # Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view. Bounding box colors # Green: Vehicles Yellow: Pedestrians Purple: Unknown Subscribe to ground truth ROS messages # LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge. Install the lgsvl_msgs ROS package # Use LGSVL Apollo or Autoware repository If you are using LGSVL's forks of Apollo or Autoware , the package is already included as a submodule in the respective workspace: LGSVL Autoware: autoware -> ros -> src -> msgs -> lgsvl_msgs LGSVL Apollo: apollo -> ros_pkgs -> src -> lgsvl_msgs Following the instructions to build the ROS workspace will build the lgsvl_msgs package as well. If you are not running LGSVL's Apollo or Autoware forks, you can directly clone our lgsvl_msgs package into your ROS workspace and build. Manually install lgsvl_msgs Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make Subscribe to ground truth messages from Simulator # You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link View estimated detections in Simulator # If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"Viewing and subscribing to ground truth data"},{"location":"perception-ground-truth/#overview","text":"You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles.","title":"Overview"},{"location":"perception-ground-truth/#view-ground-truth-obstacles-in-simulator","text":"Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view.","title":"View ground truth obstacles in Simulator"},{"location":"perception-ground-truth/#bounding-box-colors","text":"Green: Vehicles Yellow: Pedestrians Purple: Unknown","title":"Bounding box colors"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-ros-messages","text":"LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge.","title":"Subscribe to ground truth ROS messages"},{"location":"perception-ground-truth/#install-the-lgsvl_msgs-ros-package","text":"","title":"Install the lgsvl_msgs ROS package"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-messages-from-simulator","text":"You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link","title":"Subscribe to ground truth messages from Simulator"},{"location":"perception-ground-truth/#view-estimated-detections-in-simulator","text":"If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"View estimated detections in Simulator"},{"location":"python-api/","text":"Python API Guide Overview # LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command Line Parameters for more information. Table of Contents Overview Requirements Quickstart Core Concepts Simulation Non-realtime Simulation Agents EGO Vehicle NPC Vehicles Pedestrians Callbacks Agent Callbacks 'EgoVehicle NpcVehicle Callbacks Pedestrian Callbacks Sensors Camera Sensor Lidar Sensor IMU Sensor GPS Sensor Radar Sensor CAN bus Weather and Time of Day Control Controllable Objects Helper Functions Changelog Copyright and License Requirements top # Using Python API requires Python version 3.5 or later. Quickstart top # Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor). Simulator by default listens for connections on port 8181 on localhost. Click the Open Browser button to open the Simulator UI. After the default maps and vehicles have been downloaded, navigate to the Simulations tab. Create a new Simulation. Give it a name and check the API Only option. Click Submit . Select the newly created Simulation and click the \"Play\" button in the bottom right. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move. Core concepts top # The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values. Simulation top # To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load(scene = \"BorregasAve\", seed = 650387) Scene is a string representing the name of the Map in the Web UI. Currently available scenes: BorregasAve - small suburban map AutonomouStuff - small office park Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check the Web UI Maps tab for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution: Non-realtime Simulation top # The simulator can be run at faster-than-realtime speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time. Agents top # You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"Lincoln2017MKZ (Apollo 5.0)\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ (Apollo 5.0) vehicle from the Web UI Vehicles tab. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: Jaguar2015XE (Apollo 3.0) - Apollo 3.0 vehicle Jaguar2015XE (Apollo 5.0) - Apollo 5.0 vehicle Jaguar2015XE (Autoware) - Autoware vehicle Lexus2016RXHybrid (Autoware) - Autoware vehicle Lincoln2017MKZ (Apollo 5.0) - Apollo 5.0 vehicle Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information. EGO vehicle top # EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True) NPC vehicles top # You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. Pedestrians top # You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. Callbacks top # The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below. Agent Callbacks top # collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point. EgoVehicle Callbacks top # In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information. NpcVehicle Callbacks top # In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance Pedestrian Callbacks top # In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer. Sensors top # EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge Camera Sensor top # The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files. Lidar Sensor top # Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255). IMU Sensor top # You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent. GPS Sensor top # You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees Radar Sensor top # Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor. CAN bus top # Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor. Weather and Time of Day Control top # You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). Controllable Objects top # A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state Helper Functions top # Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values. Changelog top # 2020-01-30 * Extended controllable objects to support plugins - see controllable plugins 2019-09-05 * Extended DriveWaypoint to support angle, idle time and trigger distance * Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release Copyright and License top # Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Python API Guide"},{"location":"python-api/#overview","text":"LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command Line Parameters for more information.","title":"Overview"},{"location":"python-api/#requirements","text":"Using Python API requires Python version 3.5 or later.","title":"Requirements"},{"location":"python-api/#quickstart","text":"Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor). Simulator by default listens for connections on port 8181 on localhost. Click the Open Browser button to open the Simulator UI. After the default maps and vehicles have been downloaded, navigate to the Simulations tab. Create a new Simulation. Give it a name and check the API Only option. Click Submit . Select the newly created Simulation and click the \"Play\" button in the bottom right. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move.","title":"Quickstart"},{"location":"python-api/#core-concepts","text":"The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values.","title":"Core Concepts"},{"location":"python-api/#simulation","text":"To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load(scene = \"BorregasAve\", seed = 650387) Scene is a string representing the name of the Map in the Web UI. Currently available scenes: BorregasAve - small suburban map AutonomouStuff - small office park Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check the Web UI Maps tab for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution:","title":"Simulation"},{"location":"python-api/#non-realtime-simulation","text":"The simulator can be run at faster-than-realtime speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time.","title":"Non-realtime Simulation"},{"location":"python-api/#agents","text":"You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"Lincoln2017MKZ (Apollo 5.0)\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ (Apollo 5.0) vehicle from the Web UI Vehicles tab. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: Jaguar2015XE (Apollo 3.0) - Apollo 3.0 vehicle Jaguar2015XE (Apollo 5.0) - Apollo 5.0 vehicle Jaguar2015XE (Autoware) - Autoware vehicle Lexus2016RXHybrid (Autoware) - Autoware vehicle Lincoln2017MKZ (Apollo 5.0) - Apollo 5.0 vehicle Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information.","title":"Agents"},{"location":"python-api/#ego-vehicle","text":"EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True)","title":"EGO Vehicle"},{"location":"python-api/#npc-vehicles","text":"You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn.","title":"NPC Vehicles"},{"location":"python-api/#pedestrians","text":"You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback.","title":"Pedestrians"},{"location":"python-api/#callbacks","text":"The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below.","title":"Callbacks"},{"location":"python-api/#agent-callbacks","text":"collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point.","title":"Agent Callbacks"},{"location":"python-api/#egovehicle-callbacks","text":"In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information.","title":"'EgoVehicle"},{"location":"python-api/#npcvehicle-callbacks","text":"In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance","title":"NpcVehicle Callbacks"},{"location":"python-api/#pedestrian-callbacks","text":"In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer.","title":"Pedestrian Callbacks"},{"location":"python-api/#sensors","text":"EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge","title":"Sensors"},{"location":"python-api/#camera-sensor","text":"The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files.","title":"Camera Sensor"},{"location":"python-api/#lidar-sensor","text":"Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255).","title":"Lidar Sensor"},{"location":"python-api/#imu-sensor","text":"You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent.","title":"IMU Sensor"},{"location":"python-api/#gps-sensor","text":"You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees","title":"GPS Sensor"},{"location":"python-api/#radar-sensor","text":"Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"Radar Sensor"},{"location":"python-api/#can-bus","text":"Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"CAN bus"},{"location":"python-api/#weather-and-time-of-day-control","text":"You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ).","title":"Weather and Time of Day Control"},{"location":"python-api/#controllable-objects","text":"A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state","title":"Controllable Objects"},{"location":"python-api/#helper-functions","text":"Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values.","title":"Helper Functions"},{"location":"python-api/#changelog","text":"2020-01-30 * Extended controllable objects to support plugins - see controllable plugins 2019-09-05 * Extended DriveWaypoint to support angle, idle time and trigger distance * Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Changelog"},{"location":"python-api/#copyright-and-license","text":"Copyright (c) 2019 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"sensor-json-options/","text":"Sensor JSON Options This page details the different available sensors and the configuration options possible. Table of Contents Examples How to Specify a Sensor Color Camera Depth Camera Semantic Camera Lidar 3D Ground Truth 3D Ground Truth Visualizer CAN-Bus GPS Device GPS Odometry GPS-INS Status Vehicle Control Keyboard Control Wheel Control Manual Control Cruise Control IMU 2D Ground Truth 2D Ground Truth Visualizer Radar Clock Control Calibration Transform Sensor Examples top # Example JSON configurations are available here: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON Data Collection JSON How to Specify a Sensor top # A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis Color Camera top # This is the type of sensor that would be used for the Main Camera in Apollo. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters is ignored. { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Depth Camera top # This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * See notes on DistortionParameters for Color Camera. { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Semantic Camera top # This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * See notes on DistortionParameters for Color Camera. { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Lidar top # This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 3D Ground Truth top # This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 3D Ground Truth Visualizer top # This sensor will visualize bounding boxes on objects as detected by the AD Stack. It does not publish any data and instead subscribes to a topic from the AD Stack. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } CAN-Bus top # This sensor sends data about the vehicle chassis. The data includes: - Speed [m/s] - Throttle [%] - Braking [%] - Steering [+/- %] - Parking Brake Status [bool] - High Beam Status [bool] - Low Beam Status [bool] - Hazard Light Status [bool] - Fog Light Status [bool] - Left Turn Signal Status [bool] - Right Turn Signal Status [bool] - Wiper Status [bool] - Reverse Gear Status [bool] - Selected Gear [Int] - Engine Status [bool] - Engine RPM [RPM] - GPS Latitude [Latitude] - GPS Longitude [Longitude] - Altitude [m] - Orientation [3D Vector of Euler angles] - Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Device top # This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Odometry top # This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS-INS Status top # This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Vehicle Control top # This sensor is required for a vehicle to subscribe to the control topic of an AD Stack. { \"type\": \"Vehicle Control\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } Keyboard Control top # This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" } Wheel Control top # This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"Wheel Control\", \"name\": \"Wheel Car Control\" } Manual Control top # This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. {Deprecated} Will be removed next release. Use Keyboard Control { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } Cruise Control top # This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"Cruise Control\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } } IMU top # This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 2D Ground Truth top # This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 2D Ground Truth Visualizer top # This sensor will visualize bounding boxes on objects as detected by the AD Stack, it does not publish any data. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta In order for bounding boxes to align properly, parameters should match the same camera that the AD Stack is using for detection (i.e. if running Apollo, the parameters should match the sensor named \"Main Camera\"). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Radar top # This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Clock top # This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message. Only parameter to use is topic name. { \"type\": \"Clock\", \"name\": \"ROS Clock\", \"params\": { \"Topic\": \"/clock\" } } Control Calibration top # This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } } Transform Sensor top # This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform Sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform Sensor . Example usage { \"type\": \"Transform Sensor\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Total Control Calibration Criteria:","title":"Sensor Parameters"},{"location":"sensor-json-options/#examples","text":"Example JSON configurations are available here: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON Data Collection JSON","title":"Examples"},{"location":"sensor-json-options/#how-to-specify-a-sensor","text":"A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis","title":"How to Specify a Sensor"},{"location":"sensor-json-options/#color-camera","text":"This is the type of sensor that would be used for the Main Camera in Apollo. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters is ignored. { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Color Camera"},{"location":"sensor-json-options/#depth-camera","text":"This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * See notes on DistortionParameters for Color Camera. { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Depth Camera"},{"location":"sensor-json-options/#semantic-camera","text":"This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool true DistortionParameters parameters used by distortion* List of Float empty list * See notes on DistortionParameters for Color Camera. { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Semantic Camera"},{"location":"sensor-json-options/#lidar","text":"This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Lidar"},{"location":"sensor-json-options/#3d-ground-truth","text":"This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"3D Ground Truth"},{"location":"sensor-json-options/#3d-ground-truth-visualizer","text":"This sensor will visualize bounding boxes on objects as detected by the AD Stack. It does not publish any data and instead subscribes to a topic from the AD Stack. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"3D Ground Truth Visualizer"},{"location":"sensor-json-options/#can-bus","text":"This sensor sends data about the vehicle chassis. The data includes: - Speed [m/s] - Throttle [%] - Braking [%] - Steering [+/- %] - Parking Brake Status [bool] - High Beam Status [bool] - Low Beam Status [bool] - Hazard Light Status [bool] - Fog Light Status [bool] - Left Turn Signal Status [bool] - Right Turn Signal Status [bool] - Wiper Status [bool] - Reverse Gear Status [bool] - Selected Gear [Int] - Engine Status [bool] - Engine RPM [RPM] - GPS Latitude [Latitude] - GPS Longitude [Longitude] - Altitude [m] - Orientation [3D Vector of Euler angles] - Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"CAN-Bus"},{"location":"sensor-json-options/#gps-device","text":"This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Device"},{"location":"sensor-json-options/#gps-odometry","text":"This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Odometry"},{"location":"sensor-json-options/#gps-ins-status","text":"This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS-INS Status"},{"location":"sensor-json-options/#vehicle-control","text":"This sensor is required for a vehicle to subscribe to the control topic of an AD Stack. { \"type\": \"Vehicle Control\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } }","title":"Vehicle Control"},{"location":"sensor-json-options/#keyboard-control","text":"This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }","title":"Keyboard Control"},{"location":"sensor-json-options/#wheel-control","text":"This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"Wheel Control\", \"name\": \"Wheel Car Control\" }","title":"Wheel Control"},{"location":"sensor-json-options/#manual-control","text":"This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. {Deprecated} Will be removed next release. Use Keyboard Control { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }","title":"Manual Control"},{"location":"sensor-json-options/#cruise-control","text":"This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"Cruise Control\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } }","title":"Cruise Control"},{"location":"sensor-json-options/#imu","text":"This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"IMU"},{"location":"sensor-json-options/#2d-ground-truth","text":"This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"2D Ground Truth"},{"location":"sensor-json-options/#2d-ground-truth-visualizer","text":"This sensor will visualize bounding boxes on objects as detected by the AD Stack, it does not publish any data. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta In order for bounding boxes to align properly, parameters should match the same camera that the AD Stack is using for detection (i.e. if running Apollo, the parameters should match the sensor named \"Main Camera\"). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"2D Ground Truth Visualizer"},{"location":"sensor-json-options/#radar","text":"This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Radar"},{"location":"sensor-json-options/#clock","text":"This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message. Only parameter to use is topic name. { \"type\": \"Clock\", \"name\": \"ROS Clock\", \"params\": { \"Topic\": \"/clock\" } }","title":"Clock"},{"location":"sensor-json-options/#control-calibration","text":"This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } }","title":"Control Calibration"},{"location":"sensor-json-options/#transform-sensor","text":"This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform Sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform Sensor . Example usage { \"type\": \"Transform Sensor\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Total Control Calibration Criteria:","title":"Transform Sensor"},{"location":"sensor-plugins/","text":"Sensor Plugins Sensor plugins are custom sensors that can be added to a vehicle configuration. Sensor plugins must be built by the simulator and the resultant bundle named sensor_XXX must be placed in the AssetBundles/Sensors folder. If running the binary, this folder is included in the downloaded .zip. If running in Editor, the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). The sensor can be added to a vehicle configuration just like other sensors, see here Building sensor plugins to bundle is done as below 1. Open Simulator -> Build... menu item 2. Select sensor plugins in \"Sensor\" section of build window 3. Build plugins with \"Build\" button To make sensor plugin, create folder in Assets/External/Sensors , for example Assets/External/Sensors/CustomCameraSensor . Inside this folder you must place sensor prefab with same name ( CustomCameraSensor.prefab ) that will be used by simulator to instantiate at runtime. This prefab must have the sensor script added to the root of the prefab. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the sensor (e.g. CustomCameraSensor ) In the Inspector for this object, select Add Component Search for the sensor script Drag this object from the scene hierarchy into the project folder Additionally you can place C# scripts which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). Sensor plugins must have SensorType attribute which specifies the kind of sensor being implemented as well as the type of data that the sensor sends over the bridge. In addition, it must have SensorBase as the base class and must implement the OnBridgeSetup , OnVisualize , and OnVisualizeToggle methods. Sensors can optionally include CheckVisible method to prevent NPC or Pedestrians from spawning in bounds of the sensor. See the below codeblock from the ColorCamera sensor: namespace Simulator.Sensors { // The SensorType's name will match the `type` when defining a sensor in the JSON configuration of a vehicle // The requiredType list is required if data will be sent over the bridge. It can otherwise be empty. // Publishable data types are: // CanBusData, CLockData, Detected2DObjectData, Detected3DObjectData, DetectedRadarObjectData, // GpsData, ImageData, ImuData, PointCloudData, SignalData, VehicleControlData [SensorType(\"Custom Color Camera\", new[] { typeof(ImageData)})] // Inherits Monobehavior // SensorBase also defines the parameters Name, Topic, and Frame public partial class CustomCameraSensor : SensorBase { private Camera Camera; IBridge Bridge; IWriter<ImageData> Writer; // These public variables can be set in the JSON configuration [SensorParameter] [Range(1, 128)] public int JpegQuality = 75; //Sets up the bridge to send this sensor's data public override void OnBridgeSetup(IBridge bridge) { Bridge = bridge; Writer = bridge.AddWriter<ImageData>(Topic); } // Defines how the sensor data will be visualized in the simulator public override void OnVisualize(Visualizer visualizer) { Debug.Assert(visualizer != null); visualizer.UpdateRenderTexture(Camera.activeTexture, Camera.aspect); } // Called when user toggles visibility of sensor visualization // This function needs to be implemented, but otherwise can be empty public override void OnVisualizeToggle(bool state) { } // Called when NPC and Pedestrian managers need to check if visible by sensor // camera or bounds before placing object in scene public override void CheckVisible(Bounds bounds) { var activeCameraPlanes = GeometryUtility.CalculateFrustumPlanes(Camera); return GeometryUtility.TestPlanesAABB(activeCameraPlanes, bounds); } } } SensorBase in inherited from Unity's Monobehavior so any of the Messages can be used to control how and when the sensor collects data. Open-source examples are available: Comfort Sensor","title":"Sensor Plugins"},{"location":"sensor-visualizers/","text":"Sensor Visualizers When in a non-Headless Simulation, sensor visualizers can be toggled from the menu. To visualize a sensor, click the \"eye\" next to the sensor name. Sensors are identified by the name parameter from the JSON configuration. For full details on the possible JSON parameters see Sensor Parameters Not all sensors have visualizations available, only sensors who have will show their visualizations. Table of Contents Cameras Color Camera Depth Camera Semantic Camera 2D Ground Truth Lidar Radar 3D Ground Truth Cameras top # When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" again. Color Camera top # Visualized Color camera shows the same things that are visible from the normal follow and free cameras, but from the perpsective defined in the JSON configuration. Depth Camera top # Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object. Semantic Camera top # Visualized Semantic camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F 2D Ground Truth top # Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box. Lidar top # Visualized Lidar shows the point cloud that is detected. Radar top # Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs ni a green box, bicycles in a cyan box, and other EGOs in a magenta box. 3D Ground Truth top # Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"Sensor Visualization"},{"location":"sensor-visualizers/#cameras","text":"When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" again.","title":"Cameras"},{"location":"sensor-visualizers/#color-camera","text":"Visualized Color camera shows the same things that are visible from the normal follow and free cameras, but from the perpsective defined in the JSON configuration.","title":"Color Camera"},{"location":"sensor-visualizers/#depth-camera","text":"Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object.","title":"Depth Camera"},{"location":"sensor-visualizers/#semantic-camera","text":"Visualized Semantic camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F","title":"Semantic Camera"},{"location":"sensor-visualizers/#2d-ground-truth","text":"Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box.","title":"2D Ground Truth"},{"location":"sensor-visualizers/#lidar","text":"Visualized Lidar shows the point cloud that is detected.","title":"Lidar"},{"location":"sensor-visualizers/#radar","text":"Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs ni a green box, bicycles in a cyan box, and other EGOs in a magenta box.","title":"Radar"},{"location":"sensor-visualizers/#3d-ground-truth","text":"Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"3D Ground Truth"},{"location":"simulation-menu/","text":"Simulation Menu When in a non-Headless Simulation, a menu can be accessed by clicking on the \"hamburger\" menu icon in the bottom left. In an Interactive Simulation, the \"Play\" and \"Pause\" buttons are found to the right of the menu icon. Table of Contents Info Menu Controls Menu Interactive Menu Sensors Menu Bridge Menu Camera Button Vehicle Selection Info Menu top # This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. Controls Menu top # This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. See Keyboard Shortcuts for more details. Interactive Menu top # This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the Simulation while the Simulation is playing. Sensors Menu top # This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized. See Sensor Visualization for more details. Bridge Menu top # This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details. Camera Button top # The camera icon in the bottom right indicates if the view is currently a follow camera or free-roam camera. A follow camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. A free-roam camera can be moved freely around the map using all of the camera controls. Clicking the camera button toggles between the 2 camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Vehicle Selection top # The vehicle listed in the bottom right is the current active vehicle. This vehicle is affected by keyboard input. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Menu Items"},{"location":"simulation-menu/#info-menu","text":"This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu.","title":"Info Menu"},{"location":"simulation-menu/#controls-menu","text":"This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. See Keyboard Shortcuts for more details.","title":"Controls Menu"},{"location":"simulation-menu/#interactive-menu","text":"This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the Simulation while the Simulation is playing.","title":"Interactive Menu"},{"location":"simulation-menu/#sensors-menu","text":"This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized. See Sensor Visualization for more details.","title":"Sensors Menu"},{"location":"simulation-menu/#bridge-menu","text":"This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details.","title":"Bridge Menu"},{"location":"simulation-menu/#camera-button","text":"The camera icon in the bottom right indicates if the view is currently a follow camera or free-roam camera. A follow camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. A free-roam camera can be moved freely around the map using all of the camera controls. Clicking the camera button toggles between the 2 camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle.","title":"Camera Button"},{"location":"simulation-menu/#vehicle-selection","text":"The vehicle listed in the bottom right is the current active vehicle. This vehicle is affected by keyboard input. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Vehicle Selection"},{"location":"simulations-tab/","text":"Web UI Simulations Tab Explanation A Simulation can be in the following states. A Simulation will have a Valid status if it can be run A Simulation can become Invalid for several reasons: A Map or Vehicle has become Invalid since the Simulation was created A Vehicle with a bridge is missing a Bridge Connection String How to Add/Edit a Simulation # Click the Add new button or the pencil icon The dialogue that opens has 4 tabs which change the parameters of the Simulation: General Simulation Name : The name of the Simulation Select Cluster : From the dropdown, select the cluster of computers that will run the Simulation API Only : Check this if the Simulation will be controlled through the Python API. Checking this will disable most other options as they will be set through the API Headless Mode : Check this if it is not necessary to render the Simulator in the main window. Checking this will improve performance. Map & Vehicles Interactive Mode : Check this to enable Simulation controls Select Map : From the dropdown, choose the map that will be used Select Vehicle : From the dropdown, choose the vehicle that will be spawned Bridge Connection String : If the chosen vehicle has a Bridge Type, an IP:port must be provided to the bridge host + : Adds an additional vehicle. Vehicles will spawn in Spawn Info positions of the map in order Traffic Use Predefined Seed : Check this and enter a seed [int] which will be used deterministically control NPCs Enable NPC : Check this to have NPC vehicles spawn at the beginning of the Simulation Enable Pedestrians : Check this to have Pedestrians spawn at the beginning of the Simulation Weather Time of Day : Set the time of day for the Simulation Rain : [0-1] set how much rain should fall Wetness : [0-1] set how wet the roads should be Fog : [0-1] set thick fog there should be Cloudiness : [0-1] set how much cloud cover thee should be","title":"Simulations"},{"location":"simulations-tab/#how-to-addedit-a-simulation","text":"Click the Add new button or the pencil icon The dialogue that opens has 4 tabs which change the parameters of the Simulation: General Simulation Name : The name of the Simulation Select Cluster : From the dropdown, select the cluster of computers that will run the Simulation API Only : Check this if the Simulation will be controlled through the Python API. Checking this will disable most other options as they will be set through the API Headless Mode : Check this if it is not necessary to render the Simulator in the main window. Checking this will improve performance. Map & Vehicles Interactive Mode : Check this to enable Simulation controls Select Map : From the dropdown, choose the map that will be used Select Vehicle : From the dropdown, choose the vehicle that will be spawned Bridge Connection String : If the chosen vehicle has a Bridge Type, an IP:port must be provided to the bridge host + : Adds an additional vehicle. Vehicles will spawn in Spawn Info positions of the map in order Traffic Use Predefined Seed : Check this and enter a seed [int] which will be used deterministically control NPCs Enable NPC : Check this to have NPC vehicles spawn at the beginning of the Simulation Enable Pedestrians : Check this to have Pedestrians spawn at the beginning of the Simulation Weather Time of Day : Set the time of day for the Simulation Rain : [0-1] set how much rain should fall Wetness : [0-1] set how wet the roads should be Fog : [0-1] set thick fog there should be Cloudiness : [0-1] set how much cloud cover thee should be","title":"How to Add/Edit a Simulation"},{"location":"total-control-calibration-criteria/","text":"Total Control Calibration Criteria back This page has control calibration criteria JSON. { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 27, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 15, \"steering\": -65, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 } ] } }","title":"Total Control Calibration Criteria"},{"location":"vehicles-tab/","text":"Web UI Vehicles Tab Explanation A Vehicle can be in the following states. A Vehicle with a local URL or if it has already been downloaded will have a Valid status. If the URL to the Vehicle assetbundle is not local and the assetbundle is not in the local database, then the assetbundle needs to be downloaded. Currently only 1 assetbundle is downloaded at a time. If an assetbundle is downloading, the Vehicle will show a GREY dot and the status will be Downloading with the download percentage. If another assetbundle is downloading, the icon will be ORANGE and the status will be Downloading without a percentage. A downloading Vehicle can be interrupted by pressing the stop button. If the Vehicle is not usable in a Simulation it will have an Invalid status. This can be because the local assetbundle is not usable or the download was interrupted. Where to find Vehicles top # Vehicle assetbundles are available from our content website . When adding a vehicle, the link to the appropriate assetbundle can be entered as the URL or the assetbundle can be downloaded manually and the local path can be entered. The calibration files for the vehicles are available in the same page. Please see the relevant doc for instructions on how to add a vehicle to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware Example JSON configurations can be found on these pages: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON How to add a Vehicle top # Click the Add new button In the dialogue that opens, enter the name of the vehicle and the URL to the assetbundle. This can be a URL to a location in the cloud or to a location on a local drive. If the URL is not local, the assetbundle will be downloaded to the local database. How to Edit a Vehicle top # Click the pencil icon In the dialogue that opens, the name of the vehicle can be changed and the URL to the assetbundle. If the URL is changed, the assetbundle in the database will be updated (downloaded if necessary) How to Change the Configuration of a Vehicle top # Click the wrench icon In the dialogue that opens, the bridge type of the vehicle and the JSON configuration of the vehicle can be entered A JSON beautifier is recommended to make the configuration more readable The bridge type determines how the sensor data will be formatted and sent to an AD stack. All bridge types other than No bridge will require a Bridge Connection String when adding a vehicle to a simulation. This string includes the IP of the AD Stack and the open port (ex. 192.168.1.100:9090 ) The JSON determines what sensors are on the vehicle, where they are located, what topic they will publish data under, and what control inputs the vehicle accepts See below for an example JSON configuration See Sensor Parameters for full defintions of all availble sensors and how to add them to a vehicle. Bridge Types top # No bridge : This is bridge available by default. Does not require any additional information while setting up Simulation. Used when there is no need to connect to an AD Stack. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). ROS1 Bridge requires IP address and port number while setting up Simulation Configuration. ROS Apollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires IP address and port number while setting up Simulation Configuration. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires IP address, port number while setting up Simulation Configuration. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge requires IP address, port number while setting up Simulation Configuration. Example JSON top # This is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic, a LIDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic, a Manual Control input which allows the keyboard input to control the car, and a Vehicle Control input which subscribes to the Autoware AD Stack control commands. [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Vehicles"},{"location":"vehicles-tab/#where-to-find-vehicles","text":"Vehicle assetbundles are available from our content website . When adding a vehicle, the link to the appropriate assetbundle can be entered as the URL or the assetbundle can be downloaded manually and the local path can be entered. The calibration files for the vehicles are available in the same page. Please see the relevant doc for instructions on how to add a vehicle to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware Example JSON configurations can be found on these pages: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON","title":"Where to find Vehicles"},{"location":"vehicles-tab/#how-to-add-a-vehicle","text":"Click the Add new button In the dialogue that opens, enter the name of the vehicle and the URL to the assetbundle. This can be a URL to a location in the cloud or to a location on a local drive. If the URL is not local, the assetbundle will be downloaded to the local database.","title":"How to add a Vehicle"},{"location":"vehicles-tab/#how-to-edit-a-vehicle","text":"Click the pencil icon In the dialogue that opens, the name of the vehicle can be changed and the URL to the assetbundle. If the URL is changed, the assetbundle in the database will be updated (downloaded if necessary)","title":"How to Edit a Vehicle"},{"location":"vehicles-tab/#how-to-change-the-configuration-of-a-vehicle","text":"Click the wrench icon In the dialogue that opens, the bridge type of the vehicle and the JSON configuration of the vehicle can be entered A JSON beautifier is recommended to make the configuration more readable The bridge type determines how the sensor data will be formatted and sent to an AD stack. All bridge types other than No bridge will require a Bridge Connection String when adding a vehicle to a simulation. This string includes the IP of the AD Stack and the open port (ex. 192.168.1.100:9090 ) The JSON determines what sensors are on the vehicle, where they are located, what topic they will publish data under, and what control inputs the vehicle accepts See below for an example JSON configuration See Sensor Parameters for full defintions of all availble sensors and how to add them to a vehicle.","title":"How to Change the Configuration of a Vehicle"},{"location":"vehicles-tab/#bridge-types","text":"No bridge : This is bridge available by default. Does not require any additional information while setting up Simulation. Used when there is no need to connect to an AD Stack. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). ROS1 Bridge requires IP address and port number while setting up Simulation Configuration. ROS Apollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires IP address and port number while setting up Simulation Configuration. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires IP address, port number while setting up Simulation Configuration. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge requires IP address, port number while setting up Simulation Configuration.","title":"Bridge Types"},{"location":"vehicles-tab/#example-json","text":"This is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic, a LIDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic, a Manual Control input which allows the keyboard input to control the car, and a Vehicle Control input which subscribes to the Autoware AD Stack control commands. [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example JSON"}]}