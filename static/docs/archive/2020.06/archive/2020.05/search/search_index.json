{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This is the documentation website for the LGSVL Simulator. You can subscribe to our email newsletter here . Visit our website here: https://www.lgsvlsimulator.com Visit our Github here: https://github.com/lgsvl/simulator Quick Start # Getting Started Keyboard Shortcuts Release notes User Interface Maps Vehicles Sensor Parameters Clusters Simulations Configuration file and command line parameters Integration with AD # Running with Autoware.AI Instructions Sample sensor configuration Running with Autoware.Auto Instructions Sample sensor configuration Running with Apollo 5.0 Instructions Sample sensor configuration Running with Apollo 3.0 Instructions Sample sensor configuration LGSVL ROS/ROS2 Message Definitions The lgsvl_msgs package Ground truth obstacles Viewing and subscribing to ground truth data Sample sensor configuration for data collection Python API # Python API guide Python API quickstart examples Python API use case examples How to run a scenario Tutorials # Reinforcement learning with OpenAI Gym Deep learning lane following model How to create a simple ROS2-based AD stack with LGSVL Simulator Advanced # Map annotation Build Instructions Adding assets How to add a new ego vehicle NPC map navigation Plugins Sensor plugins LiDAR plugin Controllable plugins Traffic behavior plugins Vehicle dynamics Point cloud Point cloud import Point cloud rendering Support # Frequently Asked Questions Contributing Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Home"},{"location":"#quick-start","text":"Getting Started Keyboard Shortcuts Release notes User Interface Maps Vehicles Sensor Parameters Clusters Simulations Configuration file and command line parameters","title":"Quick Start"},{"location":"#integration-with-ad","text":"Running with Autoware.AI Instructions Sample sensor configuration Running with Autoware.Auto Instructions Sample sensor configuration Running with Apollo 5.0 Instructions Sample sensor configuration Running with Apollo 3.0 Instructions Sample sensor configuration LGSVL ROS/ROS2 Message Definitions The lgsvl_msgs package Ground truth obstacles Viewing and subscribing to ground truth data Sample sensor configuration for data collection","title":"Integration with AD"},{"location":"#python-api","text":"Python API guide Python API quickstart examples Python API use case examples How to run a scenario","title":"Python API"},{"location":"#tutorials","text":"Reinforcement learning with OpenAI Gym Deep learning lane following model How to create a simple ROS2-based AD stack with LGSVL Simulator","title":"Tutorials"},{"location":"#advanced","text":"Map annotation Build Instructions Adding assets How to add a new ego vehicle NPC map navigation Plugins Sensor plugins LiDAR plugin Controllable plugins Traffic behavior plugins Vehicle dynamics Point cloud Point cloud import Point cloud rendering","title":"Advanced"},{"location":"#support","text":"Frequently Asked Questions Contributing","title":"Support"},{"location":"#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"add-new-ego-vehicle/","text":"How to Add a New Ego Vehicle This tutorial works with Simulator Release 2019.05 This document will describe how to create a new ego vehicle in the LGSVL Simulator. Video # ( Link ) Getting Started # The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive . Create and Name Your Vehicle # Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later. Add Child Components # Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab. Apply the Correct References # We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references. Vehicle Controller script # For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center Car HeadLights script # For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot Car Input Controller script # For the Car Input Controller script, reference the DriverCamera. Force Feedback script # For the Force Feedback script, reference the FL and FR WheelColliders. Vehicle Animation Manager script # For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator) Vehicle Position Resetter script # For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray. Agent Setup script # For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes Adaptation to Cluster Simulation # Cluster Simulation provides synchronization of every Rigidbody component in the vehicle, no changes are required in the prefab. For advanced solutions in the cluster simulations, like synchronization of other components, refer to the Cluster Simulation Introduction Final Steps # Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle! Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"How to add a new ego vehicle"},{"location":"add-new-ego-vehicle/#video","text":"( Link )","title":"Video"},{"location":"add-new-ego-vehicle/#getting-started","text":"The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive .","title":"Getting Started"},{"location":"add-new-ego-vehicle/#create-and-name-your-vehicle","text":"Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later.","title":"Create and Name Your Vehicle"},{"location":"add-new-ego-vehicle/#add-child-components","text":"Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab.","title":"Add Child Components"},{"location":"add-new-ego-vehicle/#apply-the-correct-references","text":"We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references.","title":"Apply the Correct References"},{"location":"add-new-ego-vehicle/#vehicle-controller-script","text":"For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center","title":"Vehicle Controller script"},{"location":"add-new-ego-vehicle/#car-headlights-script","text":"For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot","title":"Car HeadLights script"},{"location":"add-new-ego-vehicle/#car-input-controller-script","text":"For the Car Input Controller script, reference the DriverCamera.","title":"Car Input Controller script"},{"location":"add-new-ego-vehicle/#force-feedback-script","text":"For the Force Feedback script, reference the FL and FR WheelColliders.","title":"Force Feedback script"},{"location":"add-new-ego-vehicle/#vehicle-animation-manager-script","text":"For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator)","title":"Vehicle Animation Manager script"},{"location":"add-new-ego-vehicle/#vehicle-position-resetter-script","text":"For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray.","title":"Vehicle Position Resetter script"},{"location":"add-new-ego-vehicle/#agent-setup-script","text":"For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes","title":"Agent Setup script"},{"location":"add-new-ego-vehicle/#adaptation-to-cluster-simulation","text":"Cluster Simulation provides synchronization of every Rigidbody component in the vehicle, no changes are required in the prefab. For advanced solutions in the cluster simulations, like synchronization of other components, refer to the Cluster Simulation Introduction","title":"Adaptation to Cluster Simulation"},{"location":"add-new-ego-vehicle/#final-steps","text":"Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle!","title":"Final Steps"},{"location":"add-new-ego-vehicle/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"api-example-descriptions/","text":"Python API Use Case Examples The LGSVL Simulator teams has created sample Python scripts that use the LGSVL Simulator Python API to test specific scenarios or perform certain tasks. These example scripts can be found on our Github here . Please contact us if you would like to contribute examples that you are using, or submit a pull request . Scenarios top # We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started. Vehicle Following top # Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane. Encroaching Oncoming Vehicle top # Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts. Other Uses top # Collecting data in KITTI format top # Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip Automated Driving System Test Cases top # The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Python API use case examples"},{"location":"api-example-descriptions/#scenarios","text":"We have created basic sample scenarios using the Python API. See here for a step-by-step guide on how to run one of these scenarios. Several are based on the sample Test Cases from NHTSA . The below scenarios assume that the simulator can be connected to an instance of Apollo 5.0. See the guide for getting connected with Apollo 5.0 here . The Apollo modules that need to be started are shown below (localization, perception, planning, prediction, routing, traffic light, transform, control): It is recommended to start Apollo and the modules before running a scenario. Apollo's destination can be set after Localization and Routing have been started.","title":"Scenarios"},{"location":"api-example-descriptions/#vehicle-following","text":"Scripts: Perform Vehicle Following This scenario simulates the EGO vehicle approaching a slower NPC from behind. The EGO is expected to accelerate up to the speed limit and catch up to the NPC. For this scenario, the destination is the end of the lane.","title":"Vehicle Following"},{"location":"api-example-descriptions/#encroaching-oncoming-vehicle","text":"Scripts: Detect and Respond to Encroaching Oncoming Vehicle This scenario simulates the EGO vehicle approaching an oncoming NPC that is half in the EGO's lane making a collision imminent. The EGO is expected to avoid a collision. Here the NPC uses the waypoint system to define its path. With waypoints, the NPC ignores other traffic and does not attempt to avoid collisions. For this scenario, the destinaion is the end of the lane. The same destination can be used as the Vehicle Following scripts.","title":"Encroaching Oncoming Vehicle"},{"location":"api-example-descriptions/#other-uses","text":"","title":"Other Uses"},{"location":"api-example-descriptions/#collecting-data-in-kitti-format","text":"Script: kitti_parser.py This script shows an example of collecting data in the KITTI format. This data can be used to train for detecting vehicles in images. This script spawns the ego vehicle in a random position in the San Francisco map. Then a number of NPC vehicles are randomly spawned in front of the ego vehicle. Camera and ground truth data is saved in the KITTI format. This data can be used to train for detecting vehicles in images. For more information on KITTI please see: http://www.cvlibs.net/datasets/kitti/index.php The data format is defined in a README file downloadable from: https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip","title":"Collecting data in KITTI format"},{"location":"api-example-descriptions/#automated-driving-system-test-cases","text":"The United States National Highway Traffic Safety Administration released a report describing a framework for establishing sample preliminary tests. The report is available online: A Framework for Automated Driving System Testable Cases and Scenarios We created several of the described tests available here: NHTSA-sample-tests These tests run the ADS at different speeds. To accomplish this with Apollo, the speed limit in the HD map of the appropriate lanes needs to be adjusted and the planning configuration should also be changed to limit Apollo's top speed. The ADS destination is described in the report. For our implementation of the Perform Lane Change tests, the same destination as the above Overtaker and Traffic Jam scenarios is used.","title":"Automated Driving System Test Cases"},{"location":"api-how-to-run-scenario/","text":"How To Run a Scenario or Test Case The following steps detail how to run the Vehicle Following scenario. This scenario and other example scenarios can be found on our examples page . Install Simulator Python API by navigating to the Api submodule directory under the simulator repository: pip3 install --user . Start the simulator. Set environment variables SIMULATOR_HOST and BRIDGE_HOST SIMULATOR_HOST is where the simulator will be run. The default value for this is \"localhost\" and does not need to be set if the simulator is running on the same machine that the python script will be run from. BRIDGE_HOST is where the AD stack will be run. This is relative to where the simulator is run. The default value is \"localhost\" which is the same machine as the simulator. For example, if computer A will run the simulator and computer B will run the AD stack SIMULATOR_HOST should be set to the IP of computer A BRIDGE_HOST should be set to the IP of computer B To set the variables for the current terminal window use export SIMULATOR_HOST=192.168.1.100 Start your AD stack. The example scripts are written for Apollo 5.0. See below for how to edit the scripts to work with other AD stacks. Select the MKZ as the vehicle SingleLaneRoad for the map Start all modules and the bridge (if relevant) Run the script ./VF_S_25.py Set the destination for the AD stack. For this scenario, the destination is the end of the current lane. The AV should start driving forward towards the NPC. It should avoid crashing into the NPC. How to Edit the EGO vehicle # In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to run a scenario"},{"location":"api-how-to-run-scenario/#how-to-edit-the-ego-vehicle","text":"In each of the example scenarios and test cases, there is a section that setups up the EGO vehicle: If using a different AD stack, the vehicle type needs to be changed. Change the agent name (orange string) to what is desired. e.g. For Autoware it might be \"Lexus2016RXHybrid (Autoware)\"","title":"How to Edit the EGO vehicle"},{"location":"api-quickstart-descriptions/","text":"Python API Quickstart Script Descriptions This document describes the example Python scripts that use the LGSVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LIDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints with fixed wait times and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the npc is printed 24-ego-drive-straight-non-realtime.py : How to run the simulator at non-realtime. 25-waypoint-flying-npc.py : How to use waypoints to define customized motion for npc. 26-npc-trigger-waypoints.py : How to use trigger waypoints that pause npc motion until an ego vehicle approaches. 27-control-traffic-lights.py : How to get and set the control policy of a controllable object (e.g., changing a traffic light signal) 28-control-traffic-cone.py : How to add and move a controllable object (e.g. a traffic cone) 29-add-random-agents.py : How to use random npcs and pedestrians in a simulation 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API quickstart examples"},{"location":"apollo-instructions/","text":"Running Apollo 3.0 with LGSVL Simulator This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing Nvidia Docker Pulling LGSVL Docker image Cloning the Repository Building Apollo and ROSbridge Launching Apollo alongside the simulator Adding a Vehicle Adding an HD Map Copyright and License Getting Started top # The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Prerequisites top # Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing Nvidia Docker top # Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image top # LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo Cloning the Repository top # This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git Building Apollo and ROSbridge top # Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make Launching Apollo alongside the simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap.sh Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Create a Simulation with the BorregasAve map and the Jaguar2015XE (Apollo 3.0) vehicle. Enter localhost:9090 for the Bridge Connection String Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Jaguar2015XE vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # The default vehicles have their calibration files included in the LGSVL Branch of Apollo 3.0 . Adding an HD Map top # The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 . Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"apollo-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle.","title":"Getting Started"},{"location":"apollo-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo-instructions/#setup","text":"","title":"Setup"},{"location":"apollo-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"apollo-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"apollo-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo","title":"Pulling LGSVL Docker image"},{"location":"apollo-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Cloning the Repository"},{"location":"apollo-instructions/#building-apollo-and-rosbridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make","title":"Building Apollo and ROSbridge"},{"location":"apollo-instructions/#launching-apollo-alongside-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap.sh Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Create a Simulation with the BorregasAve map and the Jaguar2015XE (Apollo 3.0) vehicle. Enter localhost:9090 for the Bridge Connection String Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Jaguar2015XE vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongside the simulator"},{"location":"apollo-instructions/#adding-a-vehicle","text":"The default vehicles have their calibration files included in the LGSVL Branch of Apollo 3.0 .","title":"Adding a Vehicle"},{"location":"apollo-instructions/#adding-an-hd-map","text":"The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding an HD Map"},{"location":"apollo-instructions/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"apollo-json-example/","text":"Example JSON Configuration for an Apollo 3.0 Vehicle Bridge Type top # ROS Apollo Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 Lidar /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Sample sensor configuration"},{"location":"apollo-json-example/#bridge-type","text":"ROS Apollo","title":"Bridge Type"},{"location":"apollo-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/velodyne64/compensator/PointCloud2 Lidar /apollo/sensor/camera/traffic/image_short/compressed Main Camera /apollo/sensor/camera/traffic/image_long/compressed Telephoto Camera","title":"Published Topics"},{"location":"apollo-json-example/#subscribed-topcs","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"apollo-json-example/#complete-json-configuration","text":"[ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/velodyne64/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_short/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/traffic/image_long/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"apollo-master-instructions/","text":"Running latest Apollo with LGSVL Simulator This instruction is tested after the last commit enabling LGSVL Simulator with latest Apollo master. Commits after that are assumed to work as well, but not guaranteed. Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing Nvidia Docker Cloning the Repository Building Apollo and bridge Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Copyright and License Getting Started top # The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here . Prerequisites top # Ubuntu 16.04 or later (Ubuntu 18.04 is preferred) Nvidia graphics card (required for Perception) Nvidia proprietary driver (>=410.48) must be installed Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing Nvidia Docker top # Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Cloning the Repository top # Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo Building Apollo and bridge top # Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~141 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2 ) --ram_utilization_factor 70\" Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master: Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later. Adding an HD Map top # Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions . Copyright and License top # Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"apollo-master-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here .","title":"Getting Started"},{"location":"apollo-master-instructions/#prerequisites","text":"Ubuntu 16.04 or later (Ubuntu 18.04 is preferred) Nvidia graphics card (required for Perception) Nvidia proprietary driver (>=410.48) must be installed","title":"Prerequisites"},{"location":"apollo-master-instructions/#setup","text":"","title":"Setup"},{"location":"apollo-master-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo-master-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"apollo-master-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.64 Driver Version: 440.64 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A | | 27% 29C P8 7W / 180W | 579MiB / 8117MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1745 G /usr/lib/xorg/Xorg 40MiB | | 0 1862 G /usr/bin/gnome-shell 49MiB | | 0 4409 G /usr/lib/xorg/Xorg 223MiB | | 0 4545 G /usr/bin/gnome-shell 140MiB | | 0 4962 G ...uest-channel-token=10798087356903621100 27MiB | | 0 9570 G /proc/self/exe 50MiB | | 0 17619 G ...uest-channel-token=14399957398263092148 40MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"apollo-master-instructions/#cloning-the-repository","text":"Clone latest Apollo using the following command: git clone https://github.com/ApolloAuto/apollo","title":"Cloning the Repository"},{"location":"apollo-master-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build Apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~141 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2 ) --ram_utilization_factor 70\" Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Building Apollo and bridge"},{"location":"apollo-master-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. ./scripts/bootstrap_lgsvl.sh Launch bridge (inside docker container): ./scripts/bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo Dreamview in a browser by navigating to: localhost:8888 NEW for Apollo Master: Select the Mkz Lgsvl setup mode, from the menu to the left of the vehicle menu. Select the Lincoln2017MKZ LGSVL vehicle and Borregas Ave map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"apollo-master-instructions/#adding-a-vehicle","text":"Only calibration files for Lincoln 2017 MKZ is included in the latest Apollo at this time. More calibration files map be added later.","title":"Adding a Vehicle"},{"location":"apollo-master-instructions/#adding-an-hd-map","text":"Only HD map files for Borregas Ave is included in the the latest Apollo at this time. More HD maps may be added later. You can also download additional maps from here , and manually add them to /apollo/modules/map/data/ . Or you can export your own map from our map annotation tool , name it base_map.bin and then generate routing_map and sim_map following these instructions .","title":"Adding an HD Map"},{"location":"apollo-master-instructions/#copyright-and-license","text":"Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"apollo5-0-instructions/","text":"Running Apollo 5.0 with LGSVL Simulator This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents Getting Started Prerequisites Setup Docker Installing Docker CE Installing Nvidia Docker Pulling LGSVL Docker image Cloning the Repository Building Apollo and bridge Launching Apollo alongisde the Simulator Adding a Vehicle Adding an HD Map Copyright and License Getting Started top # The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here . Prerequisites top # Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup top # Docker top # Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE top # To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps . Installing Nvidia Docker top # Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image top # LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo-5.0 Cloning the Repository top # This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git Building Apollo and bridge top # Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2 ) --ram_utilization_factor 70\" Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 . Launching Apollo alongside the Simulator top # Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. bootstrap.sh Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090 Adding a Vehicle top # The default vehicles have their calibration files included in the LGSVL Branch of Apollo 5.0 . Adding an HD Map top # The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 . Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"apollo5-0-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LGSVL Simulator. If you have not already set up the simulator, please do so first by following the instructions here .","title":"Getting Started"},{"location":"apollo5-0-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo5-0-instructions/#setup","text":"","title":"Setup"},{"location":"apollo5-0-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo5-0-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . NOTE Apollo does not work if the docker is started with sudo . We suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"apollo5-0-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"apollo5-0-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo-5.0","title":"Pulling LGSVL Docker image"},{"location":"apollo5-0-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-5.0.git","title":"Cloning the Repository"},{"location":"apollo5-0-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo (optimized, not debug, with GPU support): ./apollo.sh build_opt_gpu NOTE The Apollo build may fail on machines with less than 1GB of RAM per CPU core due to aggressive parallelization in the build, as discussed in Apollo issue 7719 . If the build fails, either re-start it until it succeeds, or try changing line ~150 of function build() in apollo.sh as follows: Replace this: JOB_ARG=\"--jobs=$(nproc) --ram_utilization_factor 80\" With this, which will use all but two cores for the build: JOB_ARG=\"--jobs=$(expr $(nproc) - 2 ) --ram_utilization_factor 70\" Alternatively, you can analyze top while building, and decide how many jobs to enable to avoid running out of memory; then set a specific value in JOB_ARG , e.g. --jobs=6 .","title":"Building Apollo and bridge"},{"location":"apollo5-0-instructions/#launching-apollo-alongide-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. bootstrap.sh Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Create a Simulation the BorregasAve map and Lincoln2017MKZ (Apollo 5.0) vehicle Enter localhost:9090 as the Bridge Connection String (Optional) Enable Traffic and Pedestrians (Optional) Set the Time of Day and weather settings Submit the Simulation Select the created Simulation and click \"Play\" Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the Lincoln2017MKZ vehicle and BorregasAve map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container). If you are using ufw, it is easiest to completely disable the firewall to allow connections sudo ufw disable If that is not possible, add the following rules: These are required even if running the simulator and Apollo on the same machine sudo ufw allow 8888 sudo ufw allow 9090","title":"Launching Apollo alongisde the Simulator"},{"location":"apollo5-0-instructions/#adding-a-vehicle","text":"The default vehicles have their calibration files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding a Vehicle"},{"location":"apollo5-0-instructions/#adding-an-hd-map","text":"The default maps have their HD map files included in the LGSVL Branch of Apollo 5.0 .","title":"Adding an HD Map"},{"location":"apollo5-0-instructions/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"apollo5-0-json-example/","text":"Example JSON Configuration for an Apollo 5.0 Vehicle Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera Subscribed Topics top # Topic Sensor Name /apollo/control Apollo Car Control Complete JSON Configuration top # [ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Sample sensor configuration"},{"location":"apollo5-0-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"apollo5-0-json-example/#published-topics","text":"Topic Sensor Name /apollo/canbus/chassis CAN Bus /apollo/sensor/gnss/best_pose GPS /apollo/sensor/gnss/odometry GPS Odometry /apollo/sensor/gnss/ins_stat GPS INS Status /apollo/sensor/gnss/imu IMU /apollo/sensor/gnss/corrected_imu IMU /apollo/sensor/conti_radar Radar /apollo/sensor/lidar128/compensator/PointCloud2 Lidar /apollo/sensor/camera/front_6mm/image/compressed Main Camera /apollo/sensor/camera/front_12mm/image/compressed Telephoto Camera","title":"Published Topics"},{"location":"apollo5-0-json-example/#subscribed-topics","text":"Topic Sensor Name /apollo/control Apollo Car Control","title":"Subscribed Topics"},{"location":"apollo5-0-json-example/#complete-json-configuration","text":"[ { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/apollo/canbus/chassis\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/best_pose\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/odometry\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/apollo/sensor/gnss/ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/apollo/sensor/gnss/imu\", \"Frame\": \"imu\", \"CorrectedTopic\": \"/apollo/sensor/gnss/corrected_imu\", \"CorrectedFrame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/apollo/sensor/conti_radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/apollo/sensor/lidar128/compensator/PointCloud2\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Telephoto Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 10, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_12mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": -4, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Apollo Car Control\", \"params\": { \"Topic\": \"/apollo/control\" } } ]","title":"Complete JSON Configuration"},{"location":"assets/","text":"Adding Assets The main repository for the LGSVL Simulator does not contain and environments or vehicles. Currently there are several open-source examples. Environments: CubeTown SingleLaneRoad Shalun SanFrancisco Vehicles: Jaguar2015XE Table of Contents Adding an Asset Building an Asset Check Asset Consistency Adding an Asset top # Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab Building an Asset top # Assets are built using the same build script as the simulator. Follow the build instructions through step 17. IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles Check Asset Consistency top # There is a tool to check if there are any inconsistencies in assets. Run Check... in Unity : Simulator -> Check... The script checks if the project structure is correct: assets are named correctly, assets are in the correct location, etc. This will generate a list of warnings and errors.","title":"Adding assets"},{"location":"assets/#adding-an-asset","text":"Assets need to be cloned into a specific location in the project: simulator/Assets/External/Environments for Environments simulator/Assets/External/Vehicles for Vehicles Clone the desired asset into the appropriate folder. Do not change the name of the folder that the asset is cloned into, it must match the name of the asset. For environments: simulator/Assets/External/Environments/Mars must contain simulator/Assets/External/Environments/Mars/Mars.unity For vehicles: simulator/Assets/External/Vehicles/Rover must contain simulator/Assets/External/Vehicles/Rover/Rover.prefab","title":"Adding an Asset"},{"location":"assets/#building-an-asset","text":"Assets are built using the same build script as the simulator. Follow the build instructions through step 17. IMPORTANT Windows and Linux support must be installed with Unity to build assetbundles","title":"Building an Asset"},{"location":"assets/#check-asset-consistency","text":"There is a tool to check if there are any inconsistencies in assets. Run Check... in Unity : Simulator -> Check... The script checks if the project structure is correct: assets are named correctly, assets are in the correct location, etc. This will generate a list of warnings and errors.","title":"Check Asset Consistency"},{"location":"autoware-auto-instructions/","text":"Autoware.Auto with LGSVL Simulator Table of Contents Overview Setup Requirements Installing Docker CE Installing Nvidia Docker Installing Autoware.auto Simulator Installation Install Ros2 dashing Install Ros2 Web Bridge Run Simulator alongside Autoware.Auto Copyright and License Overview top # This guide describes setting up and using Autoware.Auto with the LGSVL simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented. Setup top # Requirements top # Linux operating system Nvidia graphics card Installing Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Installing Nvidia Docker top # Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Install nvidia docker . Note: For docker 19.03 and newer nvidia GPUs are natively supported as devices in docker runtime, and nvidia-docker2 is deprecated, however, because Autoware.auto uses the --runtime nvidia argument nvidia-docker2 will need to be installed even for newer docker versions. Installing Autoware.auto top # Follow the installation and development setup guide for Autoware.auto. Simulator installation top # Download and extract the latest simulator release (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user . Install ROS2 dashing top # Follow these steps . Install the ROS2 Web Bridge top # Clone the ROS2 web bridge cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge ade enter git clone -b 0.2.7 https://github.com/RobotWebTools/ros2-web-bridge.git Install nodejs v10 curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - sudo apt-get install -y nodejs cd ros2-web-bridge npm install # If node.js packages are not installed, run this. Run Simulator alongside Autoware.Auto top # The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers: cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build # If you want to use autoware_auto_msgs, ros2-web-bridge needs compiled them. export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64/ source ~/AutowareAuto/install/local_setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the LGSVL Simulator by launching the executable and click on the button to open the web UI. In the Vehicles tab look for Lexus2016RXHybrid . If not available download it from here and follow these instructions to add it. Click on the wrench icon for the Lexus vehicle: Change the bridge type to ROS2 Use the following JSON configuration Autoware Auto JSON Example Switch to the Simulations tab and click the Add new button: Enter a name and switch to the Map & Vehicles tab Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu. In the bridge connection box to the right enter the bridge address (default: localhost:9090 ) Click submit Select the simulation and press the play button in the bottom right corner of the screen Launch ROS2 web bridge in a new terminal: NOTE Node.js will need to be reinstalled in the container every time it is started ade enter # ros2 web bridge should be run in ade environment. cd ros2-web-bridge source ~/AutowareAuto/install/local_setup.bash node bin/rosbridge.js You should now be able to see the lidar point cloud in rviz (see image below). If the pointcloud is not visible make sure the fixed frame is set to velodyne_front and that a PointCloud2 message is added which listens on the /points_raw topic. Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"autoware-auto-instructions/#general","text":"This guide describes setting up and using Autoware.Auto with the LGSVL simulator. As Autoware.Auto is still under-development, full self-driving is not yet possible. This guide will focus on running individual modules which have been implemented.","title":"Overview"},{"location":"autoware-auto-instructions/#setup","text":"","title":"Setup"},{"location":"autoware-auto-instructions/#requirements","text":"Linux operating system Nvidia graphics card","title":"Requirements"},{"location":"autoware-auto-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user.","title":"Installing Docker CE"},{"location":"autoware-auto-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Install nvidia docker . Note: For docker 19.03 and newer nvidia GPUs are natively supported as devices in docker runtime, and nvidia-docker2 is deprecated, however, because Autoware.auto uses the --runtime nvidia argument nvidia-docker2 will need to be installed even for newer docker versions.","title":"Installing Nvidia Docker"},{"location":"autoware-auto-instructions/#installing-autoware-auto","text":"Follow the installation and development setup guide for Autoware.auto.","title":"Installing Autoware.auto"},{"location":"autoware-auto-instructions/#simulator-installation","text":"Download and extract the latest simulator release (Optional) Download the latest PythonAPI release (make sure the release version matches the simulator) and install it using pip: cd PythonAPI pip3 install --user .","title":"Simulator Installation"},{"location":"autoware-auto-instructions/#install-ros2-dashing","text":"Follow these steps .","title":"Install Ros2 dashing"},{"location":"autoware-auto-instructions/#install-ros2-web-bridge","text":"","title":"Install Ros2 Web Bridge"},{"location":"autoware-auto-instructions/#run-simulator-alongside-autoware-auto","text":"The ROS2 web bridge allows the simulator and Autoware.auto to communicate. To test this connection we can visualize sensor data from the simulator in rviz2 (running in the Autoware.auto container). Start the Autoware.Auto containers: cd ~/adehome/AutowareAuto ade start -- --net=host --privileged # to allow connect to rosbridge Enter the container and start rviz2: ade enter cd ~/AutowareAuto colcon build # If you want to use autoware_auto_msgs, ros2-web-bridge needs compiled them. export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/nvidia/lib64/ source ~/AutowareAuto/install/local_setup.bash rviz2 -d /home/\"${USER}\"/AutowareAuto/install/autoware_auto_examples/share/autoware_auto_examples/rviz2/autoware.rviz Start the LGSVL Simulator by launching the executable and click on the button to open the web UI. In the Vehicles tab look for Lexus2016RXHybrid . If not available download it from here and follow these instructions to add it. Click on the wrench icon for the Lexus vehicle: Change the bridge type to ROS2 Use the following JSON configuration Autoware Auto JSON Example Switch to the Simulations tab and click the Add new button: Enter a name and switch to the Map & Vehicles tab Select a map from the drop down menu. If none are available follow this guide to get a map. Select the Lexus2016RXHybrid from the drop down menu. In the bridge connection box to the right enter the bridge address (default: localhost:9090 ) Click submit Select the simulation and press the play button in the bottom right corner of the screen Launch ROS2 web bridge in a new terminal: NOTE Node.js will need to be reinstalled in the container every time it is started ade enter # ros2 web bridge should be run in ade environment. cd ros2-web-bridge source ~/AutowareAuto/install/local_setup.bash node bin/rosbridge.js You should now be able to see the lidar point cloud in rviz (see image below). If the pointcloud is not visible make sure the fixed frame is set to velodyne_front and that a PointCloud2 message is added which listens on the /points_raw topic.","title":"Run Simulator alongside Autoware.Auto"},{"location":"autoware-auto-instructions/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"autoware-auto-json-example/","text":"Example JSON Configuration for an Autoware Auto Vehicle Bridge Type top # ROS2 Published Topics top # Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/vehicle_odom Vehicle Odometry Subscribed Topics top # Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State Complete JSON Configuration top # [ { \"type\": \"Transform\", \"name\": \"base_link\", \"transform\": { \"x\": -0.015, \"y\": 0.369, \"z\": -1.37, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"Vehicle State\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"Vehicle Odometry\", \"name\": \"Vehicle Odometry Sensor\", \"params\": { \"Topic\": \"/lgsvl/vehicle_odom\" } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" } ]","title":"Sample sensor configuration"},{"location":"autoware-auto-json-example/#bridge-type","text":"ROS2","title":"Bridge Type"},{"location":"autoware-auto-json-example/#published-topics","text":"Topic Sensor Name /lgsvl/state_report CAN Bus /gnss/fix GPS /lgsvl/gnss_odom GPS Odometry /imu/imu_raw IMU /lidar_front/points_raw LidarFront /lidar_rear/points_raw LidarRear /lgsvl/vehicle_odom Vehicle Odometry","title":"Published Topics"},{"location":"autoware-auto-json-example/#subscribed-topics","text":"Topic Sensor Name /lgsvl/vehicle_control_cmd Autoware Car Control /lgsvl/vehicle_state_cmd Autoware Auto Vehicle State","title":"Subscribed Topics"},{"location":"autoware-auto-json-example/#complete-json-configuration","text":"[ { \"type\": \"Transform\", \"name\": \"base_link\", \"transform\": { \"x\": -0.015, \"y\": 0.369, \"z\": -1.37, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/lgsvl/state_report\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gnss/fix\", \"Frame\": \"gnss\", \"IgnoreMapOrigin\": true }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 30.0, \"Topic\": \"/lgsvl/gnss_odom\", \"Frame\": \"odom\", \"ChildFrame\": \"base_link\", \"IgnoreMapOrigin\": true }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu/imu_raw\", \"Frame\": \"imu\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarFront\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_front/points_raw\", \"Frame\": \"lidar_front\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 1.498, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"LidarRear\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 2.0, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 20, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/lidar_rear/points_raw\", \"Frame\": \"lidar_rear\" }, \"parent\": \"base_link\", \"transform\": { \"x\": 0.022, \"y\": 1.49, \"z\": 0.308, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/lgsvl/vehicle_control_cmd\" } }, { \"type\": \"Vehicle State\", \"name\": \"Autoware Auto Vehicle State\", \"params\": { \"Topic\": \"/lgsvl/vehicle_state_cmd\" } }, { \"type\": \"Vehicle Odometry\", \"name\": \"Vehicle Odometry Sensor\", \"params\": { \"Topic\": \"/lgsvl/vehicle_odom\" } }, { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" } ]","title":"Complete JSON Configuration"},{"location":"autoware-instructions/","text":"Autoware.AI 1.12.0 with LGSVL Simulator The software and source code in this repository are intended only for use with the LGSVL simulator and should not be used in a real vehicle. Table of Contents General Setup Requirements Install Docker CE Install NVIDIA Container Toolkit Install LGSVL Simulator Install Autoware Launch Autoware Alongside LGSVL Simulator Driving by following vector map: Driving by following prerecorded waypoints: Adding a Vehicle Adding an HD Map Copyright and License General top # This guide goes through how to run Autoware.AI with the LGSVL simulator. In order to run Autoware with the LGSVL simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the LGSVL simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included. Setup top # Requirements top # Linux operating system NVIDIA graphics card Install Docker CE top # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user. Install NVIDIA Container Toolkit top # Before installing the NVIDIA Container Toolkit (nvidia-docker), make sure that you have an appropriate NVIDIA driver installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Install the NVIDIA Container Toolkit by following the instructions here . Install LGSVL Simulator top # Follow the instructions here . Install Autoware top # Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://gitlab.com/autowarefoundation/autoware.ai/docker.git If you are using the latest Docker and NVIDIA Container Toolkit, the docker/generic/run.sh script will need to be modified. To determine whether you need to do this run type nvidia-docker in a terminal. If you get output similar to: nvidia-docker is /usr/bin/nvidia-docker , the script will work as is. If not, then modify it as described below: In docker/generic/run.sh , find the following block at line 139: if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" RUNTIME=\"--runtime=nvidia\" fi Replace it with: DOCKER_VERSION=$(docker version --format '{{.Client.Version}}' | cut -d'.' -f1) if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" if [[ $DOCKER_VERSION -ge \"19\" ]] && ! type nvidia-docker; then RUNTIME=\"--gpus all\" else RUNTIME=\"--runtime=nvidia\" fi fi Launch Autoware Alongside LGSVL Simulator top # Run the Autoware 1.12.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.12.0 Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the LGSVL simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an inital pose or for localization but the current Autoware release (1.12.0) does not support GNSS coordinates outside of Japan. Fix for this is available in following pull requests: utilities#27 , common#20 , core_perception#26 These are not yet merged in Autoware master. Driving by following vector map: # To drive following the HD map follow these steps: - in rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. To follow the selected route launch these nodes: - Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. - Enable astar_avoid and velocity_set . - Enable pure_pursuit and twist_filter to start driving. Driving by following prerecorded waypoints: # A basic functionality of Autoware is to follow a prerecorded map while obeying traffic rules. To do this you will need to record a route first. Switch to the Computing tab and check the box for waypoint_saver . Make sure to select an appropriate location and file name by clicking on the app button. Now you can drive around the map using the keyboard. Once you are satisfied with your route, uncheck the box for waypoint_saver to end the route. To drive the route using Autoware: Enable waypoint_loader while making sure the correct route file is selected in the app settings. Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. Enable astar_avoid and velocity_set . Enable pure_pursuit and twist_filter to start driving. The ego vehicle should try to follow the waypoints at the velocity which they were originally recorded at. You can modify this velocity by manually editing the values csv file. Adding a Vehicle top # The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository. Adding an HD Map top # The default maps have the Vector map files included in the LGSVL Autoware Data Github repository. Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Instructions"},{"location":"autoware-instructions/#general","text":"This guide goes through how to run Autoware.AI with the LGSVL simulator. In order to run Autoware with the LGSVL simulator, it is easiest to pull the official Autoware Docker image (see the official guide, Case 1 for more details), but it is also possible to build Autoware from source . Autoware communicates with the LGSVL simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. The official Autoware Docker containers have rosbridge_suite included.","title":"General"},{"location":"autoware-instructions/#setup","text":"","title":"Setup"},{"location":"autoware-instructions/#requirements","text":"Linux operating system NVIDIA graphics card","title":"Requirements"},{"location":"autoware-instructions/#install-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps to run docker as a non-root user.","title":"Install Docker CE"},{"location":"autoware-instructions/#install-nvidia-container-toolkit","text":"Before installing the NVIDIA Container Toolkit (nvidia-docker), make sure that you have an appropriate NVIDIA driver installed. To test if the NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.59 Driver Version: 440.59 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ Install the NVIDIA Container Toolkit by following the instructions here .","title":"Install NVIDIA Container Toolkit"},{"location":"autoware-instructions/#install-lgsvl-simulator","text":"Follow the instructions here .","title":"Install LGSVL Simulator"},{"location":"autoware-instructions/#install-autoware","text":"Make sure you have Git Large File Storage (LFS) installed before cloning the repository in the next step. If git lfs outputs git: 'lfs' is not a git command. , then you need to install it: Instructions for installation are here . Verify the installation: $ git lfs install Git LFS initialized. Create a directory called shared_dir in your home directory to hold HD maps and launch files for the simulator. The Autoware Docker container will mount this folder. mkdir ~/shared_dir cd ~/shared_dir git clone https://github.com/lgsvl/autoware-data.git If there wasn't a line beginning with Filtering content: output, then Git LFS hasn't been installed. Remove the autoware-data directory, install Git LFS with git lfs install , and then re-issue the git clone . Clone the docker repository from autoware.ai into a working directory: cd $WORKING_DIRECTORY git clone https://gitlab.com/autowarefoundation/autoware.ai/docker.git If you are using the latest Docker and NVIDIA Container Toolkit, the docker/generic/run.sh script will need to be modified. To determine whether you need to do this run type nvidia-docker in a terminal. If you get output similar to: nvidia-docker is /usr/bin/nvidia-docker , the script will work as is. If not, then modify it as described below: In docker/generic/run.sh , find the following block at line 139: if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" RUNTIME=\"--runtime=nvidia\" fi Replace it with: DOCKER_VERSION=$(docker version --format '{{.Client.Version}}' | cut -d'.' -f1) if [ $CUDA == \"on\" ]; then SUFFIX=$SUFFIX\"-cuda\" if [[ $DOCKER_VERSION -ge \"19\" ]] && ! type nvidia-docker; then RUNTIME=\"--gpus all\" else RUNTIME=\"--runtime=nvidia\" fi fi","title":"Install Autoware"},{"location":"autoware-instructions/#launch-autoware-alongside-lgsvl-simulator","text":"Run the Autoware 1.12.0 container and enter into it: cd $WORKING_DIRECTORY/docker/generic ./run.sh -t 1.12.0 Once inside the container, install a missing ROS package: sudo apt update && sudo apt install ros-$ROS_DISTRO-image-transport-plugins -y If you need to check which $ROS_DISTRO you have installed run the following: ls /opt/ros/ Launch the runtime manager: roslaunch runtime_manager runtime_manager.launch A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/BorregasAve/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which also launches rosbridge. Run the LGSVL simulator Create a Simulation choosing BorregasAve map and Jaguar2015XE (Autoware) or another Autoware compatible vehicle. Enter localhost:9090 for the Bridge Connection String. Run the created Simulation A vehicle should appear in Borregas Ave in Sunnyvale, CA. In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. The vehicle may be mis-localized as the initial pose is important for NDT matching. To fix this, click \"2D Pose Estimate\" in Rviz, then click an approximate position for the vehicle on the map and drag in the direction it is facing before releasing the mouse button. This should allow NDT matching to find the vehicle pose (it may take a few tries). Note that the point cloud will not show up in rviz until ndt matching starts publishing a pose. An alternative would be to use GNSS for an inital pose or for localization but the current Autoware release (1.12.0) does not support GNSS coordinates outside of Japan. Fix for this is available in following pull requests: utilities#27 , common#20 , core_perception#26 These are not yet merged in Autoware master.","title":"Launch Autoware Alongside LGSVL Simulator"},{"location":"autoware-instructions/#driving-by-following-vector-map","text":"To drive following the HD map follow these steps: - in rviz, mark a destination by clicking '2D Nav Goal' and clicking at the destination and dragging along the road direction. Make sure to only choose a route that looks valid along the lane centerlines that are marked with orange lines in rviz. If an invalid destination is selected nothing will change in rviz, and you will need to relaunch the Mission Planning launch file in the Quick Launch tab to try another destination. After choosing a valid destination the route will be highlighted in blue in rviz. To follow the selected route launch these nodes: - Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. - Enable astar_avoid and velocity_set . - Enable pure_pursuit and twist_filter to start driving.","title":"Driving by following vector map:"},{"location":"autoware-instructions/#driving-by-following-prerecorded-waypoints","text":"A basic functionality of Autoware is to follow a prerecorded map while obeying traffic rules. To do this you will need to record a route first. Switch to the Computing tab and check the box for waypoint_saver . Make sure to select an appropriate location and file name by clicking on the app button. Now you can drive around the map using the keyboard. Once you are satisfied with your route, uncheck the box for waypoint_saver to end the route. To drive the route using Autoware: Enable waypoint_loader while making sure the correct route file is selected in the app settings. Enable lane_rule , lane_stop , and lane_select to follow traffic rules based on the vector map. Enable astar_avoid and velocity_set . Enable pure_pursuit and twist_filter to start driving. The ego vehicle should try to follow the waypoints at the velocity which they were originally recorded at. You can modify this velocity by manually editing the values csv file.","title":"Driving by following prerecorded waypoints:"},{"location":"autoware-instructions/#adding-a-vehicle","text":"The default vehicles have the calibration files included in the LGSVL Autoware Data Github repository.","title":"Adding a Vehicle"},{"location":"autoware-instructions/#adding-an-hd-map","text":"The default maps have the Vector map files included in the LGSVL Autoware Data Github repository.","title":"Adding an HD Map"},{"location":"autoware-instructions/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"autoware-json-example/","text":"Example JSON Configuration for an Autoware Vehicle Bridge Type top # ROS Published Topics top # Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera Subscribed Topics top # Topic Sensor Name /vehicle_cmd Autoware Car Control Complete JSON Configuration top # [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Sample sensor configuration"},{"location":"autoware-json-example/#bridge-type","text":"ROS","title":"Bridge Type"},{"location":"autoware-json-example/#published-topics","text":"Topic Sensor Name /nmea_sentence GPS /odom GPS Odometry /imu_raw IMU /points_raw Lidar /simulator/camera_node/image/compressed Main Camera","title":"Published Topics"},{"location":"autoware-json-example/#subscribed-topics","text":"Topic Sensor Name /vehicle_cmd Autoware Car Control","title":"Subscribed Topics"},{"location":"autoware-json-example/#complete-json-configuration","text":"[ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/odom\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu_raw\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/camera_node/image/compressed\", \"Frame\": \"camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Complete JSON Configuration"},{"location":"bridge-connection-ui/","text":"Bridge Connection UI When in a non-Headless Simulation, a list of published and subscribed topics can be found in the Simulator menu (plug icon). At the top of the menu is the selected vehicle. The bridge status can be: Disconnected , Connecting , or Connected The bridge address is the same that was entered as the Bridge Connection String when creating the Simulation. Each topic is then listed in the following format: PUB or SUB : indicates if the Simulator publishes or subscribes to messages on this topic Topic : is the topic that the messages are published/subscribed to Type : is the message type on this topic Count : is the total number of messages published/received when the bridge was connected","title":"Bridge topics"},{"location":"build-instructions/","text":"Instructions to build standalone executable When a terminal is mentioned, it refers to: cmd.exe on Windows Terminal on Ubuntu Download and Install Unity Hub Ubuntu: You may need to allow the downloaded AppImage to be run as an executable Right-click the AppImage Select Properties Go to the Permissions tab Check Allow executing file as program Alternatively, in the terminal run sudo chmod +x UnityHub.AppImage Download and Install Unity 2019.3.3f1: IMPORTANT include Mono support for both Windows and Linux when installing Unity (Optional) include support for Visual Studio for easier debugging Unity Download Archive Windows Click the Unity Hub button to have Unity Hub handle the installation process Ubuntu Right click the Unity Hub button and select Copy Link Address In a terminal, type <PATH_TO_UNITY_HUB> <COPIED_LINK> The copied link will be in the form unityhub://Unity-VERSION/XXXXXX (e.g. unityhub://2019.3.3f1/7ceaae5f7503 ) Thus, if the Unity Hub application is in the current directory, type ./UnityHub.AppImage unityhub://2019.3.3f1/7ceaae5f7503 Unity Hub will open and guide you through the installation of Unity Editor Verify installation Under the Installs tab of Unity Hub there should be the expected version shown. In the bottom-left corner of the version, there should be an icon of the other OS (e.g. on a Linux computer, the Windows logo will be shown) Download and Install Node.js Version 12.16.1 LTS is fine Windows Download and run the .msi Ubuntu The instructions are from the NodeJS Github curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - sudo apt-get install -y nodejs Verify installation Open a terminal and type node --version v12.16.1 should print out Make sure you have git-lfs installed before cloning the Simulator repository . Instructions for installation are here Verify installation In a terminal enter git lfs install > Git LFS initialized. should print out Clone simulator from GitHub Open a terminal and navigate to where you want the Simulator to be downloaded to e.g. If you want the Simulator in your Documents folder, use cd in the terminal so that the input for the terminal is similar to /Documents$ git clone --single-branch https://github.com/lgsvl/simulator.git Verify download The git clone will create a Simulator folder Open a File Explorer and navigate to where the Simulator folder is Navigate to Simulator/Assets/Materials/EnvironmentMaterials/ There should be a EnvironmentDamageAlbedo.png in this folder Open the image, it should be a mostly grey square that looks like concrete If the image cannot be opend, Git LFS was not installed before cloning the repository Install Git LFS following step 4 In a terminal, navigate to the Simulator folder so that the terminal is similar to /Simulator$ git lfs pull Check the image again Run Unity Hub In the Projects tab, click Add and select the Simulator folder that was created by git clone in Step 5 In the Projects tab, verify that the Simulator is using Unity Version 2019.3.3f1 from the dropdown Double-click the name of the project to launch Unity Editor Open a terminal and navigate to the WebUI folder of the Simulator project Window ex. C:\\Users\\XXX\\Documents\\Simulator\\WebUI Linux ex. /home/XXX/Projects/Simulator/WebUI In the terminal run npm install to install dependencies, do this only once or if dependencies change inside packages.json file The output will be similar to below In Unity Editor run Build WebUi... in Unity : Simulator -> Build WebUI... Open Build... in Unity : Simulator -> Build... Check the Environments, Vehicles, and Sensors that should be generated as AssetBundles The Simulator repository does not contain any Environments, Vehicles, or Sensors. They are separate repositories on GitHub See assets documentation for information on how to add Environments and Vehicles They will be located in a folder called AssetBundles in the folder selected as the build location These may also be built separately from the Simulator. In this case they will be put into Simulator/AssetBundles folder of the project (Optional) Click Build to only build the AssetBundles. Load the LoaderScene.unity and click the Play button at the top of the editor to start the simulator. Select the Target OS for the build This is only used when building the Simulator. AssetBundles are built for Linux and Windows automatically Verify Build Simulator is checked for the Simulator to be built Select a folder that the simulator will be built in (Optional) Check Development Build to create a Development Build Click Build NOTE You will get an error when building AssetBundles if either Windows or Linux support is not installed in Unity Open Unity Hub and go to the Installs tab Click the vertical 3-dots next to the Unity version of the Simulator Click Add Modules Check Windows Build Support (Mono) or Linux Build Support (Mono) Click Done Test Simulator top # Ubuntu - Install Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Select graphics options then press Ok Click Open Browser In the Maps tab, Add new map with the URL to an environment assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\environment_borregasave In the Vehicles tab, Add new vehicle with the URL to a vehicle assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\vehicle_jaguar2015xe (Optional) Add a manual control \"sensor\" to the vehicle to enable driving Click the wrench icon next to the vehicle name In the text box insert [{\"type\": \"Manual Control\", \"name\": \"Manual Car Control\"}] In the Simulations tab, Add new simulation with the added map and vehicle Press the Play button The Unity window should now show a vehicle in the built environment","title":"Build instructions"},{"location":"build-instructions/#test-simulator","text":"Ubuntu - Install Vulkan userspace library sudo apt-get install libvulkan1 Double-click the Simulator.exe that was built Select graphics options then press Ok Click Open Browser In the Maps tab, Add new map with the URL to an environment assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\environment_borregasave In the Vehicles tab, Add new vehicle with the URL to a vehicle assetbundle ex. C:\\Users\\XXX\\Desktop\\Simulator\\AssetBundles\\vehicle_jaguar2015xe (Optional) Add a manual control \"sensor\" to the vehicle to enable driving Click the wrench icon next to the vehicle name In the text box insert [{\"type\": \"Manual Control\", \"name\": \"Manual Car Control\"}] In the Simulations tab, Add new simulation with the added map and vehicle Press the Play button The Unity window should now show a vehicle in the built environment","title":"Test Simulator"},{"location":"changelog/","text":"Changelog All notable changes and release notes for LGSVL Simulator will be documented in this file. [2020.05] - 2020-06-02 # Added # Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc. Changed # Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository [2020.03] - 2020-04-03 # Added # Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2 Changed # Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn [2020.01] - 2020-01-31 # Added # Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LIDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto Changed # Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS [2019.12] - 2020-01-21 # Added # Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map. Changed # Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh. [2019.11] - 2019-11-19 # Added # OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins. Changed # Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps. [2019.10] - 2019-10-28 # Added # Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor. Changed # Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start. [2019.09] - 2019-09-06 # Added # Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle Changed # Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations [2019.07] - 2019-08-09 # Added # Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization Changed # User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor Removed # Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps [2019.05 and older] # Please see release notes for previous versions on our Github releases page.","title":"Release notes"},{"location":"changelog/#202005-2020-06-02","text":"","title":"[2020.05] - 2020-06-02"},{"location":"changelog/#added","text":"Support for custom traffic vehicle behavior plugins Support for Apollo 5.5 ( latest master ) Support for distributed simulation Velodyne VLS-128 support Python API function to spawn random non-ego vehicles and pedestrians Unlit shadows support for point cloud environment rendering AD system-agnostic message definitions to \"lgsvl_msgs\" for vehicle control, CAN bus and status data, etc.","title":"Added"},{"location":"changelog/#changed","text":"Refactored traffic agent Waypoint Mode with fixes for idle and trigger waypoints Changed gnss_odom message coordinate system to right-handed coordinate system Fixed \"ignore map origin\" option for gnss_odom message Fixed acceleration values in IMU sensor Fix sky flickering issue during point cloud rendering Fixed LiDAR readback error Removed and replaced documentation source files from main repository to separate Documentation repository","title":"Changed"},{"location":"changelog/#202003-2020-04-03","text":"","title":"[2020.03] - 2020-04-03"},{"location":"changelog/#added_1","text":"Velodyne VLP-16 and VLP-32C support through custom LiDAR sensor plugin Support for LiDAR sensor plugin with direct UDP socket interface Support for point cloud environment rendering Extensible vehicle dynamics Functional Mockup Interface 2.0 (FMI 2.0) interface for vehicle dynamics Support for fisheye camera lens distortion Support for sensor_msgs/NavSatFix message type for GPS messages in ROS2 Vehicle Odometry sensor for reporting steering angles and velocity in ROS2","title":"Added"},{"location":"changelog/#changed_1","text":"Upgraded to Unity 2019.3.3f1 Fixed black artifacts in Linux bug Fixed display issues in Unity Editor bug Fixed camera sensor invalid data bug 3D Ground Truth sensor now generates different ID for vehicle after each respawn","title":"Changed"},{"location":"changelog/#202001-2020-01-31","text":"","title":"[2020.01] - 2020-01-31"},{"location":"changelog/#added_2","text":"Controllable Object plugin support, dynamically add and control Controllables with the Python API Ability to create LIDAR sensor with non-uniform beam distribution Ability to add distortion effects to Camera sensors Support for Autoware.Auto messages (ROS2) Ability to create custom message types for sensor plugins TransformSensor which can act as a parent for other sensors VehicleStateSensor which is needed by Autoware.Auto","title":"Added"},{"location":"changelog/#changed_2","text":"Added SanFrancisco map as a default map option Allow null texture bundles without an error Pedestrian mass reduced to 70kg Fixed lagging of 2D Ground Truth boxes (in visualization and data) ComfortSensor included with Simulator binary TrafficCone included with Simulator binary Fixed exported HD map lane relations Fixed ROS2 GPS","title":"Changed"},{"location":"changelog/#201912-2020-01-21","text":"","title":"[2019.12] - 2020-01-21"},{"location":"changelog/#added_3","text":"Custom callback for Python API, useful in sensor plugins. Ability to specify count and types of NPCs to spawn on the map.","title":"Added"},{"location":"changelog/#changed_3","text":"Optimized texture usage in maps - significantly reduces size on disk and GPU memory used. Allow boundary type to be optional for Apollo HD map import. Fix Python API to be able load maps & vehicles that are shared between multiple users. Separated bundle file format version for map and vehicle bundles. Improved headlights for EGO vehicles. Improved HD map annotation icons. Fixed Sedan NPC mesh.","title":"Changed"},{"location":"changelog/#201911-2019-11-19","text":"","title":"[2019.11] - 2019-11-19"},{"location":"changelog/#added_4","text":"OpenDrive 1.4 HD map import. Rain drops on ground when it is raining. Separated Apollo HD map export between version 3.0 and 5.0 to support 5.0 specific features. Cache vehicle prefabs when loading same vehicle multiple times into simulation. Ability to login to account via command-line. Ability for vehicle to have multiple interior lights. Fixes #474. Allow Color, Depth and Semantic cameras to have higher capture & publish rate. Building & loading custom sensors as plugins.","title":"Added"},{"location":"changelog/#changed_4","text":"Fixed traffic lights signal colors on Shalun map. Fixed exceptions when NPCs are despawned while still in intersection. Fixed errors when adding pedestrian to map without NavMesh. Fixed Lanelet2 boundary import and export. Multiple fixes for OpenDrive import and export. Fixed wrong raycast layers in 2D Ground Truth sensor to detect if NPC is visible. Fixed missing timestamp when publishing ROS/Cyber messages from 2D Ground Truth sensor. Limit number of spawned NPCs and vehicles for large maps.","title":"Changed"},{"location":"changelog/#201910-2019-10-28","text":"","title":"[2019.10] - 2019-10-28"},{"location":"changelog/#added_5","text":"Apollo HD map import Accurate sun position in sky based of map location. Including time of sunrise and sunset. Control calibration sensor to help calibrating AD stack control. Ported Shalun map from previous non-HDRP simulator version. Ground Truth sensor for traffic light. Python API method to get controllable object by position. Python API method to convert multiple map coordinates in single call. Python API method to perform multiple ray-casts in single call. Sensor for controlling vehicle with steering wheel (Logitech G920). Platform independent asset bundles (Windows and Linux). Allow to set custom data path for database and downloaded bundles. Visualize data values for non-image senors (GPS, IMU, etc). Populate scene with required objects when new scene is created in Unity Editor.","title":"Added"},{"location":"changelog/#changed_5","text":"Fixed exceptions in ROS Bridge where if it receives message on topic that it has not subscribed. Fixed 3D Ground Truth sensor to report correct NPC orientation angles. Fixed Radar sensor to visualize pedestrians. Fixed Color camera to render mountains in BorregaAve. Fixed EGO vehicle collision callback to Python API. Fixed WebUI redirect loop that happens if you are logged out. Fixed reported NPC vehicle speed. Fixes #347 and #317. Fixed gear shifting for EGO vehicle control. Fixes #389. Fixed NPC waypoint following where NPCs stopped if assigned speed is too low. Fixed semantic segmentation for vehicles and pedestrians added with Python API. Fixed ROS2 message publishing (seq field was missing in Header). Fixes #413. Fixed issue with database on some non-English locales. Fixes #381. Fixed point cloud generation in Unity Editor. Fixed browser loosing cookie when session ends in WebUI. Fixed slowness in Python API when running without access to Internet. Fixed issue when multiple users could not use same map url. Improved error messages when simulation fails to start.","title":"Changed"},{"location":"changelog/#201909-2019-09-06","text":"","title":"[2019.09] - 2019-09-06"},{"location":"changelog/#added_6","text":"Sensor visualization UI HD map export to OpenDrive 1.4 format ROS service support for ROS bridge Python API to support more robust waypoints for NPC vehicles Python API with ability to control traffic lights on map Hyundai Nexo vehicle","title":"Added"},{"location":"changelog/#changed_6","text":"Improved NPC movement and right turns on red traffic light Fixed NPC vehicle despawning logic so they don't get stuck in intersections Change NPC vehicles colliders from box to mesh to improves collision precision Updated generated protobuf message classes for latest Apollo 5.0 Fixed 3D Ground Truth message type for ROS Fixed 3D and 2D Ground Truth bounding box locations","title":"Changed"},{"location":"changelog/#201907-2019-08-09","text":"","title":"[2019.07] - 2019-08-09"},{"location":"changelog/#added_7","text":"Separate Asset Bundles for environments and vehicles Fully deterministic physics simulation Faster-than-real-time capability with Python API Lanelet2 HD map format import/export Ability to edit sensor configuration dynamically Multi-user support - account login allows different users to login to one machine running simulator BorregasAve 3D environment as a default provided map AutonomouStuff 3D environment as a default provided map of parking lot SingleLaneRoad 3D environment as a default provided map CubeTown 3D environment as a default provided map Lexus RX and Lincoln MKZ vehicle support LiDAR outputs intensity value based on reflectivity of material (instead of color) Support for Apollo 5.0 Support for Autoware 1.12 Light reflections on road from wetness Better sky rendering, including clouds Ability to import point cloud files for visualization","title":"Added"},{"location":"changelog/#changed_7","text":"User interface - use web UI for main user interaction Render pipeline - Unity High Definition Render Pipeline Significantly improved LiDAR simulation performance using multithreading Improved map annotation for easier use in Editor Improved point cloud generation from 3D environment in Editor","title":"Changed"},{"location":"changelog/#removed","text":"Support for Duckiebot, EP_rigged, SF_rigged, Tugbot vehicles Support for SimpleLoop, SimpleMap, SimpleRoom, Duckietown, DuckieDownTown, SanFrancisco, Shalun maps","title":"Removed"},{"location":"changelog/#201905-and-older","text":"Please see release notes for previous versions on our Github releases page.","title":"[2019.05 and older]"},{"location":"cluster-simulation-introduction/","text":"Cluster Simulation Introduction What a Cluster is top # A Cluster is a single unit of simulation. By default there is always local machine available as a cluster. When a user launches the simulator, there is a command-line option available to select between simulation master and client. When Simulator is started as master (default mode) user can access Web UI and perform simulation tasks. When Simulator is started as a client (using command-line argument) simulator is not rendering anything, but expects connection from a master. For a simulation to start, master and client should share the same maps, vehicles and simulation configuration. Simulation master is responsible to provide all required configuration and client is responsible to download all asset bundles required to start the simulation. Simulation master and other clients are waiting for all nodes to download and cache required asset bundles. Master is responsible for assigning sensors to each client for simulation (including itself). After sensors are assigned each client starts the simulation while synchronizing the position of vehicles. See Cluster Simulation Quickstart for details on running a Cluster Simulation. Why use a Cluster top # Clusters allow for better performance when generating data from multiple sensors. Unity does not have multiple-GPU support so multiple instances of the simulator are required. This also allows for large-scale simulations with multiple vehicles on the same map. Simulation Synchronization top # For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. Simulator starts a simulation on the master and sends clients configuration required to set up the same map and objects on the map. When the simulation changes on master it sends updates messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Managers. Components Synchronization top # Every component that has to be synchronized between cluster machines requires messages sender on the master and messages receiver on the clients. Taking a vehicle with rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. Default implementation required each DistributedObject to have a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. For more advanced solutions refer to Distributed Objects . Due to the performance impact, the Simulator by default does not synchronize Transform components. To add this functionality refer to Distributed Components .","title":"Cluster Simulation Introduction [](#top)"},{"location":"cluster-simulation-introduction/#what-a-cluster-is","text":"A Cluster is a single unit of simulation. By default there is always local machine available as a cluster. When a user launches the simulator, there is a command-line option available to select between simulation master and client. When Simulator is started as master (default mode) user can access Web UI and perform simulation tasks. When Simulator is started as a client (using command-line argument) simulator is not rendering anything, but expects connection from a master. For a simulation to start, master and client should share the same maps, vehicles and simulation configuration. Simulation master is responsible to provide all required configuration and client is responsible to download all asset bundles required to start the simulation. Simulation master and other clients are waiting for all nodes to download and cache required asset bundles. Master is responsible for assigning sensors to each client for simulation (including itself). After sensors are assigned each client starts the simulation while synchronizing the position of vehicles. See Cluster Simulation Quickstart for details on running a Cluster Simulation.","title":"What a Cluster is"},{"location":"cluster-simulation-introduction/#why-use-a-cluster","text":"Clusters allow for better performance when generating data from multiple sensors. Unity does not have multiple-GPU support so multiple instances of the simulator are required. This also allows for large-scale simulations with multiple vehicles on the same map.","title":"Why use a Cluster"},{"location":"cluster-simulation-introduction/#simulation-synchronization","text":"For the proper functioning of the sensors distributed to the clients, it is required to synchronize the whole simulation environment. Simulator starts a simulation on the master and sends clients configuration required to set up the same map and objects on the map. When the simulation changes on master it sends updates messages to every client. Simulator by default synchronizes every Rigidbody in ego vehicles, NPCs, pedestrians and controllable objects instantiated by Simulator Managers.","title":"Simulation Synchronization"},{"location":"cluster-simulation-introduction/#components-synchronization","text":"Every component that has to be synchronized between cluster machines requires messages sender on the master and messages receiver on the clients. Taking a vehicle with rigidbody as an example, Simulator adds DistributedObject component to the vehicle GameObject to synchronize enable and disable calls and DistributedRigidbody components which send required data to mock the state of Rigidbody on clients. Default implementation required each DistributedObject to have a unique path in the hierarchy - objects on the same hierarchy level require unique GameObject names. For more advanced solutions refer to Distributed Objects . Due to the performance impact, the Simulator by default does not synchronize Transform components. To add this functionality refer to Distributed Components .","title":"Components Synchronization"},{"location":"cluster-simulation-quickstart/","text":"Cluster Simulation Quickstart Start Simulators top # Cluster Simulation requires a single Simulator application running in the master mode and at least one Simulator application running in the client mode on another machine. Each Simulator application has to be running and waiting for the simulation start. See Configuration File and Command-Line Parameters for details on how to set up the Simulator on each machine in a cluster. Prepare The Cluster top # Open the WebUI on the machine where the Simulator application is running in the master mode and prepare a cluster with all the clients' IP addresses. See Clusters Tab for details on how to set up a cluster in the WebUI. Prepare And Start A Simulation top # Open the WebUI on the machine where the Simulator application is running in the master mode and prepare a simulation that uses prepared cluster with the clients' IP addresses. Start the simulation, the master will try to establish a connection with the clients if successful the simulation will start on each machine. Note that clients will download required maps and vehicles' asset bundles, this may cause an additional delay when starting a simulation.","title":"Cluster Simulation Quickstart [](#top)"},{"location":"cluster-simulation-quickstart/#start-simulators","text":"Cluster Simulation requires a single Simulator application running in the master mode and at least one Simulator application running in the client mode on another machine. Each Simulator application has to be running and waiting for the simulation start. See Configuration File and Command-Line Parameters for details on how to set up the Simulator on each machine in a cluster.","title":"Start Simulators"},{"location":"cluster-simulation-quickstart/#prepare-the-cluster","text":"Open the WebUI on the machine where the Simulator application is running in the master mode and prepare a cluster with all the clients' IP addresses. See Clusters Tab for details on how to set up a cluster in the WebUI.","title":"Prepare The Cluster"},{"location":"cluster-simulation-quickstart/#prepare-and-start-simulation","text":"Open the WebUI on the machine where the Simulator application is running in the master mode and prepare a simulation that uses prepared cluster with the clients' IP addresses. Start the simulation, the master will try to establish a connection with the clients if successful the simulation will start on each machine. Note that clients will download required maps and vehicles' asset bundles, this may cause an additional delay when starting a simulation.","title":"Prepare And Start A Simulation"},{"location":"clusters-tab/","text":"Web UI Clusters Tab Explanation What a Cluster is top # A Cluster allows connecting multiple machines rendering the same simulation, but different sensors on each machine, this allows to run multiple sensors with better performance. Master establishes a connection with each defined client, divides required sensors between every machine and sends the state of every Simulation element to connected clients synchronizing the environment. For more details about the cluster, simulations refer to the Cluster Simulation Introduction . How to Add a Cluster top # Click the Add new button In the dialogue that opens, enter the name of the cluster and IPv4 of each machine that will be in the cluster. A blank list of IPs will default to the localhost. How to Edit a Cluster top # Click the pencil icon In the dialogue that opens, the name of the cluster can be changed and the list of IPs can be modified. Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Clusters"},{"location":"clusters-tab/#what-a-cluster-is","text":"A Cluster allows connecting multiple machines rendering the same simulation, but different sensors on each machine, this allows to run multiple sensors with better performance. Master establishes a connection with each defined client, divides required sensors between every machine and sends the state of every Simulation element to connected clients synchronizing the environment. For more details about the cluster, simulations refer to the Cluster Simulation Introduction .","title":"What a Cluster is"},{"location":"clusters-tab/#how-to-add-a-cluster","text":"Click the Add new button In the dialogue that opens, enter the name of the cluster and IPv4 of each machine that will be in the cluster. A blank list of IPs will default to the localhost.","title":"How to Add a Cluster"},{"location":"clusters-tab/#how-to-edit-a-cluster","text":"Click the pencil icon In the dialogue that opens, the name of the cluster can be changed and the list of IPs can be modified.","title":"How to Edit a Cluster"},{"location":"clusters-tab/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"config-and-cmd-line-params/","text":"Configuration File and Command Line Parameters Configuration File top # To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. Simulator configuration file includes parameters shared between different users and allows administrator to setup deployment specific parameters. Simulator uses YAML format for storing data in configuration file. See a table below to find out all supported parameters: Parameter Name Type Default Value Description hostname string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. port integer 8080 Port number used by HTTP server to host WebUI. headless bool false Whether or not simulator should work in headless mode only. If parameter is set to true - non headless simulation should fail to start, WebUI should not allow to create non-headless simulations. client bool false Whether or not simulator should work in client mode only. If parameter is set to true - HTTP server does not start and user only can run this instance as a part of cluster. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. api_hostname string localhost Name of the Python API host. By default it equal to hostname. Python API should respond to queries only related to the api_hostname provided. Star (*) can be used as a wildcard to match any domain. api_port integer 8181 Port number used by Python API to connect. cloud_url string TBD Cloud URL points to a simulator API endpoint in the cloud and is responsible for user authentication, new user registration and sharing content. Simulator uses our public API endpoint by default, but that could be changed for private on-premise deployment. Command Line Parameters top # Simulator accepts provided command line parameters during start. Command line parameters overrides values from Configuration File. Only most important parameters can be provided via command line, see more details about Simulator Configuration File in the section related to configuration file. List of supported command line parameters can be found below: Parameter Name Type Default Value Description --hostname or -h string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. --port or -p integer 8080 Port number used by HTTP server to host WebUI. --client or -c none Whether or not simulator should work in client mode only. If parameter is present - HTTP server does not start and user only can run this instance as a part of cluster. --master or -m none Whether or not simulator should work in master mode. If parameter is present - HTTP server starts and user can use WebUI regardless of what was specified in Simulator Configuration File. --username or -u string Provides username for authentication with the cloud. --password or -w string Provides password for authentication with the cloud. --agree none Accepts the license agreement and forces to skip it in Web UI for specified user. This parameter is optional and can only be used only when username has been provided.","title":"Configuration File and Command Line Parameters"},{"location":"config-and-cmd-line-params/#configuration-file","text":"To use a custom config.yml file, include it in the same folder as Simulator.exe or modify the one in the root directory of the cloned project. Simulator configuration file includes parameters shared between different users and allows administrator to setup deployment specific parameters. Simulator uses YAML format for storing data in configuration file. See a table below to find out all supported parameters: Parameter Name Type Default Value Description hostname string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. port integer 8080 Port number used by HTTP server to host WebUI. headless bool false Whether or not simulator should work in headless mode only. If parameter is set to true - non headless simulation should fail to start, WebUI should not allow to create non-headless simulations. client bool false Whether or not simulator should work in client mode only. If parameter is set to true - HTTP server does not start and user only can run this instance as a part of cluster. read_only bool false Whether or not the user is allowed to change anything in the database. This mode is used to run Simulator in public demo mode. api_hostname string localhost Name of the Python API host. By default it equal to hostname. Python API should respond to queries only related to the api_hostname provided. Star (*) can be used as a wildcard to match any domain. api_port integer 8181 Port number used by Python API to connect. cloud_url string TBD Cloud URL points to a simulator API endpoint in the cloud and is responsible for user authentication, new user registration and sharing content. Simulator uses our public API endpoint by default, but that could be changed for private on-premise deployment.","title":"Configuration File"},{"location":"config-and-cmd-line-params/#command-line-parameters","text":"Simulator accepts provided command line parameters during start. Command line parameters overrides values from Configuration File. Only most important parameters can be provided via command line, see more details about Simulator Configuration File in the section related to configuration file. List of supported command line parameters can be found below: Parameter Name Type Default Value Description --hostname or -h string localhost Name of the HTTP server host. Simulator should respond to queries only related to the hostname provided. Star (*) can be used as a wildcard to match any domain. --port or -p integer 8080 Port number used by HTTP server to host WebUI. --client or -c none Whether or not simulator should work in client mode only. If parameter is present - HTTP server does not start and user only can run this instance as a part of cluster. --master or -m none Whether or not simulator should work in master mode. If parameter is present - HTTP server starts and user can use WebUI regardless of what was specified in Simulator Configuration File. --username or -u string Provides username for authentication with the cloud. --password or -w string Provides password for authentication with the cloud. --agree none Accepts the license agreement and forces to skip it in Web UI for specified user. This parameter is optional and can only be used only when username has been provided.","title":"Command Line Parameters"},{"location":"contributing/","text":"Contributing to LGSVL Simulator As an open project and community, we welcome and highly encourage contributions back to LGSVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of LGSVL Simulator, you can help contribute to the LGSVL Simulator project in several different ways. Feedback, questions, bug reports, and issues # The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com . Submitting a Pull Request # We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms. Documentation # We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website. Demonstrations # If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application. Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Contributing"},{"location":"contributing/#feedback-questions-bug-reports-and-issues","text":"The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com .","title":"Feedback, questions, bug reports, and issues"},{"location":"contributing/#submitting-a-pull-request","text":"We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms.","title":"Submitting a Pull Request"},{"location":"contributing/#documentation","text":"We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website.","title":"Documentation"},{"location":"contributing/#demonstrations","text":"If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application.","title":"Demonstrations"},{"location":"contributing/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"control-calibration/","text":"How to Collect Data with Control Calibration Sensor Control Calibration Sensor is for collecting control data to generate control calibration table which can be referred by control module to decide throttle, brake and steering command. Setup # Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map Instructions # Add WideFlatMap into WebUI. The assetbundle is available on the content website Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"How to collect data with control calibration sensor"},{"location":"control-calibration/#setup","text":"Check if there is Wide Flat Map ls /apollo/modules/map/data/wide_flat_map","title":"Setup"},{"location":"control-calibration/#instructions","text":"Add WideFlatMap into WebUI. The assetbundle is available on the content website Create a vehicle with a CyberRT bridge type and add total control calibration criteria into ego vehicle's sensor parameters . Start Apollo docker container /apollo/docker/scripts/dev_start.sh Go into Apollo docker container with /apollo/docker/scripts/dev_into.sh Run Cyber bridge /apollo/scripts/bridge.sh Run localization module mainboard -d /apollo/modules/localization/dag/dag_streaming_rtk_localization.dag Run dreamview bootstrap.sh In a browser, navigate to Dreamview at http://localhost:8888/ Choose vehicle model. Choose Map as Wide Flat Map. Choose setup mode to Vehicle Calibration. In Others tab, choose Data Collection Monitor In Modules tab, turn Recorder on. In the Simulator's WebUI, create a simulation with WideFlatMap and the vehicle created in Step 2 (with the bridge connection string to the computer running Apollo). In Simulator's WebUI, start simulation. You can see progress bar filled as Apollo collects data. Once all progress bars are filled, vehicle control calibration is done.","title":"Instructions"},{"location":"controllable-plugins/","text":"Controllable Plugins Controllable plugins are custom controllables that can be added to a scene at runtime with the API. Before running the simulator (running the executable or pressing Play in the Editor) controllable plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/Controllables folder. Open-source example: TrafficCone Table of Contents Building Controllable Plugins Creating Controllable Plugins Controllable Logic Building Controllable Plugins top # Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details. Creating Controllable Plugins top # Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel Controllable Logic top # Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable, { public bool Spawned { get; set; } public string UID { get; set; } public string Key => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public string DefaultControlPolicy { get; set; } = \"\"; public string CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; CurrentState = \"\"; } public void Control(List<ControlAction> controlActions) { for (int i = 0; i < controlActions.Count; i++) { var action = controlActions[i].Action; var value = controlActions[i].Value; switch (action) { case = \"state\": // set in policy parse CurrentState = value; // switch (CurrentState) // set logic break; default: Debug.LogError($\"'{action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable plugins"},{"location":"controllable-plugins/#building-controllable-plugins","text":"Open Simulator -> Build... menu item Select controllable plugins in \"Controllables\" section of build window Build plugins with \"Build\" button The bundle named controllable_XXX will be placed in the AssetBundles/Controllables folder. If building the binary, this folder is included in the target destination. See build instructions for more details.","title":"Building Controllable Plugins"},{"location":"controllable-plugins/#creating-controllable-plugins","text":"Create a folder in Assets/External/Controllables , e.g. Assets/External/Controllables/TrafficCone. Inside this folder you must place the controllable prefab with same name ( TrafficCone.prefab ) that will be used by simulator to instantiate at runtime, the controllable logic script and Models folder with materials and textures. This prefab must have a logic script that inherits interface IControllable and added to the root of the prefab. The controllable tag and layer for traffic cone is set to Obstacle. If collisions are desired, then add a collider components. A Rigidbody component can be added if velocity changes are desired, if not, velocity change commands will be ignored. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the controllable (e.g. TrafficCone ) In the Inspector for this object, select Add Component Search for the controllable script Add Rigidbody if needed Add Collider if needed Drag this object from the scene hierarchy into the project folder to create a new prefab, delete prefab in Hierarchy panel","title":"Creating Controllable Plugins"},{"location":"controllable-plugins/#controllable-logic","text":"Additionally place a C# script which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). This is required even for simple objects such as a Traffic Cone. Controllable scripts must inherit interface IControllable which allows controllables to receive API commands. In addition, it must implement all interface variables and methods. See the below code block from TrafficCone.cs: namespace Simulator.Controllable { public class TrafficCone : MonoBehaviour, IControllable, { public bool Spawned { get; set; } public string UID { get; set; } public string Key => UID; public string ControlType { get; set; } = \"cone\"; public string CurrentState { get; set; } public string[] ValidStates { get; set; } = new string[] { }; public string[] ValidActions { get; set; } = new string[] { }; public string DefaultControlPolicy { get; set; } = \"\"; public string CurrentControlPolicy { get; set; } private void Awake() { CurrentControlPolicy = DefaultControlPolicy; CurrentState = \"\"; } public void Control(List<ControlAction> controlActions) { for (int i = 0; i < controlActions.Count; i++) { var action = controlActions[i].Action; var value = controlActions[i].Value; switch (action) { case = \"state\": // set in policy parse CurrentState = value; // switch (CurrentState) // set logic break; default: Debug.LogError($\"'{action}' is an invalid action for '{ControlType}'\"); break; } } } } }","title":"Controllable Logic"},{"location":"create-ros2-ad-stack/","text":"How to create a ROS2-based AD stack with LGSVL Simulator This tutorial works with Simulator Release 2019.05 This documentation describes how to develop ROS2 nodes to receive sensor data from LGSVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with LGSVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously. Table of Contents Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package setup.py package.xml Building a ROS2 Package Running Rosbridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to LGSVL Simulator Running ROS2 Node References Copyright and License Requirements # Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator Setup # Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away. Installing Docker CE # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook Creating a ROS2 Package # A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml setup.py # from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, ) package.xml # <?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package> Building a ROS2 Package # Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install Running Rosbridge # Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge Writing ROS2 Subscriber Node # ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously. Subscribe to a single topic # import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Subscribe to multiple topics simultaneously # In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main() Writing ROS2 Publisher Node # The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge. Publish command back to LGSVL Simulator # import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Running ROS2 Node # Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node} References # Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"How to create a simple ROS2-based AD stack"},{"location":"create-ros2-ad-stack/#requirements","text":"Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator","title":"Requirements"},{"location":"create-ros2-ad-stack/#setup","text":"Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away.","title":"Setup"},{"location":"create-ros2-ad-stack/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"create-ros2-ad-stack/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"create-ros2-ad-stack/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"create-ros2-ad-stack/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"create-ros2-ad-stack/#creating-a-ros2-package","text":"A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml","title":"Creating a ROS2 Package"},{"location":"create-ros2-ad-stack/#setuppy","text":"from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, )","title":"setup.py"},{"location":"create-ros2-ad-stack/#packagexml","text":"<?xml version=\"1.0\"?> <package format=\"2\"> <name>lane_following</name> <version>0.0.1</version> <description>ROS2-based End-to-End Lane Following model</description> <maintainer email=\"david.uhm@lge.com\">David Uhm</maintainer> <license>BSD</license> <exec_depend>rclpy</exec_depend> <exec_depend>std_msgs</exec_depend> <exec_depend>sensor_msgs</exec_depend> <test_depend>ament_copyright</test_depend> <test_depend>ament_flake8</test_depend> <test_depend>ament_pep257</test_depend> <test_depend>python3-pytest</test_depend> <export> <build_type>ament_python</build_type> </export> </package>","title":"package.xml"},{"location":"create-ros2-ad-stack/#building-a-ros2-package","text":"Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install","title":"Building a ROS2 Package"},{"location":"create-ros2-ad-stack/#running-rosbridge","text":"Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge","title":"Running Rosbridge"},{"location":"create-ros2-ad-stack/#writing-ros2-subscriber-node","text":"ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously.","title":"Writing ROS2 Subscriber Node"},{"location":"create-ros2-ad-stack/#subscribe-to-a-single-topic","text":"import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Subscribe to a single topic"},{"location":"create-ros2-ad-stack/#subscribe-to-multiple-topics-simultaneously","text":"In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main()","title":"Subscribe to multiple topics simultaneously"},{"location":"create-ros2-ad-stack/#writing-ros2-publisher-node","text":"The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge.","title":"Writing ROS2 Publisher Node"},{"location":"create-ros2-ad-stack/#publish-command-back-to-lgsvl-simulator","text":"import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Publish command back to LGSVL Simulator"},{"location":"create-ros2-ad-stack/#running-ros2-node","text":"Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node}","title":"Running ROS2 Node"},{"location":"create-ros2-ad-stack/#references","text":"Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters","title":"References"},{"location":"create-ros2-ad-stack/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"custom-distributed-class/","text":"Custom Distributed Class Distributed system supports sending distributed messages from any class implementing IMessageSender to the objects of classes IMessageReceiver . A custom class can implement both interfaces, address key property will be shared. Address Key top # Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value. Registration top # Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration. Message Sender top # IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method. Message Receiver top # IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Custom Distributed Class [](#top)"},{"location":"custom-distributed-class/#address-key","text":"Key property of the IMessageSender and IMessageReceiver is an address for the distributed messages. This key must be globally unique and deterministic, on every machine in every Simulation run the key of an object has to return the same value.","title":"Address Key"},{"location":"custom-distributed-class/#registration","text":"Every object that is going to send or receive distributed messages has to register itself in the MessagesManager . In the Simulator MessagesManager instance is available in the: Loader.Instance.Network.MessagesManager , this property has a null reference if the simulation does not use a cluster. Whenever an object is ready to send or receive distributed messages it has to register itself with the RegisterObject method of the MessagesManager and when an object will no longer send or receiver messages it has to unregister itself with UnregisterObject method of the MessagesManager . Messages received for the unregistered address key will be stored and passed to proper objects after registration.","title":"Registration"},{"location":"custom-distributed-class/#message-sender","text":"IMessageSender implementation requires: UnicastMessage , a basic implementation invokes UnicastMessage method of an MessagesManager instance; BroadcastMessage , a basic implementation invokes BroadcastMessage method of an MessagesManager instance; UnicastInitialMessages , requires sending every data required for the object initialization using UnicastMessage method.","title":"Message Sender"},{"location":"custom-distributed-class/#message-receiver","text":"IMessageReceiver implementation requires: ReceiveMessage , parses every incomming data after object is registered.","title":"Message Receiver"},{"location":"distributed-components/","text":"Distributed Components DistributedComponent is an abstract class with the implementation of sending snapshots from the authoritative object to all connected peers. For example DistributedTransform synchronize transforms states in the clients' simulations to the corresponding transform state on the master basing on the position, rotation and scale sent in the snapshots. DistributedTransform component added to GameObject in the simulation will synchronize the transform, note that every DistributedComponent requires a DistributedObject added to the same GameObject or any parent GameObject. Custom Distributed Component top # Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data. Distributed Components With Deltas top # A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data. Distributed Transform top # DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value Distributed Rigidbody top # DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Components [](#top)"},{"location":"distributed-components/#custom-distributed-component","text":"Extending the DistributedComponent component requires the following implementations: ComponentKey property, if multiple components of this type are allowed in a single GameObject it has to be a unique key otherwise, it can be for example class name. GetSnapshot method which returns snapshot data inside a ByteStack object (refer to Distributed Messages for more information about ByteStack ). ApplySnapshot method which parses and applies the snapshot data from the message content to the object. Note that the ApplySnapshot method has to pop data in reverse order than GetSnapshot is pushing data.","title":"Custom Distributed Component"},{"location":"distributed-components/#distributed-components-with-deltas","text":"A snapshot includes data required to recreate the same state on the client. Sending the whole snapshot with a whole object's state when only a single element changes will contain too much redundant data. DistributedComponentWithDeltas extends the basic implementation with two methods: SendDelta which sends the message with passed delta data inside a ByteStack object. ApplyDelta abstract method which parses and applies the delta data from the message content to the object. Note that the SendDelta method has to pop data in reverse order than ApplyDelta is pushing data.","title":"Distributed Components With Deltas"},{"location":"distributed-components/#distributed-transform","text":"DistributedTransform sends local position, local rotation and local scale of a transform component in the snapshots from the master to the clients. Only a single DistributedTransform component can be attached to a GameObject. Snapshots are send up to 60 times per second only if any element of the snapshot changes. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Transform"},{"location":"distributed-components/#distributed-rigidbody","text":"DistributedRigidbody sends the position and rotation of the rigidbody in the same GameObject from the master to the clients. With the default setting snapshots are just applied to the rigidbodies. It is possible to change SimulationType to ExtrapolateVelocities , with this setting DistributedRigidbody extrapolates received velocity and angular velocity. Applied position and rotation includes the corrections calculated from the extrapolated velocities. Snapshots are send up to 60 times per second only if rigidbody is not in sleeping mode. The snapshots limit can be changed on the master by changing the SnapshotsPerSecondLimit property value","title":"Distributed Rigidbody"},{"location":"distributed-messages/","text":"Distributed Messages DistributedMessage is a class that is used to exchange data between peers in the distributed system. Every message contains the Content where the actual data is stored and meta-data like address key, message type, and timestamp. Address Key top # AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message. Content top # Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing. Message Type top # DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used. Timestamp top # The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master. Bytes Stack top # BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push . Byte Compression top # ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Distributed Messages [](#top)"},{"location":"distributed-messages/#address-key","text":"AddressKey determines which object will receive the message on other simulations. DistributedComponent uses own key as an address key so the message will be received by corresponding components on other machines. AddressKey has to be set when sending a new message.","title":"Address Key"},{"location":"distributed-messages/#content","text":"Content contains the actual data of the message in the BytesStack object. Note that poping data required reverse order than pushing.","title":"Content"},{"location":"distributed-messages/#message-type","text":"DistributedMessageType determines how the message will be handled in the UDP protocol. This type has to be set when sending a new message. Available distributed message types: ReliableUnordered packets won't be dropped, won't be duplicated, can arrive without order. Sequenced packets can be dropped, won't be duplicated, will arrive in order. ReliableOrdered packets won't be dropped, won't be duplicated, will arrive in order. ReliableSequenced packets can be dropped (except the last one), won't be duplicated, will arrive in order. Unreliable packets can be dropped, can be duplicated, can arrive without order. ReliableOrdered messages are the most reliable, but may add significant delays, this type is best for the initialization messages like add new NPC command. Unreliable packets have the lowest delay between sending and handling a message but are not reliable. Snapshots are sent as unreliable , an incoming snapshot will override the data and delayed packets will not be used.","title":"Message Type"},{"location":"distributed-messages/#timestamp","text":"The distributed system adds UTC timestamp to every message right before sending it, there is no need to set timestamp value when sending a message. A timestamp of the received message is set before passing the message to the addressed object. This timestamp is already corrected on the client by the value of connection latency and shows the approximate UTC DateTime when the message has been sent by the master.","title":"Timestamp"},{"location":"distributed-messages/#bytes-stack","text":"BytesStack is the message's content where the data is stored. Every data has to be represented as a set of bytes and has to be pushed to the stack. BytesStack class supports byte , int , uint , long , float , double , bool and string values. Operations that can be executed on those values are: Push - adds the value on the top of the stack; Fetch - reads the data from the top of the stack and does not remove it from the stack; Pop - reads the data from the top of the stack and removed it from the stack, note that Pop calls have to be called in reverse order than Push .","title":"Bytes Stack"},{"location":"distributed-messages/#byte-compression","text":"ByteCompression class adds extension methods to the BytesStack which add different values to the stack with limited bytes count: CompressFloatToInt and DecompressFloatFromInt ; PushEnum and PopEnum ; PushCompressedColor and PopDecompressedColor ; PushCompressedVector3 and PushCompressedVector3 ; PushUncompressedVector3 and PopUncompressedVector3 ; PushCompressedPosition and PopDecompressedPosition - position bounds are limited to the map bounds; PushCompressedRotation and PopDecompressedRotation .","title":"Byte Compression"},{"location":"distributed-objects/","text":"Distributed Objects DistributedObject component synchronizes the GameObject state on the cluster simulation clients. DistributedObject can limit broadcasts sent by the DistributedComponents added to the children GameObjects and by default changes the part of components' keys. Custom Key top # Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value. Advanced top # DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject . Selective Distribution top # Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes. Authoritative Object top # Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. In this case, the client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Distributed Objects [](#top)"},{"location":"distributed-objects/#custom-key","text":"Every object synchronized in the cluster simulation required a unique key that is the same on every machine and cannot change after initialization. DistributedObject uses own path in the hierarchy as the unique key as a default implementation, but the synchronization will fail if there are two GameObjects with the same name in the same hierarchy path. The key of a DistributedObject can be changed by any component attached to the same GameObject which implements the IGloballyUniquelyIdentified interface. When registering a DistributedObject search if there is an IGloballyUniquelyIdentified implementation attached to the GameObject if an implementation is available GUID is used as the key prefix. If the GUID is null or empty registration waits until GUID changes to a not empty value.","title":"Custom Key"},{"location":"distributed-objects/#advanced","text":"DistributedObject implementation provides features that require additional scripting. This section describe advanced use cases that can be achieved with the DistributedObject .","title":"Advanced"},{"location":"distributed-objects/#selective-distribution","text":"Some objects should not be distributed to all the connected clients. It is possible to limit the distribution by setting the Selective Distribution to True and adding endpoints to the list (using AddEndPointToSelectiveDistribution method). With Selective Distribution enabled only endpoints on the list will receive updates from the DistributedObject and all the DistributedComponents in children. Selective distribution is prepared for the use-case when more logic has to be handled by the client. For example, if a client should count the waypoints of selected NPC those waypoints have to be synchronized between that client and master. In this case, pathfinding will be calculated on the client, the master will perform the simulation updates and other clients will not be informed about those changes.","title":"Selective Distribution"},{"location":"distributed-objects/#authoritative-object","text":"Available distributed components in the Simulator got different logic for the master and client. All the objects on the master are authoritative and send their state to the clients, where nonauthoritative objects apply the changes. This behavior can be changed by setting the custom value to the protected property IsAuthoritative , but it has to be done in the overridden Initialize method before calling the base method. In the case from the previous paragraph Selective Distribution client would perform the pathfinding and set the waypoints positions. In this case, the client sets the position of those GameObjects and has to send them to the master. To reverse the behavior where a master is sending the position of a DistributedRigidbody to the client, IsAuthoritative property has to be reversed ( false value on the master and true on the client). This way client will update the master with the Rigidbody changes.","title":"Authoritative Object"},{"location":"distributed-python-api/","text":"Distributed Python API Cluster Simulation performs changes only on the master simulation, clients' simulations apply the changes received from the master and don't require to react on every API command. Only selected commands are distributed to the clients, for example, AddAgent , LoadScene and Reset commands. Command Setup top # If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods.","title":"Distributed Python API [](#top)"},{"location":"distributed-python-api/#command-setup","text":"If command should be distributed to the clients it has to implement the IDistributedObject interface. Master simulation can modify the arguments that will be sent to the clients inside the Execute methods.","title":"Command Setup"},{"location":"ego-vehicle-dynamics/","text":"EGO Vehicle Dynamics LGSVL Simulator supports multiple dynamics models for EGO vehicles. The default dynamics model is a C# based model that uses Unity's PhysX physics engine and components. The model receives controller input and applies force to the Unity wheel colliders. For users who would like to change or replace vehicle dynamics with their own models, the simulator offers the following ways to do so. Simple Model Interface - pure C# dynamics Full Model Interface - FMI 2.0 supported dynamics Initial Setup # In order to alter the dynamics model in LGSVL Simulator, there are a few steps needed to setup the development environment. Link Download and install Unity Hub Download and install Unity 2019.1.10f1 with Windows and Linux build support modules Download and install Node.js Install git-lfs before cloning LGSVL Simulator Clone LGSVL Simulator repository from GitHub Clone CubeTown repository from GitHub into Assets -> External -> Environments -> CubeTown folder in the newly cloned LGSVL Simulator clone Clone Jaguar2015XE repository from GitHub into Assets -> External -> Vehicles -> Jaguar2015XE folder in the newly cloned LGSVL Simulator clone Navigate to WebUI folder in the root folder of the LGSVL Simulator Open a terminal window and run: npm install Then in the same terminal window run: npm run pack or you can run Simulator -> Build WebUI in Simulator project in Unity Editor Open LGSVL Simulator from Unity Hub Simple Model Interface # Simple Model Interface is our pure C# dynamic model. LGSVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any pure C# dynamic model class as long as it inherits IVehicleDynamics and is compiled in LGSVL Simulator run-time executable. If creating a new SMI class, place in Assets -> Scripts -> Dynamics folder. Future updates will create a dll of the class so it won't need to be compiled in the simulator executable. Simple Model Interface Setup # Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models, e.g., Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the LGSVL Simulator bundle creation window Select CubeTown and Ego vehicle Select Build and after it is completed, in the root of the LGSVL Simulator you will have a folder called AssetBundles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Vehicles, create or assign the new prefab path to the vehicle name you want. Apply any sensors needed in the JSON configuation Under Maps, create or assign the new CubeTown map. Under Simulations add map and vehicle and run Full Model Interface # Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics but will need to be compiled with the simulator. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the LGSVL Simulator bundle creation window Select CubeTown if you have not created a bundle yet and Ego vehicle Select Build and after it is completed, in the root of the LGSVL Simulator you will have a folder called AssetBundles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Vehicles, create or assign the new prefab path to the vehicle name you want. Apply any sensors needed in the JSON configuation Under Maps, create or assign the new CubeTown map if not setup yet Under Simulations add map and vehicle and run Full Model Interface Run-time behavior # When LGSVL Simulator loads a vehicle bundle with an FMU included, it saves the fmu.dll to a folder in Unity's persistent data folder. This folder is contained in the hidden folder AppData in Windows. Here it is loaded by the FMU class, loaded and passed to ExampleVehicleFMU class. Then ExampleVehicleFMU class can call FMU specific methods in the dll. Config.cs will hold references all opened dll's so LGSVL Simulator will not try to open it multiple times. Currently, LGSVL Simulator only supports one FMU in Windows only. Copyright and License # Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Vehicle dynamics"},{"location":"ego-vehicle-dynamics/#initial-setup","text":"In order to alter the dynamics model in LGSVL Simulator, there are a few steps needed to setup the development environment. Link Download and install Unity Hub Download and install Unity 2019.1.10f1 with Windows and Linux build support modules Download and install Node.js Install git-lfs before cloning LGSVL Simulator Clone LGSVL Simulator repository from GitHub Clone CubeTown repository from GitHub into Assets -> External -> Environments -> CubeTown folder in the newly cloned LGSVL Simulator clone Clone Jaguar2015XE repository from GitHub into Assets -> External -> Vehicles -> Jaguar2015XE folder in the newly cloned LGSVL Simulator clone Navigate to WebUI folder in the root folder of the LGSVL Simulator Open a terminal window and run: npm install Then in the same terminal window run: npm run pack or you can run Simulator -> Build WebUI in Simulator project in Unity Editor Open LGSVL Simulator from Unity Hub","title":"Initial Setup"},{"location":"ego-vehicle-dynamics/#simple-model-interface","text":"Simple Model Interface is our pure C# dynamic model. LGSVL Simulator vehicles that are provided, use the VehicleSMI class. It is located in Assets -> Scripts -> Dynamics -> Examples. This class inherits IVehicleDynamics and has required methods for simulation. Users can make any pure C# dynamic model class as long as it inherits IVehicleDynamics and is compiled in LGSVL Simulator run-time executable. If creating a new SMI class, place in Assets -> Scripts -> Dynamics folder. Future updates will create a dll of the class so it won't need to be compiled in the simulator executable.","title":"Simple Model Interface"},{"location":"ego-vehicle-dynamics/#simple-model-interface-setup","text":"Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing with your own Add your new c# dynamics class component that inherits IVehicleDynamics If you plan to use Unity's physics engine, be sure to look at how VehicleSMI caches references to the wheel colliders and wheel meshes. This AxleInfo, in the C# example, enables you to apply force to the wheels and match the movement to the wheel models, e.g., Set any public references needed for your new dynamics model and save the prefab, Ctrl-S Open Simulator -> Build... to open the LGSVL Simulator bundle creation window Select CubeTown and Ego vehicle Select Build and after it is completed, in the root of the LGSVL Simulator you will have a folder called AssetBundles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Vehicles, create or assign the new prefab path to the vehicle name you want. Apply any sensors needed in the JSON configuation Under Maps, create or assign the new CubeTown map. Under Simulations add map and vehicle and run","title":"Simple Model Interface Setup"},{"location":"ego-vehicle-dynamics/#full-model-interface","text":"Full Model Interface supports FMI2.0 Functional Mock-up Interface . Since user FMU's can vary greatly, we have provided an exampleFMU.fmu for testing in Windows only. It can be found in the source code in Assets -> Resources. ExampleVehicleFMU.cs is provided to see how the Full Model Interface system works. It is located in Assets -> Scripts -> Dynamics -> Examples. Users can create their own FMU class that inherits from IVehicleDynamics but will need to be compiled with the simulator. Create or select an EGO vehicle prefab in the Project panel only, not in scene Double click this EGO vehicle prefab. This opens the prefab editor scene Remove VehicleSMI.cs from the vehicle component list in the Inspector panel. This is the component that you are replacing Add Component ExampleVehicleFMU.cs ExampleFMU.fmu requires Unity physics solver Toggle button Non Unity Physics to Unity Physics Set Axles size to 2 Setup Axles data. You will be dragging gameobjects from the Hierarchy panel to the Inspector panel Drag the correct wheel colliders and wheel meshes to public references from the prefab, see SMI setup Enable Motor and Steering for the front axle Set Brake Bias to 0.5f for front and back axles Import the ExampleFMU.fmu Toggle button Import FMU Choose ExampleFMU.fmu in Assets -> Resources -> ExampleFMU.fmu FMU will unpack in the repository vehicle folder in External -> Vehicles -> VehicleName -> FMUName folder FMUImporter.cs will parse the XML file into FMUData in the VehicleFMU class instance on prefab Model Variables will be listed so users can reference by index Each model variable has multiple values that are displayed in the scroll area Open Simulator -> Build... to open the LGSVL Simulator bundle creation window Select CubeTown if you have not created a bundle yet and Ego vehicle Select Build and after it is completed, in the root of the LGSVL Simulator you will have a folder called AssetBundles where the new bundles will be created. Open Loader.scene, run in editor and open the WebUI by pressing the Open Browser... button in the Game panel view Under Vehicles, create or assign the new prefab path to the vehicle name you want. Apply any sensors needed in the JSON configuation Under Maps, create or assign the new CubeTown map if not setup yet Under Simulations add map and vehicle and run","title":"Full Model Interface"},{"location":"ego-vehicle-dynamics/#full-model-interface-run-time-behavior","text":"When LGSVL Simulator loads a vehicle bundle with an FMU included, it saves the fmu.dll to a folder in Unity's persistent data folder. This folder is contained in the hidden folder AppData in Windows. Here it is loaded by the FMU class, loaded and passed to ExampleVehicleFMU class. Then ExampleVehicleFMU class can call FMU specific methods in the dll. Config.cs will hold references all opened dll's so LGSVL Simulator will not try to open it multiple times. Currently, LGSVL Simulator only supports one FMU in Windows only.","title":"Full Model Interface Run-time behavior"},{"location":"ego-vehicle-dynamics/#copyright-and-license","text":"Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"faq/","text":"LGSVL Simulator FAQ What are the recommended system specs? What are the minimum REQUIRED system specs? Does the simulator run on Windows/Mac/Linux? Why does the simulator not open on Linux? Which Unity version is required and how do I get it? Why does my Simulator say \"Invalid: Out of date Assetbundle\"? How do I setup development environment for Unity on Ubuntu? Where are Unity log files located Why are assets/scenes missing/empty after cloning from git? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? ROS Bridge won How do I control the ego vehicle (my vehicle) spawn position? How can I add a custom ego vehicle to LGSVL Simulator? How can I add extra sensors to vehicles in LGSVL Simulator? How can I add a custom map to LGSVL Simulator? How can I create or edit map annotations? Why are pedestrians not spawning when annotated correctly? Why can't I find catkin_make command when building Apollo? Why is Apollo perception module turning on and off all the time? Why does the Apollo vehicle stop at stop line and not cross intersections? Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" Why does Rviz not load the Autoware vector map? Why are there no maps when I make a local build? Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool Why does the simulator start and then say the simulation is \"Invalid\"? Why are there no assets when building the simulator from Unity Editor? Other questions? What are the recommended system specs? What are the minimum REQUIRED system specs? top # For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, Nvidia GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, Nvidia graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and Nvidia drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended. Does the simulator run on Windows/Mac/Linux? top # Officially, you can run LGSVL Simulator on Windows 10 and Ubuntu 16.04 (or later). We do not support macOS at this time. Why does the simulator not open on Linux? top # The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1 Which Unity version is required and how do I get it? top # LGSVL Simulator is currently on Unity version 2019.1.10f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (20191.10f1) here: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 We are constantly working to ensure that LGSVL Simulator runs on the latest version of Unity which supports all of our required functionality. Why does my Simulator say \"Invalid: Out of date Assetbundle\"? top # Assetbundle versions change as we add new features. To get the latest assetbundles, update the URL of the Map/Vehicle to the one on the content website . How do I setup development environment for Unity on Ubuntu? top # Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2019.1.10f1: curl -fLo UnitySetup https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 16.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 16.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code Where are Unity log files located? top # Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LG Silicon Valley Lab\\LGSVL Simulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LG Silicon Valley Lab/LGSVL Simulator/Player.log Linux Editor ~/.config/unity3d/Editor.log Why are assets/scenes missing/empty after cloning from git? top # We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? top # If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git ROS Bridge won't connect? top # First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports. How do I control the ego vehicle (my vehicle) spawn position? top # Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component. How can I add a custom ego vehicle to LGSVL Simulator? top # Please see our tutorial on how to add a new ego vehicle to LGSVL Simulator here . How can I add extra sensors to vehicles in LGSVL Simulator? top # Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the availble sensors. How can I add a custom map to LGSVL Simulator? top # See Maps for details. How can I create or edit map annotations? top # Please see our tutorial on how to add map annotations in LGSVL Simulator here . Why are pedestrians not spawning when annotated correctly? top # LGSVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh. Why can't I find catkin_make command when building Apollo? top # Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo. Why is Apollo perception module turning on and off all the time? top # This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo Why does the Apollo vehicle stop at stop line and not cross intersections? top # Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED). Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" top # This is expected behavior. LGSVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it. Why does Rviz not load the Autoware vector map? top # Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516 Why are there no maps when I make a local build? top # See Build Instructions . It is not required to build the whole simulator using this tool. Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool ? top # Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added. Why does the simulator start and then say the simulation is \"Invalid\"? top # If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified. Why are there no assets when building the simulator from Unity Editor? top # Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project. Other questions? top # See our Github issues page, or email us at contact@lgsvlsimulator.com .","title":"FAQ"},{"location":"faq/#what-are-the-recommended-system-specs-what-are-the-minimum-required-system-specs","text":"For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz Quad core CPU, Nvidia GTX 1080 graphics card (8GB memory), and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, Nvidia graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and Nvidia drivers provide better performance on Windows than on Linux. If Apollo or Autoware will be running on the same system, upgrading to a GPU with at least 10GB memory is recommended.","title":"What are the recommended system specs? What are the minimum REQUIRED system specs?"},{"location":"faq/#does-the-simulator-run-on-windows-mac-linux","text":"Officially, you can run LGSVL Simulator on Windows 10 and Ubuntu 16.04 (or later). We do not support macOS at this time.","title":"Does the simulator run on Windows/Mac/Linux?"},{"location":"faq/#why-does-the-simulator-not-open-on-linux","text":"The Simulator requires the vulkan libraries to be installed on Linux: sudo apt install libvulkan1","title":"Why does the simulator not open on Linux?"},{"location":"faq/#which-unity-version-is-required-and-how-do-i-get-it","text":"LGSVL Simulator is currently on Unity version 2019.1.10f1, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (20191.10f1) here: https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 We are constantly working to ensure that LGSVL Simulator runs on the latest version of Unity which supports all of our required functionality.","title":"Which Unity version is required and how do I get it?"},{"location":"faq/#why-does-my-simulator-say-invalid-out-of-date-assetbundle","text":"Assetbundle versions change as we add new features. To get the latest assetbundles, update the URL of the Map/Vehicle to the one on the content website .","title":"Why does my Simulator say \"Invalid: Out of date Assetbundle\"?"},{"location":"faq/#how-do-i-setup-development-environment-for-unity-on-ubuntu","text":"Install Unity Editor dependencies: sudo apt install \\ gconf-service lib32gcc1 lib32stdc++6 libasound2 libc6 libc6-i386 libcairo2 libcap2 libcups2 \\ libdbus-1-3 libexpat1 libfontconfig1 libfreetype6 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 \\ libgl1 libglib2.0-0 libglu1 libgtk2.0-0 libgtk-3-0 libnspr4 libnss3 libpango1.0-0 libstdc++6 \\ libx11-6 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 \\ libxrender1 libxtst6 zlib1g debconf libgtk2.0-0 libsoup2.4-1 libarchive13 libpng16-16 Download and install Unity 2019.1.10f1: curl -fLo UnitySetup https://beta.unity3d.com/download/f007ed779b7a/UnitySetup-2019.1.10f1 chmod +x UnitySetup ./UnitySetup --unattended --install-location=/opt/Unity --components=Unity Install .NET Core SDK, available from https://dotnet.microsoft.com/download On Ubuntu 16.04 run following commands: wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb sudo apt-get install apt-transport-https sudo apt-get update sudo apt-get install dotnet-sdk-2.2 Install Mono, available from https://www.mono-project.com/download/stable/#download-lin On Ubuntu 16.04 run following commands: sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF sudo apt install apt-transport-https ca-certificates echo \"deb https://download.mono-project.com/repo/ubuntu stable-xenial main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list sudo apt update sudo apt install mono-devel Install Visual Studio Code, available in Ubuntu Software or from https://code.visualstudio.com/docs/setup/linux Open VS Code and install C# extension Press Ctrl+Shift+X Search for C# Install extension C# for Visual Studio Code (powered by OmniSharp) Install Unity Debug Extension, available here: https://marketplace.visualstudio.com/items?itemName=Unity.unity-debug Set Unity preferences to use VS Code. See instructions here: https://code.visualstudio.com/docs/other/unity#_setup-vs-code-as-unity-script-editor To find out where Code is installed use which code","title":"How do I setup development environment for Unity on Ubuntu?"},{"location":"faq/#where-are-unity-log-files-located","text":"Version Location Windows Binary %APPDATA%\\..\\LocalLow\\LG Silicon Valley Lab\\LGSVL Simulator\\Player.log Windows Editor %APPDATA%\\..\\Local\\Unity\\Editor\\Editor.log Linux Binary ~/.config/unity3d/LG Silicon Valley Lab/LGSVL Simulator/Player.log Linux Editor ~/.config/unity3d/Editor.log","title":"Where are Unity log files located"},{"location":"faq/#why-are-assets-scenes-missing-empty-after-cloning-from-git","text":"We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name \"WebSocketSharp\" could not be found. Are you missing a using directive or an assembly reference?","title":"Why are assets/scenes missing/empty after cloning from git?"},{"location":"faq/#why-do-i-get-an-error-saying-som-files-e-g-rosbridge-websocket-launch-are-missing-in-apollo","text":"If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo?"},{"location":"faq/#ros-bridge-won-t-connect","text":"First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports.","title":"ROS Bridge won"},{"location":"faq/#how-do-i-control-the-ego-vehicle-my-vehicle-spawn-position","text":"Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component.","title":"How do I control the ego vehicle (my vehicle) spawn position?"},{"location":"faq/#how-can-i-add-a-custom-ego-vehicle-to-lgsvl-simulator","text":"Please see our tutorial on how to add a new ego vehicle to LGSVL Simulator here .","title":"How can I add a custom ego vehicle to LGSVL Simulator?"},{"location":"faq/#how-can-i-add-extra-sensors-to-vehicles-in-lgsvl-simulator","text":"Adding sensors to a vehicle is done by editing the configuration JSON in the WebUI. See Sensor JSON Options for details on all the availble sensors.","title":"How can I add extra sensors to vehicles in LGSVL Simulator?"},{"location":"faq/#how-can-i-add-a-custom-map-to-lgsvl-simulator","text":"See Maps for details.","title":"How can I add a custom map to LGSVL Simulator?"},{"location":"faq/#how-can-i-create-or-edit-map-annotations","text":"Please see our tutorial on how to add map annotations in LGSVL Simulator here .","title":"How can I create or edit map annotations?"},{"location":"faq/#why-are-pedestrians-not-spawning-when-annotated-correctly","text":"LGSVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window -> AI -> Navigation and bake the NavMesh.","title":"Why are pedestrians not spawning when annotated correctly?"},{"location":"faq/#why-can-t-i-find-catkin-make-command-when-building-apollo","text":"Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo.","title":"Why can't I find catkin_make command when building Apollo?"},{"location":"faq/#why-is-apollo-perception-module-turning-on-and-off-all-the-time","text":"This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo","title":"Why is Apollo perception module turning on and off all the time?"},{"location":"faq/#why-does-the-apollo-vehicle-stop-at-stop-line-and-not-cross-intersections","text":"Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED).","title":"Why does the Apollo vehicle stop at stop line and not cross intersections?"},{"location":"faq/#dreamview-in-apollo-shows-hardware-gps-triggers-safety-mode-no-gnss-status-message","text":"This is expected behavior. LGSVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it.","title":"Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\""},{"location":"faq/#why-does-rviz-not-load-the-autoware-vector-map","text":"Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516","title":"Why does Rviz not load the Autoware vector map?"},{"location":"faq/#why-are-there-no-maps-when-i-make-a-local-build","text":"See Build Instructions . It is not required to build the whole simulator using this tool.","title":"Why are there no maps when I make a local build?"},{"location":"faq/#what-is-the-target-waypoint-missing-when-using-the-map-annotation-tool","text":"Make sure the meshes that make up the road have the Default layer assigned to them and they have a Mesh Collider added.","title":"Why is the TARGET_WAYPOINT missing when using the Map Annotation Tool"},{"location":"faq/#why-does-the-simulator-start-and-then-say-the-simulation-is-invalid","text":"If the vehicle(s) selected for the simulation have a bridge, then a Bridge Connection String is required. The format of the string is IP:port (e.g. localhost:9090 ). The simulator does not assume a port so it must be specified.","title":"Why does the simulator start and then say the simulation is \"Invalid\"?"},{"location":"faq/#why-are-there-no-assets-when-building-the-simulator-from-unity-editor","text":"Assets (environments and vehicles) are not included in the main simulator repository to reduce it's size. Maps and vehicles can get large because of 3D assets and textures. See Adding Assets for instructions on how to add assets to the project.","title":"Why are there no assets when building the simulator from Unity Editor?"},{"location":"faq/#other-questions","text":"See our Github issues page, or email us at contact@lgsvlsimulator.com .","title":"Other questions?"},{"location":"getting-started/","text":"LGSVL Simulator: An Autonomous Vehicle Simulator Website | Documentation | Download # Table of Contents Website | Documentation | Download Introduction Getting Started Downloading and starting simulator Building and running from source Simulator Instructions Guide to simulator functionality Contact Copyright and License Introduction top # LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the TierIV's Autoware and Baidu's Apollo 5.0 and Apollo 3.0 platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo 5.0 fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork . Getting Started top # You can find complete and the most up-to-date guides on our documentation website . Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Quad core CPU Nvidia GTX 1080 (8GB memory) Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux. If running Apollo or Autoware on the same system as the Simulator, it is recommended to upgrade to a GPU with at least 10GB memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required). Downloading and starting simulator top # Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Verify that the latest NVIDIA driver is installed On Windows Open NVIDIA GeForce Experience Go the Drivers tab Check for Updates Download and install update if available Alternatively download driver manually from NVIDIA's website On Ubuntu Add the Graphics Drivers PPA from here under Adding this PPA to your system Open Software & Updates Go to the Additional Drivers tab Select the latest NVIDIA driver In a terminal also install libvulkan with sudo apt install libvulkan1 Unzip the downloaded folder and run the executable. Building and running from source # Check out our instructions for getting started with building from source here . Simulator Instructions top # After starting the simulator, you should see a button \"Open Browser...\" to open the UI in the browser. Click the button. Go to the Simulations tab and select the appropriate map and vehicle. For a standard setup, select \"BorregasAve\" for map and \"Jaguar2015XE (Apollo 5.0)\" for vehicle. Click \"Run\" to begin. The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo 5.0 repositories for instructions on running the platforms with the simulator. Guide to simulator functionality top # Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator. Contact top # Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at contact@lgsvlsimulator.com . Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Getting started"},{"location":"getting-started/#website-documentation-download","text":"","title":"Website | Documentation | Download"},{"location":"getting-started/#introduction","text":"LG Electronics America R&D Center has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the TierIV's Autoware and Baidu's Apollo 5.0 and Apollo 3.0 platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo 5.0 fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork .","title":"Introduction"},{"location":"getting-started/#getting-started","text":"You can find complete and the most up-to-date guides on our documentation website . Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Quad core CPU Nvidia GTX 1080 (8GB memory) Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux. If running Apollo or Autoware on the same system as the Simulator, it is recommended to upgrade to a GPU with at least 10GB memory. If running Apollo or Autoware on a different system as the Simulator, a gigabit connection between the systems is required (a gigabit switch is sufficient, gigabit internet is not required).","title":"Getting Started"},{"location":"getting-started/#downloading-and-starting-simulator","text":"Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Verify that the latest NVIDIA driver is installed On Windows Open NVIDIA GeForce Experience Go the Drivers tab Check for Updates Download and install update if available Alternatively download driver manually from NVIDIA's website On Ubuntu Add the Graphics Drivers PPA from here under Adding this PPA to your system Open Software & Updates Go to the Additional Drivers tab Select the latest NVIDIA driver In a terminal also install libvulkan with sudo apt install libvulkan1 Unzip the downloaded folder and run the executable.","title":"Downloading and starting simulator"},{"location":"getting-started/#building-and-running-from-source","text":"Check out our instructions for getting started with building from source here .","title":"Building and running from source"},{"location":"getting-started/#simulator-instructions","text":"After starting the simulator, you should see a button \"Open Browser...\" to open the UI in the browser. Click the button. Go to the Simulations tab and select the appropriate map and vehicle. For a standard setup, select \"BorregasAve\" for map and \"Jaguar2015XE (Apollo 5.0)\" for vehicle. Click \"Run\" to begin. The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo 5.0 repositories for instructions on running the platforms with the simulator.","title":"Simulator Instructions"},{"location":"getting-started/#guide-to-simulator-functionality","text":"Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator.","title":"Guide to simulator functionality"},{"location":"getting-started/#contact","text":"Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at contact@lgsvlsimulator.com .","title":"Contact"},{"location":"getting-started/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"ground-truth-json-example/","text":"Example JSON Configuration For Data Collection Bridge Type top # CyberRT Published Topics top # Topic Sensor Name /apollo/sensor/camera/front_6mm/image/compressed Main Camera /simulator/ground_truth/3d_detections 3D Ground Truth /simulator/ground_truth/2d_detections 2D Ground Truth /simulator/depth_camera Depth Camera /simulator/semantic_camera Semantic Camera Subscribed Topics top # Topic Sensor Name /simulator/ground_truth/3d_visualize 3D Ground Truth Visualizer /simulator/ground_truth/2d_visualize 2D Ground Truth Visualizer JSON Configuration top # [ { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"DetectionRange\": 100, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } ]","title":"Sample sensor configuration for data collection"},{"location":"ground-truth-json-example/#bridge-type","text":"CyberRT","title":"Bridge Type"},{"location":"ground-truth-json-example/#published-topics","text":"Topic Sensor Name /apollo/sensor/camera/front_6mm/image/compressed Main Camera /simulator/ground_truth/3d_detections 3D Ground Truth /simulator/ground_truth/2d_detections 2D Ground Truth /simulator/depth_camera Depth Camera /simulator/semantic_camera Semantic Camera","title":"Published Topics"},{"location":"ground-truth-json-example/#subscribed-topics","text":"Topic Sensor Name /simulator/ground_truth/3d_visualize 3D Ground Truth Visualizer /simulator/ground_truth/2d_visualize 2D Ground Truth Visualizer","title":"Subscribed Topics"},{"location":"ground-truth-json-example/#json-configuration","text":"[ { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/apollo/sensor/camera/front_6mm/image/compressed\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"DetectionRange\": 100, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Semantic Camera\", \"name\": \"Semantic Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/semantic_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } ]","title":"JSON Configuration"},{"location":"keyboard-shortcuts/","text":"Simulator Controls Key Bindings # Officially supported: # F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam Camera Controls # Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down For developer use: # N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location Miscellaneous # H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker Logitech G920 Wheel Inputs # Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Keyboard shortcuts"},{"location":"keyboard-shortcuts/#key-bindings","text":"","title":"Key Bindings"},{"location":"keyboard-shortcuts/#officially-supported","text":"F1 - Help menu \u2190 \u2191 \u2193 \u2192 - Drive vehicle forward/brake, turn 1 - 0 - Agent select and follow cam","title":"Officially supported:"},{"location":"keyboard-shortcuts/#camera-controls","text":"Mouse Right-Click hold & drag - Look/Rotate ~ - Free Camera W S - Zoom A D - Strafe Q E - Up/Down","title":"Camera Controls"},{"location":"keyboard-shortcuts/#for-developer-use","text":"N - Toggle Non-Player Character (NPC) vehicles P - Toggle Pedestrians End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear F12 - Reset Active Vehicle location","title":"For developer use:"},{"location":"keyboard-shortcuts/#miscellaneous","text":"H - Toggle headlights F - Toggle fog lights I - Toggle interior light Right Shift - Toggle parking brake (needs to be off for vehicle to move) M - Toggle hazard lights < - Toggle left blinker > - Toggle right blinker","title":"Miscellaneous"},{"location":"keyboard-shortcuts/#logitech-g920-wheel-inputs","text":"Right Pedal RB - Accelerate Middle Pedal LB - Brake Rotate Wheel - Turn A - Cycle through Agents B - Toggle Non-Player Character (NPC) vehicles X - Toggle Sensor Visualizers Y - Cycle through Camera states (Follow camera, Cinematic camera, Free camera) Start - [Hamburger Menu/3 horizonal lines] Pause Simulation (Interactive Simulation Only) Select - [Overlapping Squares] Open Menu RSB - Cycle through Agent headlight states LSB - Toggle Reverse Center - [Xbox] Toggle Interior Light (if available)","title":"Logitech G920 Wheel Inputs"},{"location":"lane-following/","text":"ROS2 End-to-End Lane Following Model with LGSVL Simulator This tutorial works with Simulator Release 2019.05 This documentation describes applying a deep learning neural network for lane following in LGSVL Simulator . In this project, we use LGSVL Simulator for customizing sensors (one main camera and two side cameras) for a car, collect data for training, and deploying and testing a trained model. This project was inspired by NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Video # Table of Contents # Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with LGSVL Simulator Collect data from LGSVL Simulator Data preprocessing Train a model Drive with your trained model in LGSVL Simulator Future Works and Contributing References Getting Started # First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for LGSVL Simulator to connect. Prerequisites # Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU) Setup # Installing Docker CE # To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker # Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image # docker pull lgsvl/lanefollowing:latest What's inside Docker Image # Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Crystal + rosbridge Jupyter Notebook Features # Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend Training Details # Network Architecture # The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11 Hyperparameters # Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2 Dataset # Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images Center Image # Left Image # Right Image # Original Image # Cropped Image # Data Distribution # How to Collect Data and Train Your Own Model with LGSVL Simulator # Collect data from LGSVL Simulator # To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (geometry_msgs/TwistStamped) To launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect To drive a car and publish messages over rosbridge in training mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera , Left Camera , Right Camera , and check Publish Control Command The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files. Data preprocessing # Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training. Train a model # We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously. Drive with your trained model in LGSVL Simulator # Now, it's time to deploy your trained model and test drive with it using LGSVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual To drive a car in autonomous mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera (we don't need side cameras for inference) Your car will start driving autonomously and try to mimic your driving behavior when training the model. Future Works and Contributing # Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network) References # Lane Following Github Repository LGSVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Deep Learning lane following model"},{"location":"lane-following/#video","text":"","title":"Video"},{"location":"lane-following/#table-of-contents","text":"Getting Started Prerequisites Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Features Training Details Network Architecture Hyperparameters Dataset How to Collect Data and Train Your Own Model with LGSVL Simulator Collect data from LGSVL Simulator Data preprocessing Train a model Drive with your trained model in LGSVL Simulator Future Works and Contributing References","title":"Table of Contents"},{"location":"lane-following/#getting-started","text":"First, clone this repository: git clone --recurse-submodules https://github.com/lgsvl/lanefollowing.git Next, pull the latest Docker image: docker pull lgsvl/lanefollowing:latest To build ROS2 packages: docker-compose up build Now, launch the lane following model: docker-compose up drive (Optional) If you want visualizations, run drive_visual instead of drive: docker-compose up drive_visual That's it! Now, the lane following ROS2 node and the rosbridge should be up and running, waiting for LGSVL Simulator to connect.","title":"Getting Started"},{"location":"lane-following/#prerequisites","text":"Docker CE NVIDIA Docker NVIDIA graphics card (required for training/inference with GPU)","title":"Prerequisites"},{"location":"lane-following/#setup","text":"","title":"Setup"},{"location":"lane-following/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"lane-following/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"lane-following/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"lane-following/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS2 Crystal + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"lane-following/#features","text":"Training mode: Manually drive the vehicle and collect data Autonomous Mode: The vehicle drives itself based on Lane Following model trained from the collected data ROS2-based Time synchronous data collection node deploying a trained model in a node Data preprocessing for training Data normalization Data augmentation Splitting data into training set and test set Writing/Reading data in HDF5 format Deep Learning model training: Train a model using Keras with TensorFlow backend","title":"Features"},{"location":"lane-following/#training-details","text":"","title":"Training Details"},{"location":"lane-following/#network-architecture","text":"The network has 559,419 parameters and consists of 9 layers, including 5 convolutional layers, 3 fully connected layers, and an output layer. Layer (type) Output Shape Param # lambda_1 (Lambda) (None, 70, 320, 3) 0 conv2d_1 (Conv2D) (None, 33, 158, 24) 1824 conv2d_2 (Conv2D) (None, 15, 77, 36) 21636 conv2d_3 (Conv2D) (None, 6, 37, 48) 43248 conv2d_4 (Conv2D) (None, 4, 35, 64) 27712 conv2d_5 (Conv2D) (None, 2, 33, 64) 36928 dropout_1 (Dropout) (None, 2, 33, 64) 0 flatten_1 (Flatten) (None, 4224) 0 dense_1 (Dense) (None, 100) 422500 dense_2 (Dense) (None, 50) 5050 dense_3 (Dense) (None, 10) 510 dense_4 (Dense) (None, 1) 11","title":"Network Architecture"},{"location":"lane-following/#hyperparameters","text":"Learning rate: 1e-04 Learning rate decay: None Dropout rate: 0.5 Mini-batch size: 128 Epochs: 30 Optimization algorithm: Adam Loss function: Mean squared error Training/Test set ratio: 8:2","title":"Hyperparameters"},{"location":"lane-following/#dataset","text":"Number of training data: 48,624 labeled images Number of validation data: 12,156 labeled images","title":"Dataset"},{"location":"lane-following/#center-image","text":"","title":"Center Image"},{"location":"lane-following/#left-image","text":"","title":"Left Image"},{"location":"lane-following/#right-image","text":"","title":"Right Image"},{"location":"lane-following/#original-image","text":"","title":"Original Image"},{"location":"lane-following/#cropped-image","text":"","title":"Cropped Image"},{"location":"lane-following/#data-distribution","text":"","title":"Data Distribution"},{"location":"lane-following/#how-to-collect-data-and-train-your-own-model-with-lgsvl-simulator","text":"","title":"How to Collect Data and Train Your Own Model with LGSVL Simulator"},{"location":"lane-following/#collect-data-from-lgsvl-simulator","text":"To collect camera images as well as corresponding steering commands for training, we provide a ROS2 collect node which subscribes to three camera image topics and a control command topic, approximately synchronizes time stamps of those messages, and then saves them as csv and jpg files. The topic names and types are as below: - Center camera: /simulator/sensor/camera/center/compressed (sensor_msgs/CompressedImage) - Left camera: /simulator/sensor/camera/left/compressed (sensor_msgs/CompressedImage) - Right camera: /simulator/sensor/camera/right/compressed (sensor_msgs/CompressedImage) - Control command: /simulator/control/command (geometry_msgs/TwistStamped) To launch rosbridge and collect ROS2 node in a terminal: docker-compose up collect To drive a car and publish messages over rosbridge in training mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera , Left Camera , Right Camera , and check Publish Control Command The node will start collecting data as you drive the car around. You should be able to check log messages in the terminal where the collect node is running. The final data is saved in lanefollowing/ros2_ws/src/lane_following/train/data/ as csv and jpg files.","title":"Collect data from LGSVL Simulator"},{"location":"lane-following/#data-preprocessing","text":"Before start training your model with the data you collected, data preprocessing is required. This task includes: - Resize image resolutions from 1920 x 1080 to 200 x 112 - Crop top portion of images as we are mostly interested in road part of an image - Data augmentation by adding artificial bias to side camera images or flipping images - Data normalization to help the model converge faster - Split data into training and testing dataset To run data preprocessing and obtain datasets for training: docker-compose up preprocess This will preprocess your data and write outputs in lanefollowing/ros2_ws/src/lane_following/train/data/hdf5/ into HDF5 format for better I/O performance for training.","title":"Data preprocessing"},{"location":"lane-following/#train-a-model","text":"We use Keras with TensorFlow backend for training our model as an example. The hyperparameters such as learning rate, batch size, or number of epochs were chosen empirically. You can train a model as is but you are also welcome to modify the model architecture or any hyperparameters as you like in the code. To start training: docker-compose up train After training is done, your final trained model will be in lanefollowing/ros2_ws/src/lane_following/train/model/{current-date-and-time-in-utc}.h5 and your model is ready to drive autonomously.","title":"Train a model"},{"location":"lane-following/#drive-with-your-trained-model-in-lgsvl-simulator","text":"Now, it's time to deploy your trained model and test drive with it using LGSVL Simulator. You can replace your trained model with an existing one in lanefollowing/ros2_ws/src/lane_following/model/model.h5 as this is the path for deployment. To launch rosbridge and drive ROS2 node in a terminal: docker-compose up drive Or, if you want visualizations as well, run drive_visual instead: docker-compose up drive_visual To drive a car in autonomous mode: - Launch LGSVL Simulator - Click Free Roaming mode - Select San Francisco map and XE_Rigged-lgsvl vehicle - Make sure the simulator establishes connection with rosbridge - Click Run to begin - Enable Main Camera (we don't need side cameras for inference) Your car will start driving autonomously and try to mimic your driving behavior when training the model.","title":"Drive with your trained model in LGSVL Simulator"},{"location":"lane-following/#future-works-and-contributing","text":"Though the network can successfully drive and follow lanes on the bridge, there's still a lot of room for future improvements (i.e., biased to drive straight, afraid of shadows, few training data, and etc). - To improve model robustness collect more training data by driving in a wide variety of environments - Changing weather and lighting effects (rain, fog, road wetness, time of day) - Adding more road layouts and road textures - Adding more shadows on roads - Adding NPC cars around the ego vehicle - Predict the car throttle along with the steering angle - Take into accounts time series analysis using RNN (Recurrent Neural Network)","title":"Future Works and Contributing"},{"location":"lane-following/#references","text":"Lane Following Github Repository LGSVL Simulator NVIDIA's End-to-End Deep Learning Model for Self-Driving Cars","title":"References"},{"location":"lane-following/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"lgsvl-msgs/","text":"lgsvl_msgs lgsvl_msgs is a ROS / ROS2 hybrid package that provides AD stack agnostic message definitions for interfacing with the LGSVL Simualtor. The package contains the following definitions: - Detection3DArray.msg # A list of 3D detections - Detection3D.msg # 3D detection including id, label, score, and 3D bounding box - BoundingBox3D.msg # A 3D bounding box definition - Detection2DArray.msg # A list of 2D detections - Detection2D.msg # 2D detection including id, label, score, and 2D bounding box - BoundingBox2D.msg # A 2D bounding box definition - SignalArray.msg # A list of traffic light detections - Signal.msg # 3D detection of a traffic light including id, label, score, and 3D bounding box - CanBusData.msg # Can bus data for an ego vehicle published by the simulator - VehicleControlData.msg # Defines vehicle control data the simulator subscribes to - VehicleStateData.msg # Data published by the simulator describing the full state of an ego vehicle Installation lgsvl_msgs is built by the ROS build farm. Binaries can be installed on Ubuntu as follows: sudo apt update sudo apt install ros-$ROS_DISTRO-lgsvl_msgs Note: It may take up to a week for the builds to become available after a release. Building from source ROS Create a catkin workspace for the package and clone the repository into the source folder: mkdir -p catkin_ws/src cd catkin_ws/src git clone https://github.com/lgsvl/lgsvl_msgs.git Build the package from workspace root: cd .. catkin_make Source in terminal where rosnodes who publish / subscribe to the messages are running: source devel/setup.bash ROS 2 Clone repository and build with colcon: git clone https://github.com/lgsvl/lgsvl_msgs.git cd lgsvl_msgs colcon build Source in terminal where rosnodes who publish / subscribe to the messages are running: source install/setup.bash","title":"The lgsvl_msgs package"},{"location":"lidar-plugin/","text":"Lidar Sensor Plugin This page introduces the Velodyne Lidar Sensor plugin, as well as how to build your own Lidar sensor plugin. Table of Contents Velodyne Lidar Sensor Plugin Velodyne Lidar Sensor JSON options Velodyne Lidar Sensor Usage Running with Autoware Running with Apollo 5.0 Build Your Own Lidar Sensor Plugin Velodyne Lidar Sensor Plugin top # This sensor plugin is for Velodyne Lidar . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded LGSVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne Lidar Sensor is implemented following exact intrinsics of real Velodyne Lidar, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne Lidar sensor has azimuth offset same as the real Lidar, while the normal Lidar Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard Lidar Sensor , which generates point cloud and publishes it via bridge, Velodyne Lidar sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth. Velodyne Lidar Sensor JSON options top # Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne Lidar String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as Lidar Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne Lidar plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne Lidar since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidar\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Velodyne Lidar Sensor Usage top # Running with Autoware # Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the LGSVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have LGSVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C Lidar in LGSVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C Lidar in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it. Running with Apollo 5.0 # Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the Lidar model if your Lidar setting is different to the default setting of Apollo 5.0. To configure the Lidar model, you can edit velodyne.dag file. Note that if more than one Lidar is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne Lidar sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 Lidars. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne Lidar sensor into our sample JSON . If you have LGSVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C Lidar visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C Lidar in Cyber Visualizer. Build Your Own Lidar Sensor Plugin top # If you want to build your own Lidar sensor plugin to support other Lidar models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"Lidar sensor plugin"},{"location":"lidar-plugin/#velodyne-lidar-sensor-plugins","text":"This sensor plugin is for Velodyne Lidar . VLP-16, VLP-32C and VLS-128 are currently supported. The built asset bundle of this plugin (named sensor_VelodyneLidarSensor ) can be found in AssetBundles/Sensors folder when you unzip the downloaded LGSVL Simulator (i.e. in the same level of the Simulator executable). The Velodyne Lidar Sensor is implemented following exact intrinsics of real Velodyne Lidar, such as elevation angles and azimuth offsets. Particularly, each laser beam in Velodyne Lidar sensor has azimuth offset same as the real Lidar, while the normal Lidar Sensor assumes all laser beams are on same vertical line (i.e. no azimuth offset). In contrast to the standard Lidar Sensor , which generates point cloud and publishes it via bridge, Velodyne Lidar sensor generates data packets and position packets and sends them out via UDP socket. Velodyne driver running on the host machine (the machine which receives the packets) is responsible for converting these packets into point cloud and publish it out. This will greatly alleviate the burden on bridge bandwidth, so that the simulation can support more sensors (e.g. camera sensors) simultaneously. See this issue for an example of exhausted bridge bandwidth.","title":"Velodyne Lidar Sensor Plugin"},{"location":"lidar-plugin/#velodyne-lidar-sensor-json-options","text":"Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF VelodyneLidarType defines type of Velodyne Lidar String VLP_16 HostName IP address of host String UDPPortData UDP port for data packets Int 2368 UDPPortPosition UDP port for position packets Int 8308 * Most of parameters except the last four are same as Lidar Sensor . Details of last four parameters are as follows: Value of VelodyneLidarType can only be \"VLP_16\", \"VLP_32C\" or \"VLS_128\". Note that it uses underscore ('_') not dash ('-'). HostName is the IP address of the machine which receives the UDP packets (a.k.a. host machine). UDPPortData and UDPPortPosition are UPD ports for data packets and position packets. If more than one Velodyne Lidar plugin is used, each one should have a unique port. VerticalRayAngles , LaserCount , FieldOfView , and CenterAngle will be ignored for Velodyne Lidar since they will be set internally following the corresponding model spec. VLP-32C configuration sample: { \"type\": \"VelodyneLidar\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\", \"VelodyneLidarType\": \"VLP_32C\", \"HostName\": \"127.0.0.1\", \"UdpPortData\": 2368, \"UdpPortPosition\": 8308 }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Velodyne Lidar Sensor JSON options"},{"location":"lidar-plugin/#velodyne-lidar-sensor-usage","text":"","title":"Velodyne Lidar Sensor Usage"},{"location":"lidar-plugin/#running-with-autoware","text":"Autoware is based on ROS. For ROS-based systems, ROS Velodyne driver can be used. Detailed steps of running ROS Velodyne driver are as follows: 1. Create a workspace folder and enter it mkdir velodyne_ws && cd velodyne_ws 2. Clone ROS Velodyne driver into src folder git clone https://github.com/ros-drivers/velodyne.git src 3. Build the Velodyne drive as a ROS node catkin_make 4. Setup running environment source /opt/$ROS_DISTRO/setup.bash source devel/setup.bash 5. Configuration of device IP Before running the Velodyne driver, you need to modify the launch files to setup device IP (i.e. the IP of the machine where the LGSVL Simulator is running). For VLP-16, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP16_points.launch and put the device IP after device_ip . For VLP-32, edit velodyne_ws/src/velodyne/velodyne_pointcloud/launch/VLP-32C_points.launch and put the device IP after device_ip . ROS Velodyne driver does not support VLS-128 for now. For more details please refer to the official page . 6. Launch Velodyne driver For VLP-16: roslaunch velodyne_pointcloud VLP16_points.launch For VLP-32C: roslaunch velodyne_pointcloud VLP-32C_points.launch If you have LGSVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and ROS topic /velodyne_points is published by the driver. You can also use RViz to visualize the point cloud in that topic. Fig. 1 shows point cloud of VLP-32C visualized in the simulator, Fig. 1: Visualized point clouds of VLP-32C Lidar in LGSVL Simulator. and Fig. 2 shows the same point cloud visualized in RViz (click to see in full resolution): Fig. 2: Visualized point clouds of VLP-32C Lidar in RViz. Note that the output topic name ( /velodyne_points ) of ROS Velodyne driver is hard-coded and not configurable, while Autoware assumes point cloud published into ROS topic /points_raw . To have ROS Velodyne driver running with Autoware, you have to either use <remap> tag in Autoware launch files to may /velodyne_points to /points_raw , or modify the topic name in the source code of ROS Velodyne driver and rebuild it.","title":"Running with Autoware"},{"location":"lidar-plugin/#running-with-apollo-50","text":"Apollo 5.0 is based on CyberRT and comes with its own Velodyne driver . Detailed steps of running ROS Velodyne driver are as follows: 1. Follow these instructions to start Apollo 5.0 and launch bridge. 2. (optional) Configure the Lidar model if your Lidar setting is different to the default setting of Apollo 5.0. To configure the Lidar model, you can edit velodyne.dag file. Note that if more than one Lidar is used, each has different data port and position port (configured in their corresponding .conf files . You need to set UDPPortData and UDPPortPosition for each Velodyne Lidar sensor accordingly. The default launch file and dag file of Apollo 5.0 use VLS-128 and VLP-16 Lidars. If you need to use VLP-32C, in addition to add VLP-32C to dag file, you may need to modify static_transform_conf.pb.txt to include your own VLP-32C extrinsics if you want to get compensated point cloud. 3. Launch GPS , Localization , Transform , and Velodyne modules in Module Controller page of Dreamview. Fig. 3 shows the Dreamview web interface: Fig. 3: Dreamview web interface. On the Simulator side, you can add Velodyne Lidar sensor into our sample JSON . If you have LGSVL Simulator running on client machine, you should be able to see UDP packets received on both data port and position port, and cyber_monitor should shows point clouds published into corresponding Cyber channels. You can also use cyber_visualizer to visualize the point cloud in those channels. Fig. 4 shows the point cloud of VLP-32C Lidar visualized in Cyber Visualizer (click to see in full resolution): Fig. 4: Visualized point clouds of VLP-32C Lidar in Cyber Visualizer.","title":"Running with Apollo 5.0"},{"location":"lidar-plugin/#build-your-own-lidar-sensor-plugin","text":"If you want to build your own Lidar sensor plugin to support other Lidar models, you can follow the general instructions on building sensor plugins. Instead of deriving your plugin class from SensorBase , you can derive your class from LidarSensorBase , so that you can reuse most of the code there, focusing only on raw data generation and sending.","title":"Build Your Own Lidar Sensor Plugin"},{"location":"map-annotation/","text":"Map Annotation The LGSVL Simulator supports creating, editing, and exporting of HD maps of existing 3D environments (Unity scenes). The maps can be saved in the currently supported Apollo, Autoware or Lanelet2 formats. Currently, annotating map is only recommended while running the simulator as a Unity project in a Windows environment. Table of Contents Creating a New Map Annotate Lanes Create Parent Object Make Lanes Make Boundary Lines Annotate Intersections Create Parent Object Create Intersection Lanes Create Traffic Signals Create Traffic Signs Create Stop Lines Create Pole Annotate Self-Reversing Lanes Annotation Information Annotate Self-Reversing Lanes under Traffic Lanes Annotate Self-Reversing Lanes under Intersections Annotate Other Features Create Pedestrian Path Create Junction Create Crosswalk Create Clear Area Create Parking Space Create Speed Bump Export Map Annotations Import Map Annotations Map Formats Creating a New Map top # Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Make sure your roads has added Mesh Collider . Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show existing map annotation. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All Annotate Lanes top # Create Parent Object top # In the Map prefab (if you don't have one, you can create an empty GameObject and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes section 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane Make Lanes top # Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to deterin how many waypoints to add in between Verify Lane is the selected Map Object Type Verify NO_TURN is the selected Lane Turn Type Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted Make Boundary Lines top # Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible If you are annotating map for Lanelet2 format, you also need to annotate boundary lines for every lane and drag them into the corresponding field in the lane object. Annotate Intersections top # Create Parent Object top # In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated. Adjust the Trigger Bounds of the MapIntersection so that the box covers the center of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection Create Intersection Lanes top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating for Lanelet2, you also need to annotate boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line For NPCs to properly navigate an intersection, the Yield To Lanes list must be manually filled in. With View Selected toggled in the Map Annotation Tool, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list Create Traffic Signals top # Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The annotation directions must match the directions of the mesh. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal Create Traffic Signs top # Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box Create Stop Lines top # Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stopline and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should be in the same direction as the lanes. A StopLine needs to with the lanes that approch it. The last waypoint of approaching lanes should be past the line. Create Pole top # Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights Annotate Self-Reversing Lanes top # These types of lanes are only supported on Apollo 5.0 Annotation Information top # There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane. Annotate Self-Reversing Lanes under Traffic Lanes top # Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Self-Reversing Lanes under Intersections top # Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane1_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane1_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane1_forward, MapLane1_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way. Annotate Other Features top # Other annotation features may be included in the TrafficLane or Intersection objects or they may be sorted into other parent objects. Create Pedestrian Path top # This annotation controls where pedestrians will pass with the highest priority. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object Create Junction top # Junction annotations can be used by the AD stack if needed. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction Create Crosswalk top # Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk Create Clear Area top # Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea Create Parking Space top # Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space. Create Speed Bump top # Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints. Export Map Annotations top # HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map Import Map Annotations top # The simulator can import a variety of formats of annotated map. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly. Map Formats top # For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map annotation"},{"location":"map-annotation/#creating-a-new-map","text":"Make sure your roads belong to the layer of Default since waypoints will be only created on this layer. Make sure your roads has added Mesh Collider . Open HD Map Annotation in Unity : Simulator -> Annotate HD Map By default, map annotation is not shown. Click View All under View Modes to show existing map annotation. Before annotating, drag in the correct Parent Object , for example TrafficLanes . Then every new object you create will be under TrafficLanes object. The expected object hierarchy is as follows: Map - This prefab which will be the object containing all the HD Annotations. The MapHolder script should be added to this prefab TrafficLanes - This object will hold all of the MapLaneSection and MapLane which cannot be grouped under MapLaneSection MapLaneSection - This will hold all of the lanes in a section of road MapLane - A single annotated lane Intersections - This object will hold all of the MapIntersection MapIntersection - This object will hold all of the annotations for an intersection MapLane - A single annotated lane MapLine - A single annotated line (e.g. a stop line) MapSignal - A single annotated traffic signal MapSign - A single annotated traffic sign (e.g. a stop sign) BoundaryLines - This object will hold all of boundary lines To make Map a prefab, drag it from the scene Hierarchy into the Project folder After annotation is done, remember to save: select the Map prefab, in the Inspector click Overrides -> Apply All","title":"Creating a New Map"},{"location":"map-annotation/#annotate-lanes","text":"","title":"Annotate Lanes"},{"location":"map-annotation/#create-parent-object-lanes","text":"In the Map prefab (if you don't have one, you can create an empty GameObject and make it a prefab), create a new object and name it \"TrafficLanes\" In the Inspector of the Map , drag the new TrafficLanes object into the Traffic Lanes holder In TrafficLanes , create a new object Add the MapLaneSection script to the object and position it close the section of road that will be annotated MapLaneSection script is needed for sections that contain more than 1 lane. For sections with only 1 lane, they can be left as a child object of the TrafficLanes object or grouped with the boundary lines under a different parent object without the MapLaneSection script. Each MapLaneSection will contain parallel lanes section 1 MapLane per lane of road as well as 1 MapLine per boundary line (optional) If the annotations will be broken up into multiple MapLane (e.g. a straight section and a curved section), multiple MapLaneSection are required A MapLane cannot begin or end in the middle of another MapLane (e.g. a lane splits or merges). This situation would require a 2nd MapLaneSection to be created where the split/merge begins. The other lanes on the road will also need to be broken up into multiple MapLane to be included in the multiple MapLaneSection MapLaneSection is used to automate the computation of relation of neighboring lanes. Please make sure every lane under MapLaneSection has at least 3 waypoints. Example of single lane splitting into a right-turn only lane and a straight lane","title":"Create Parent Object"},{"location":"map-annotation/#make-lanes","text":"Select the Lane/Line option under Create Mode A large yellow TARGET_WAYPOINT will appear in the center of the scene. This is where the TEMP_WAYPOINT objects will be placed Drag in the appropriate MapLaneSection to be the Parent Object Click Waypoint button to create a new TEMP_WAYPOINT . This is where the lane will begin Move the scene so that the TARGET_WAYPOINT is in the desired location of the next TEMP_WAYPOINT With 2 waypoints, the Create Straight function will connect the waypoints and use the number in Waypoint Count to determine how many waypoints to add in between More than 2 waypoints can be used with the Create Straight function and they will be connected in the order they were created, and the Waypoint Count value will not be used With 3 waypoints, the Create Curve function can be used to create a Bezier curve between the 1st and 3rd waypoints and the Waypoint Count value will be used to deterin how many waypoints to add in between Verify Lane is the selected Map Object Type Verify NO_TURN is the selected Lane Turn Type Select the appropriate lane boundry types Enter the speed limit of the lane (in m/s) Enter the desired number of waypoints in the lane (minimum 2 for a straight lane and 3 for a curved lane) Click the appropriate Connect button to create the lane To adjust the positions of the waypoints, with the MapLane selected, in the Inspector check Display Handles . Individual waypoints can now have their position adjusted","title":"Make Lanes"},{"location":"map-annotation/#make-boundary-lines","text":"Drag in the appropriate MapLaneSection to be the Parent Object The same process as for lanes can be used to create boundary lines, but the Map Object Type will be BoundaryLine It is better to have the direction of a boundary line match the direction of the lane if possible If you are annotating map for Lanelet2 format, you also need to annotate boundary lines for every lane and drag them into the corresponding field in the lane object.","title":"Make Boundary Lines"},{"location":"map-annotation/#annotate-intersections","text":"","title":"Annotate Intersections"},{"location":"map-annotation/#create-parent-object-intersection","text":"In the Map prefab, create a new object and name it \"Intersections\" In the Inspector of the Map , drag the new Intersections object into the Intersections holder In Intersections , create a new object Add the MapIntersection script to the new object and position it in the center of the intersection that will be annotated. Adjust the Trigger Bounds of the MapIntersection so that the box covers the center of the intersection The MapIntersection will contain all lanes, traffic signals, stop lines, and traffic signs in the intersection","title":"Create Parent Object"},{"location":"map-annotation/#create-intersection-lanes","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object The same process for creating normal MapLane is used here If the Lane involves changing directions, verify the correct Lane Turn Type is selected If you are annotating for Lanelet2, you also need to annotate boundary lines for each lane, remember to set VIRTUAL as the Line Type for the line objects If there is a stop line in the intersection, the start of the intersection lanes should be after the stop line For NPCs to properly navigate an intersection, the Yield To Lanes list must be manually filled in. With View Selected toggled in the Map Annotation Tool, the lanes in the Yield To Lanes list will be highlighted in yellow to make it easier to verify that the correct lanes are in the list. Before an NPC enters an intersection lane, it will check that there are no NPCs on the lanes in the Yield To Lanes before continuing. Enter the number of lanes that the current lanes yields to as the size of Yield To Lanes For each element in the list, drag in a lane that takes priority over the selected lane For example, generally when turning left the NPC will yield to the oncoming traffic going straight so the straight lanes should be in the left turn lane's Yield To Lanes list","title":"Create Intersection Lanes"},{"location":"map-annotation/#create-traffic-signals","text":"Select the Signal option under Create Mode Drag in the MapIntersection as the Parent Object Select the correct Signal Type Traffic Signals are created on top of existing meshes. The annotation directions must match the directions of the mesh. Select the mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Click Create Signal to annotate the traffic signal","title":"Create Traffic Signals"},{"location":"map-annotation/#create-traffic-signs","text":"Select the Sign option under Create Mode Drag in the MapIntersection as the Parent Object Select the Sign mesh that is going to be annotated Select the Forward Vector and Up Vector so that it matches the selected mesh Change the Sign Type to the desired type Click Create Sign to create the annotation Find the stop line MapLine that is associated with the created MapSign Select the MapSign and drag the MapLine into the Stop Line box Verify the annotation is created in the correct orientation. The Z-axis (blue) should be facing the same way the sign faces. Move the Bounding Box so that is matches the sign Bound Offsets adjusts the location of the box Bound Scale adjusts the size of the box","title":"Create Traffic Signs"},{"location":"map-annotation/#create-stop-lines","text":"Select the Lane/Line option under Create Mode Drag in the MapIntersection as the Parent Object A similar process to boundary lines and traffic lanes is used for stop lines Change the Map Object Type to StopLine Forward Right vs Forward Left depend on the direction of the lane related to this stopline and the order that the waypoints were created in. The \"Forward\" direction should match the direction of the lane Example: In a right-hand drive map (cars are on the right side of the road), if the waypoints are created from the outside of the road inwards, then Forward Right should be selected. To verify if the correct direction was selected, Toggle Tool Handle Rotation so that the tool handles are in the active object's rotation. The Z-axis of the selected Stop Line should be in the same direction as the lanes. A StopLine needs to with the lanes that approch it. The last waypoint of approaching lanes should be past the line.","title":"Create Stop Lines"},{"location":"map-annotation/#create-pole","text":"Poles are required for Autoware Vector maps. In an intersection with traffic lights, there is 1 MapPole on each corner, next to a stop line. The MapPole holds references to all traffic lights that signal the closest stop line. Select the Pole option under Create Mode Drag in the MapIntersection as the Parent Object A TARGET_WAYPOINT will appear in the center of the scene. This is where the pole annotation will be created. Position the TARGET_WAYPOINT on the corner of the intersection near the stop line Click Create Pole to create the annotation Find the traffic light MapSignal that are associated with the closest stop line Select the MapPole In the Inspector, change the Signal Lights size to be the number of MapSignal that signal the stop line Drag 1 MapSignal into each element of Signal Lights","title":"Create Pole"},{"location":"map-annotation/#annotate-self-reversing-lanes","text":"These types of lanes are only supported on Apollo 5.0","title":"Annotate Self-Reversing Lanes"},{"location":"map-annotation/#annotation-information","text":"There are TrafficLanes and Intersections objects. TrafficLanes object is for straight lane without any branch. Intersections object is for branching lanes, which has same predecessor lane.","title":"Annotation Information"},{"location":"map-annotation/#annotate-self-reversing-lanes-under-traffic-lanes","text":"Add MapLaneSection game object under TrafficLanes. MapLaneSection has one pair of lanes, which are forward and reverse lanes. Annotate lane given name to MapLane_forward and move it under MapLaneSection. Use straight line connect menu with way point count and move each point to shape lane. Use Display Lane option to show width of lane. In order for parking to work, there should have some space between parking space and lane. Duplicate the MapLane_forward and rename it to MapLane_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane_forward, MapLane_reverse) and change properties like the following: Is Self Reverse Lane: True Lane Turn Type: NO_TURN Left Bound Type: SOLID_WHITE Right Bound Type: SOLID_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map) Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Traffic Lanes"},{"location":"map-annotation/#annotate-self-reversing-lanes-under-intersections","text":"Add MapIntersection game object under Intersections. MapIntersection has several pair of lanes. Annotate lane given name to MapLane1_forward move it under MapIntersection. Use curve connect menu with way point count, 5. Duplicate the MapLane1_forward and rename it to MapLane1_reverse. Choose MapLane_reverse and click Reverse Lane. Reverse Lane will have way points in reverse order and make tweak to each way point. Routing module doesn't work right when forward and reverse lane have same way point coordinates. Choose both lanes (MapLane1_forward, MapLane1_reverse) and change properties like the following: Is Self Reverse Lane: True Left Bound Type: DOTTED_WHITE Right Bound Type: DOTTED_WHITE Speed Limit: 4.444444 (This value is indicated in sunnyvale_with_two_offices map). Choose each lane and decide its turn type considering its direction. Lane Turn Type: NO_TURN or LEFT_TURN or RIGHT_TURN Set Self Reverse Lane for each lane As for forward lane, you can drag reverse lane game object to Self Reverse Lane of forward lane. As for reverse lane, you can do the same way.","title":"Annotate Self-Reversing Lanes under Intersections"},{"location":"map-annotation/#annotate-other-features","text":"Other annotation features may be included in the TrafficLane or Intersection objects or they may be sorted into other parent objects.","title":"Annotate Other Features"},{"location":"map-annotation/#create-pedestrian-path","text":"This annotation controls where pedestrians will pass with the highest priority. Pedestrians can walk anywhere but will stay on annotated areas if possible. Select the Pedestrian option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT on the desired path Click Waypoint to create a TEMP_WAYPOINT Repeat to create a trail of TEMP_WAYPOINT along the desired path Click Connect to create the MapPedestrian object","title":"Create Pedestrian Path"},{"location":"map-annotation/#create-junction","text":"Junction annotations can be used by the AD stack if needed. Select the Junction option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one vertex of the junction Click Waypoint to create a TEMP_WAYPOINT Create the desired number of TEMP_WAYPOINTS Click Connect to create the MapJunction","title":"Create Junction"},{"location":"map-annotation/#create-crosswalk","text":"Select the CrossWalk option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the crosswalk Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapCrossWalk","title":"Create Crosswalk"},{"location":"map-annotation/#create-clear-area","text":"Select the ClearArea option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one corner of the clear area Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapClearArea","title":"Create Clear Area"},{"location":"map-annotation/#create-parking-space","text":"Select the ParkingSpace option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to the corner of the parking space closest to an approaching vehicle Click Waypoint to create a TEMP_WAYPOINT Create 4 TEMP_WAYPOINT in the order shown below Click Connect to create the MapParkingSpace NOTE: Apollo 5.0 requires some space between the edge of a lane and the parking space. To verify that there is space, select the MapLane that goes past the MapParkingSpace and click Display Lane . This will show the width of the lane facilitate adjustment of the lane to have a gap between the lane and parking space.","title":"Create Parking Space"},{"location":"map-annotation/#create-speed-bump","text":"Select the SpeedBump option under Create Mode Drag in the desired Parent Object A TARGET_WAYPOINT will appear in the center fo the scene. This is where the TEMP_WAYPOINT will be created. Position the TARGET_WAYPOINT to one end of the speed bump Click Waypoint to create a TEMP_WAYPOINT Create 2 TEMP_WAYPOINT in the order shown below The TEMP_WAYPOINT should be wide enough to be outside the lane Click Connect to create the MapSpeedBump To verify that the MapSpeedBump is wide enough, select the MapLane that the MapSpeedBump crosses. Enable Display Lane to visualize the width of the lane. If necesary, select the MapSpeedBump and enable Display Handles to adjust the positions of the individual waypoints.","title":"Create Speed Bump"},{"location":"map-annotation/#export-map-annotations","text":"HD Map Annotations may be exported in a variety of formats. Current supported formats are: Apollo 5.0 HD Map Apollo 3.0 HD Map Autoware Vector Map Lanelet2 Map OpenDrive Map To export a map: Open the HD Map Export tool in Unity : Simulator -> Export HD Map Select the desired format from the dropdown Export Format Enter the desired save location of the exported map Click Export to create the exported map","title":"Export Map Annotations"},{"location":"map-annotation/#import-map-annotations","text":"The simulator can import a variety of formats of annotated map. Current supported formats are: Lanelet2 Map Apollo 5.0 HD Map OpenDRIVE Map To import a map: Open the HD Map Export tool in Unity : Simulator -> Import HD Map Select the format of the input map from the dropdown Import Format Select the file or folder that will be imported Click Import to import the map annotations into the simulator Lanelet2 map importer Notes: Lanes will be automatically imported and grouped as MapLaneSection if possible Intersections with four-way traffic lights / stop signs or two-way stop signs can be imported and grouped under MapIntersection Left-turn lanes are automatically found and their corresponding lanes to yield are also obtained automatically to get NPCs working correctly For each MapIntersection , you need to manually adjust X and Z for the Trigger Bounds as explained in annotate intersection part . Remember to check objects under Intersections are grouped correctly.","title":"Import Map Annotations"},{"location":"map-annotation/#map-formats","text":"For more information on the map formats, please see the links below: Apollo HD Autoware Vector Lanelet2 OpenDRIVE","title":"Map Formats"},{"location":"maps-tab/","text":"Web UI Maps Tab Explanation A Map can be in the following states. A Map with a local URL or if it has already been downloaded will have a Valid status. If the URL to the Map assetbundle is not local and the assetbundle is not in the local database, then the map needs to be downloaded. Currently only 1 assetbundle is downloaded at a time. If an assetbundle is downloading, the Map will show a GREY dot and the status will be Downloading with the download percentage. If another assetbundle is downloading, the icon will be ORANGE and the status will be Downloading without a percentage. A downloading Map can be interrupted by pressing the stop button. If the Map is not usable in a Simulation it will have an Invalid status. This can be because the local assetbundle is not usable or the download was interrupted. Where to find Maps top # Map AssetBundles and HD maps are available on our content website . When adding a map, the link to the appropriate AssetBundle can be entered as the URL or the AssetBundle can be downloaded manually and the local path can be entered. The HD maps for maps are available in the same page. Please see the relevant doc for instructions on how to add an HD map to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware How to Add a Map top # Click the Add new button. In the dialogue that opens, enter the name of the map and the URL to the AssetBundle. This can be a URL to a location in the cloud (e.g. the link to the assetbundle on the content website ) or to a location on a local drive (the absolute path to the environment_XXX file). If the URL is not local, the AssetBundle will be downloaded to the local database. How to Edit a Map top # Click the pencil icon In the dialogue that opens, the name of the map can be changed and the URL to the AssetBundle. If the URL is changed, the AssetBundle in the database will be updated (downloaded if necessary) How to Annotate a Map top # Please see Map Annotation for more information on how to annotate a map in Unity. This also details how to export or import and HD map. How to Migrate a Map top # If you have created maps in our old simulator (before HDRP), you can reuse those maps in current HDRP simulator by following steps: In old simulator You need to make some changes in header inside HDMapTool.cs (line 148-154): Convert your MapOrigin Northing/Easting to Longitude/Latitude. Replace values for left and right with Longitude. Repalce values for top and bottom with Latitude. You also need to change zone in header. If you have intersections annotated, you need to annotate junctions like here for each intersection in the old simulator since we are importing intersections based on junction in HDRP simulator. Otherwise, signals/signs will not be grouped correctly, every signal/sign will be created as an intersection object. Export map to Apollo Map file: base_map.bin . In current HDRP simulator Select Simulator->Import HD Map , and set Import Format as Apollo 5 HD Map . Select the exported map and import. Note, you need to check intersections, some signals/signs may be grouped incorrectly.","title":"Maps"},{"location":"maps-tab/#where-to-find-maps","text":"Map AssetBundles and HD maps are available on our content website . When adding a map, the link to the appropriate AssetBundle can be entered as the URL or the AssetBundle can be downloaded manually and the local path can be entered. The HD maps for maps are available in the same page. Please see the relevant doc for instructions on how to add an HD map to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware","title":"Where to find Maps"},{"location":"maps-tab/#how-to-add-a-map","text":"Click the Add new button. In the dialogue that opens, enter the name of the map and the URL to the AssetBundle. This can be a URL to a location in the cloud (e.g. the link to the assetbundle on the content website ) or to a location on a local drive (the absolute path to the environment_XXX file). If the URL is not local, the AssetBundle will be downloaded to the local database.","title":"How to Add a Map"},{"location":"maps-tab/#how-to-edit-a-map","text":"Click the pencil icon In the dialogue that opens, the name of the map can be changed and the URL to the AssetBundle. If the URL is changed, the AssetBundle in the database will be updated (downloaded if necessary)","title":"How to Edit a Map"},{"location":"maps-tab/#how-to-annotate-a-map","text":"Please see Map Annotation for more information on how to annotate a map in Unity. This also details how to export or import and HD map.","title":"How to Annotate a Map"},{"location":"maps-tab/#how-to-annotate-a-map","text":"If you have created maps in our old simulator (before HDRP), you can reuse those maps in current HDRP simulator by following steps: In old simulator You need to make some changes in header inside HDMapTool.cs (line 148-154): Convert your MapOrigin Northing/Easting to Longitude/Latitude. Replace values for left and right with Longitude. Repalce values for top and bottom with Latitude. You also need to change zone in header. If you have intersections annotated, you need to annotate junctions like here for each intersection in the old simulator since we are importing intersections based on junction in HDRP simulator. Otherwise, signals/signs will not be grouped correctly, every signal/sign will be created as an intersection object. Export map to Apollo Map file: base_map.bin . In current HDRP simulator Select Simulator->Import HD Map , and set Import Format as Apollo 5 HD Map . Select the exported map and import. Note, you need to check intersections, some signals/signs may be grouped incorrectly.","title":"How to Annotate a Map"},{"location":"npc-map-navigation/","text":"NPC Map Navigation In the latest Simulator Release, NPC vehicles follow the annotated HD map. No extra steps are required. The following tutorial works with Simulator Release 2019.05 Non-player character (NPC) vehicles now use the MapSegmentBuilder classes to navigate annotated maps. Map Manager Use SanFrancisco.scene as a template to build map data for NPCs. Remove SFTraffic_New.prefab from scene and any associated scripts, e.g., TrafPerformanceManager.cs, TrafInfoManager.cs and TrafSystem.cs. Create a new GameObject at position Vector3.zero and Quaternion.identity, named Map . Add MapManager.cs component to this object and save as a prefab in project assets. The public field, SpawnLanesHolder, of MapManager.cs requires the MapLaneSegmentBuilder holder transform. The public field, IntersectionsHolder, of MapManager.cs requires the TrafficLights holder transform. This has intersection meshes and scripts for lights. The public fields Green, Yellow, and Red are materials for the segmentation camera system. If using the Traffic meshes from SanFrancisco.scene, these need to be added here. If not, IntersectionComponent.cs and associated scripts will need to be edited. Map Lane and Intersection Grouping Create TrafficLanes holder object as a child of Map.prefab . Place all MapLaneSegmentBuilder objects into TrafficLanes holder object for all non intersection lanes. Create IntersectionLanes holder object as a child of Map.prefab . Create a new Intersection holder object as a child of IntersectionLanes transform for each intersection annotation. Be sure its world position is in the center of each intersection. Place MapLaneSegmentBuilder and MapStopLineSegmentBuilder objects into Intersection holder object for each intersection. Map Intersection Builder For each Intersection holder, add the MapIntersectionBuilder.cs component. Traffic Lights Create a TrafficLights holder object to hold all traffic light meshes or place all traffic meshes under the map annotation Intersections . Just be sure to have the root holder be in MapManager.cs IntersectionHolder public reference. Create a Intersection holder object. Be sure its world position is in the center of each intersection. Add IntersectionComponent.cs to each Intersection holder object. Place TrafficLightPole facing it's corresponding StopLineSegmentBuilder object. The transfom needs to be Z axis or gizmo arrow forward, parallel to the StopLineSegmentBuilder object Z axis or gizmo arrow forward. Add IntersectionTrafficLightSetComponent.cs. Place as a child of the Intersection holder object. For opposite facing TrafficLightPoles and StopLineSegmentBuilders , be sure to orient transforms in Z axis or gizmo arrow forward but perpendicular to other facing light poles and stoplines. Add TrafficLight meshes as children of the TrafficLightPole . Add IntersectionTrafficLightSetComponent.cs to each TrafficLight . StopLine and MapLaneSegmentBuilder overlap MapLaneSegmentBuilders final waypoint needs to be slightly overlapping the MapStopLineBuilder","title":"NPC map navigation"},{"location":"npc-plugins/","text":"NPC Plugins With NPC (non player character, that is non ego traffic) plugins we added the ability to customize the appearance and behavior of NPC vehicles in your simulations to allow you to add local variations of vehicle styles and brands or implement encounters with vehicles displaying distinguished behaviors such as erratic driving or vehicles stopping frequently like deliveries or trash pick up. Table of Contents NPC Models Building NPC Plugins NPC Behaviors Creating NPC Behavior Plugins Creating NPC Model Plugins NPC Meta Data Copyright and License NPC Models # NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type exemplary spawning weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarily, colors for each spawned vehicle is picked from a table for each vehicle type. In offline mode, NPC models are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically. Building NPC Plugins top # NPCs are grouped into collections to allow you to import regional collections of vehicles. This means that you can place e.g. NorthAmericaNPCs at beneath the NPC source folder at <simulator source root>/Assets/External/NPCs to have all NorthAmericaNPCs vehicles show up in the build menu in a group. To build AssetBundles of NPCs, Open Simulator -> Build... menu item Select behaviors in e.g. \"NPCs/NorthAmericaNPCs\" section of build window Build plugins with \"Build\" button The bundle named npc_XXX will be placed in the AssetBundles/NPCs/<COLLECTION> folder. If building the binary, this folder is included in the target destination. See build instructions for more details. NPC Behaviors # NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided builtin NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder. Open-source example: DrunkDriver Creating NPC Behavior Plugins top # Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/ExampleNPCs/DrunkDriver . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/ExampleNPCs/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6) Creating NPC Model Plugins top # Create a folder in under your collection folder for each Model, eg. Assets/External/NPCs/ExampleNPCs/DumpTruck . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name contains... Component Type Description Head Light headlights which are controlled by the NPCController to off, regular, high beam etc states Brake Light braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeft Light left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRight Light right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light light turns on when NPC is reversing Body Renderer Renderer used to generate MeshCollider for collision and raycast checks Body Material on any Renderer this material will have its _BaseColor changed to give each NPC a random color from a list LightHead Material on any Renderer a material which is controlled to emit light in conjunction with headlight Lights LightBrake Material on any Renderer a material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material on any Renderer a material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material on any Renderer a material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material on any Renderer a material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is converted to Wheel. NPCs with less than four wheels are assumed to be bikes and will try to stay upright by applying a force each physics update \"Wheel\" and \"Front\" Renderer wheels that are supposed to turn when steering NPC Meta Data # Additional informaton about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab. Currently, only NPC size type is specified here. Copyright and License top # Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Traffic behavior plugins"},{"location":"npc-plugins/#npc-models","text":"NPC Models allow customization of NPC visuals, most notably the 3D model of vehicles, but also the material used for the car paint or windows. With supporting custom NPC models we also added the support for motorbikes and vehicles with more two axles as well as more than one steering axle as is found on heavy vehicles. We classify NPCs into size categories which decide the spawning frequency as well as which paint colors are more common for which NPC size type. The following table lists all currently implemented size types and the weight that correlates to their relative probability of encountering this type. NPC Size Type exemplary spawning weight Compact 5 MidSize 6 Luxury 2 Sport 1 LightTruck 2 SUV 4 MiniVan 3 Large 2 Emergency 1 Bus 1 Trailer 0 Motorcycle 1 In this example, trailer type has a weight of zero, indicating that it should not spawn by itself, while MidSize type has the highest weight, indicating that it is the most frequent vehicle on the road. The NPC Type is configured via a Component added to the root of the prefab . Similarily, colors for each spawned vehicle is picked from a table for each vehicle type. In offline mode, NPC models are detected from <simulator installation path>/AssetBundles/NPCs and added to the simulation automatically.","title":"NPC Models"},{"location":"npc-plugins/#building-npc-plugins","text":"NPCs are grouped into collections to allow you to import regional collections of vehicles. This means that you can place e.g. NorthAmericaNPCs at beneath the NPC source folder at <simulator source root>/Assets/External/NPCs to have all NorthAmericaNPCs vehicles show up in the build menu in a group. To build AssetBundles of NPCs, Open Simulator -> Build... menu item Select behaviors in e.g. \"NPCs/NorthAmericaNPCs\" section of build window Build plugins with \"Build\" button The bundle named npc_XXX will be placed in the AssetBundles/NPCs/<COLLECTION> folder. If building the binary, this folder is included in the target destination. See build instructions for more details.","title":"Building NPC Plugins"},{"location":"npc-plugins/#npc-behaviors","text":"NPC Behaviors allow NPC Vehicles to behave in custom ways different from the provided builtin NPC behavior and are enabled and configured through the Python API. Before running the simulator (running the executable or pressing Play in the Editor) NPC behavior plugins must be built by the simulator using Simulator -> Build... menu and the built asset placed in the AssetBundles/NPCs folder. Open-source example: DrunkDriver","title":"NPC Behaviors"},{"location":"npc-plugins/#creating-npc-behavior-plugins","text":"Create a folder in under your collection folder for each behavior, eg. Assets/External/NPCs/ExampleNPCs/DrunkDriver . If your plugin is a pure behavior, that is it has no visual components, we require one C# file to have the same name as the folder under the collection folder, for example note how the name DrunkDriver repeats in both the folder name and file name: Assets/External/NPCs/ExampleNPCs/DrunkDriver/DrunkDriver.cs . The main class has to inherit from NPCBehaviourBase and implement its abstract methods to hook into the NPC simulation framework. It is also possible to inherit from NPCLaneFollowingBehaviour if you want to implement a slight modification of the default lane following behavior, such as this simple DrunkDriver example: using System.Collections; using System.Collections.Generic; using UnityEngine; using Simulator.Api; using Simulator.Map; using Simulator.Utilities; using SimpleJSON; // In this example, we try to simulate a driver under ther influence of alcohol. // We modify the default NPCLaneFollowingBehaviour because in principle this behavior // shuld just add variation the the default behavior. public class NPCDrunkDriverBehaviour : NPCLaneFollowBehaviour { public float steerCorrectionMinTime = 0.0f; public float steerCorrectionMaxTime = 0.4f; public float steerDriftMin = 0.00f; public float steerDriftMax = 0.09f; protected float currentSteerDrift = 0.0f; protected float nextSteerCorrection = 0; // This function in the base class controls the NPC steering protected override void SetTargetTurn() { // we reduce the frequency at which steering is updated to the target heading to // simulate loss of attention or reduced reaction time if(nextSteerCorrection < Time.fixedTime) { float steerCorrectionIn = RandomGenerator.NextFloat(steerCorrectionMinTime, steerCorrectionMaxTime); nextSteerCorrection = Time.fixedTime + steerCorrectionIn; // we add drift to the steering to simulate loss of fine motor skills currentSteerDrift = RandomGenerator.NextFloat(steerDriftMin, steerDriftMax); currentSteerDrift = currentSteerDrift * Mathf.Abs(RandomGenerator.NextFloat(-1.0f, 1.0f)); // we can reuse the base steering at reduced frequency base.SetTargetTurn(); } else { // steering drift correlating to driving speed currentTurn += currentSteerDrift * currentSpeed; } } } If you want to configure aspects of your NPC behavior from the Python API, you can add a class implementing the ICommand interface as shown in this example: class DrunkDriverControl : ICommand { public string Name => \"agent/drunk/config\"; public void Execute(JSONNode args) { var uid = args[\"uid\"].Value; var api = ApiManager.Instance; if (!api.Agents.TryGetValue(uid, out GameObject npc)) { api.SendError(this, $\"Agent '{uid}' not found\"); return; } var behaviour = npc.GetComponent<NPCDrunkDriverBehaviour>(); if (behaviour == null) { api.SendError(this, $\"Agent '{uid}' is not a drunk driving NPC agent\"); return; } if(args.HasKey(\"correctionMinTime\")) behaviour.steerCorrectionMinTime = args[\"correctionMinTime\"].AsFloat; if(args.HasKey(\"correctionMaxTime\")) behaviour.steerCorrectionMaxTime = args[\"correctionMaxTime\"].AsFloat; if(args.HasKey(\"steerDriftMin\")) behaviour.steerDriftMin = args[\"steerDriftMin\"].AsFloat; if(args.HasKey(\"steerDriftMax\")) behaviour.steerDriftMax = args[\"steerDriftMax\"].AsFloat; api.SendResult(this); } } example usage from Python: # common setup code omitted, check PythonAPI/quickstart for examples # you can check if it has been loaded: behaviours = sim.available_npc_behaviours for i in range(len(behaviours)): if behaviours[i][\"name\"]==\"NPCDrunkDriverBehaviour\": drunkDriverAvailable = True #later... npc = sim.add_agent(agent, lgsvl.AgentType.NPC, state) if drunkDriverAvailable: inp = input(\"make drunk driver? yN\") if (inp == \"y\" or inp == \"Y\"): npc.set_behaviour(\"NPCDrunkDriverBehaviour\") # usage of example command from C# plugin. npc.remote.command(\"agent/drunk/config\", { \"uid\": npc.uid, \"correctionMinTime\":0.0, \"correctionMaxTime\":0.6, \"steerDriftMin\": 0.00, \"steerDriftMax\":0.09}) # can still use lane following methods as those are inherited npc.follow_closest_lane(True, 5.6)","title":"Creating NPC Behavior Plugins"},{"location":"npc-plugins/#creating-npc-model-plugins","text":"Create a folder in under your collection folder for each Model, eg. Assets/External/NPCs/ExampleNPCs/DumpTruck . All assets of a NPC such as textures, additional materials or scripts are expected to live inside its folder. When an NPC is spawned, a NPCController script is attached to the root of the prefab instance, and it will try to find several components to animate the vehicle. To identify the components, we look at the Name property of the Components as follows: GameObject Name contains... Component Type Description Head Light headlights which are controlled by the NPCController to off, regular, high beam etc states Brake Light braking lights, controlled by the NPCController to reflect the vehicle braking IndicatorLeft Light left indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorRight Light right indicator lights controlled by the NPCController when turning or when hazard lights are on IndicatorReverse Light light turns on when NPC is reversing Body Renderer Renderer used to generate MeshCollider for collision and raycast checks Body Material on any Renderer this material will have its _BaseColor changed to give each NPC a random color from a list LightHead Material on any Renderer a material which is controlled to emit light in conjunction with headlight Lights LightBrake Material on any Renderer a material which is controlled to emit light in conjunction with brake Lights IndicatorLeft Material on any Renderer a material which is controlled to emit light in conjunction with left indicator Lights IndicatorRight Material on any Renderer a material which is controlled to emit light in conjunction with right indicator Lights IndicatorReverse Material on any Renderer a material which is controlled to emit light in conjunction with reversing Light Wheel Renderer Renderer is converted to Wheel. NPCs with less than four wheels are assumed to be bikes and will try to stay upright by applying a force each physics update \"Wheel\" and \"Front\" Renderer wheels that are supposed to turn when steering","title":"Creating NPC Model Plugins"},{"location":"npc-plugins/#npc-meta-data","text":"Additional informaton about the NPC prefab is added by attaching a Unity MonoBehaviour Component called NPCMetaData added to each prefab. Currently, only NPC size type is specified here.","title":"NPC Meta Data"},{"location":"npc-plugins/#copyright-and-license","text":"Copyright (c) 2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"openai-gym/","text":"Reinforcement Learning with OpenAI Gym OpenAI Gym is a toolkit for developing reinforcement learning algorithms. Gym provides a collection of test problems called environments which can be used to train an agent using a reinforcement learning. Each environment defines the reinforcement learnign problem the agent will try to solve. To facilitate developing reinforcement learning algorithms with the LGSVL Simulator , we have developed gym-lgsvl , a custom environment that using the openai gym interface. gym-lgsvl can be used with general reinforcement learning algorithms implementations that are compatible with openai gym. Developers can modify the environment to define the specific reinforcement learning problem they are trying to solve. Requirements # Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv Setup # Install LGSVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e . Getting Started # The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5 Customizing the environment # The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py . CONFIG # Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation. Reward calculation # The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode. Sensors # By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called. NPC Behavior # The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around. Copyright and License # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Reinforcement learning with OpenAI Gym"},{"location":"openai-gym/#requirements","text":"Python >= 3.5 Pip lgsvl simulator API openai gym numpy opencv","title":"Requirements"},{"location":"openai-gym/#setup","text":"Install LGSVL Simulator using this guide : Install Python API using this guide . Install openai gym, numpy, and opencv: pip install --user gym, numpy, opencv-python Clone the gym-lgsvl repository: git clone https://github.com/lgsvl/gym-lgsvl.git Install gym-lgsvl using pip: cd gym-lgsvl/ pip install --user -e .","title":"Setup"},{"location":"openai-gym/#getting-started","text":"The simulator must be running on the menu scene to be used with the gym-lgsvl . The scene can be loaded either in the Unity Editor or the simulator binary build (download latest release ). The binary build will have superior performance. The script named random_agent.py will launch a random agent which will sample acceleration and steering values for the ego vehicle for every step of the episode. Run the agent to test your setup: ./random_agent.py After a few seconds the BorregasAve scene should load and spawn the ego vehicle along with some NPC vehicles. The ego vehicle will drive randomly. gym_lgsvl can be used with RL libraries that support openai gym environments. Below is an example of training using the A2C implementation from baselines : python -m baselines.run --alg=a2c --env=gym_lgsvl:lgsvl-v0 --num_timesteps=1e5","title":"Getting Started"},{"location":"openai-gym/#customizing-the-environment","text":"The specifics of the environment you will need will depend on the reinforcement learning problem you are trying to solve. By default, the gym-lgsvl environment has a simple setup intended to be a starting point for building more advanced problems. Training an agent with the default environment would be difficult without modiification. In the default configuration, the vehicle uses a single front facing camera as observation and uses continuous control parameters for driving the vehicle. For more advanced state representations, modifications will be needed. The entire environment is defined in lgsvl_env.py .","title":"Customizing the environment"},{"location":"openai-gym/#config","text":"Some of the basic configuration are passed to the environment through CONFIG . action_space and observation_space definitions are required and are defined using gym.spaces . The default action_space is: \"action_space\" : spaces.Box( np.array([-1,-1]), np.array([+1,+1,]), dtype=np.float32, ), which defines a continuous action space defined as a 2-D array. The first element is the steering value and the second is braking/throttle (negative values are braking and positive are throttle). The observation space is defined as a single camera image from the front camera using the Box space from gym: \"observation_space\" : spaces.Box( low=0, high=255, shape=(297, 528, 3), dtype=np.uint8 ) # RGB image from front camera The shape tuple specifies the image size. The simulator API will always send 1920x1080 images. If any other size is used to define the observation space, the camera images will be resized to the specified size before being passed on as an observation.","title":"CONFIG"},{"location":"openai-gym/#reward-calculation","text":"The environment also calculates a reward function based on the distance travelled by the ego vehicle and collisions. The reward is calculated in the _calculate_reward() function based on distance travelled. The collision penalty is added on at the end (if applicable) when the collision callback is invoked ( _on_collision ). The collision callback will also terminate the episode and start a new episode.","title":"Reward calculation"},{"location":"openai-gym/#sensors","text":"By default the ego vehicle will only use the front facing camera. The ego vehicle is setup using _setup_ego() . Sensors are also defined here. To define more sensors, grab the sensor from the sensors list and invoke its specific methods to save data. To collect observations, _get_observations() is called.","title":"Sensors"},{"location":"openai-gym/#npc-behavior","text":"The NPCs are defined in _setup_npc . The NPCs are spawned randomly around the ego vehicle spawn point and will follow the lanes and traffic rules to move around.","title":"NPC Behavior"},{"location":"openai-gym/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"perception-ground-truth/","text":"Ground Truth Obstacles This guide is for Simulator Release 2019.05 . To visualize ground truth sensor data in version 2019.07, please activate the ground truth sensors according to the example sensor configuration file. Overview # You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles. View ground truth obstacles in Simulator # Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view. Bounding box colors # Green: Vehicles Yellow: Pedestrians Purple: Unknown Subscribe to ground truth ROS messages # LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge. Install the lgsvl_msgs ROS package # Use LGSVL Apollo or Autoware repository # If you are using LGSVL's forks of Apollo or Autoware , the package is already included as a submodule in the respective workspace: LGSVL Autoware: autoware -> ros -> src -> msgs -> lgsvl_msgs LGSVL Apollo: apollo -> ros_pkgs -> src -> lgsvl_msgs Following the instructions to build the ROS workspace will build the lgsvl_msgs package as well. If you are not running LGSVL's Apollo or Autoware forks, you can directly clone our lgsvl_msgs package into your ROS workspace and build. Manually install lgsvl_msgs # Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make Subscribe to ground truth messages from Simulator # You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link View estimated detections in Simulator # If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"Viewing and subscribing to ground truth data"},{"location":"perception-ground-truth/#overview","text":"You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles.","title":"Overview"},{"location":"perception-ground-truth/#view-ground-truth-obstacles-in-simulator","text":"Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view.","title":"View ground truth obstacles in Simulator"},{"location":"perception-ground-truth/#bounding-box-colors","text":"Green: Vehicles Yellow: Pedestrians Purple: Unknown","title":"Bounding box colors"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-ros-messages","text":"LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge.","title":"Subscribe to ground truth ROS messages"},{"location":"perception-ground-truth/#install-the-lgsvl_msgs-ros-package","text":"","title":"Install the lgsvl_msgs ROS package"},{"location":"perception-ground-truth/#use-lgsvl-apollo-or-autoware-repository","text":"If you are using LGSVL's forks of Apollo or Autoware , the package is already included as a submodule in the respective workspace: LGSVL Autoware: autoware -> ros -> src -> msgs -> lgsvl_msgs LGSVL Apollo: apollo -> ros_pkgs -> src -> lgsvl_msgs Following the instructions to build the ROS workspace will build the lgsvl_msgs package as well. If you are not running LGSVL's Apollo or Autoware forks, you can directly clone our lgsvl_msgs package into your ROS workspace and build.","title":"Use LGSVL Apollo or Autoware repository"},{"location":"perception-ground-truth/#manually-install-lgsvl_msgs","text":"Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make","title":"Manually install lgsvl_msgs"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-messages-from-simulator","text":"You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link","title":"Subscribe to ground truth messages from Simulator"},{"location":"perception-ground-truth/#view-estimated-detections-in-simulator","text":"If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"View estimated detections in Simulator"},{"location":"pointcloud-import/","text":"Point Cloud Import LGSVL Simulator uses its own format to store all of the point cloud data for rendering. To make any point cloud usable within the simulator, it has to be processed first. Built-in point cloud importer tool provides all the functionality required to convert most popular point cloud file formats (PCD, PLY, LAS, LAZ) into data usable for simulation. It is also able to create approximated mesh collider based on imported data. If you're using pre-built point cloud environments, you don't have to use this tool. Its main purpose is to create new environments compatible with Simulator from raw point cloud data. Accessing the importer # To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened. Import process overview # Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page. Importer settings # Importer window is split into multiple sections, each with settings related to different subject. Input Files # This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories. Output Files # This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ . Tree Settings # This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded. Mesh Settings # This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher. Build Settings # This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Point cloud import"},{"location":"pointcloud-import/#accessing-the-importer","text":"To access point cloud importer window, open Simulator project in Unity editor, then navigate into Simulator/Import Point Cloud on the menu bar. Point cloud importer window will be opened.","title":"Accessing the importer"},{"location":"pointcloud-import/#import-process-overview","text":"Importer window requires you to at least specity input and output files, but also gives you access to additional settings. All of these are described in more detail in their respective sections. To start the import process, just click \"Import\" button at the bottom of the window. Depending on the point cloud size and chosen settings, this process might take a while. You'll see multiple progress bars during this time for preprocessing steps and the main import process itself. If you cancel the import process, partially processed data is not usable. Processing points can be performed on multiple threads, but first pass is always single-threaded. Progress bar reporting that only one thread is used during this time is normal. After the first pass, import process speeds up severely depending on amount of threads available. Importer will validate some of your import settings before the import process. There is a chance that \"Import\" button will be disabled. Error or warning message is always displayed in this case, with an information about the reason. Most convenient way of using the importer is with target environment scene open in the background - in this case the tool will offer to setup open scene with all the components required for rendering after the import process succeeds. You can also decide to skip this step and set the components yourself. Details about this are available on the point cloud rendering page.","title":"Import process overview"},{"location":"pointcloud-import/#importer-settings","text":"Importer window is split into multiple sections, each with settings related to different subject.","title":"Importer settings"},{"location":"pointcloud-import/#input-files","text":"This tab is used to select point cloud files that should be converted into combatible format. Multiple files to import can be added at once. All of the files listed in this tab will be merged into single point cloud after importing. If you prefer to keep them separated, import each one of them individually (rendering multiple point clouds is supported, but decreases performance). All of the input files must be of one of following formats: PCD , PLY , LAS , LAZ . Mixing multiple formats is supported. Add Folder button will add all of the compatible files from selected folder into the list. This does not include subdirectories.","title":"Input Files"},{"location":"pointcloud-import/#output-files","text":"This tab expects you to specify a directory that will be used to store all of the generated files. This directory does not have to be inside of the project's folder and can be located in any accessible place on the drive. If you decide to use directory inside the project, path relative to Assets folder fill be used instead of absolute path to help with cross-machine migration. Using directory with processed point cloud data already present will overwrite all of previous data. Warning about this is displayed in such case. Please note that output directory will contain a lot of files after import process. If this directory is located under Assets folder, Unity will generate metadata for each of the files. This process takes a long time and the metadata is completely unused, so it's recommended to skip meta file generation altogether. This can be done by using Unity's special folder names - just make sure that folder name ends with ~ .","title":"Output Files"},{"location":"pointcloud-import/#tree-settings","text":"This tab describes data structure and layout of the imported point cloud. Default settings are working well with most of the standard data sets, so changing them is usually not required. Parameter Name Description Tree Type Tree data structure used to subdivide point cloud. Octree and Quadtree variants are available. Sampling Sampling used on each level of the tree structure, determining distribution of points in the final cloud. Cell Center uses voxel-based, uniform sampling. Poisson Disk offers more natural-looking distribution, but is slower and does not support mesh generation. Root Node Subdivision Determines amount of per-axis samples for each node. Higher values will create larger nodes, while lower values will create deeper tree structures. Node Branch Threshold Minimum amount of points in the node required for splitting it further. Prevents creating mostly empty nodes and overly deep tree structure. Max Tree Depth Maximum amount of levels in the tree structure. Levels deeper than the specified value will be discarded. Min Point Distance Minimum distance between points in a node. Points below this threshold will be discarded.","title":"Tree Settings"},{"location":"pointcloud-import/#mesh-settings","text":"This tab contains options related to collider mesh generation. Note that mesh generation will only work for Cell Center sampling. Parameter Name Description Generate Mesh If this option is enabled, mesh collider will be generated and additional settings will be available. Road Only Mesh If this option is enabled, mesh generation will attempt to only create mesh for the road, ignoring other structures. This option is not suitable for all data sets - if you experience problems, try again with this feature disabled. Mesh Detail Level Level of the tree that should be used for mesh generation. This should corelate with represented area size - for single intersection level 2 is usually enough, while larger areas with multiple roads might require level 5 or higher.","title":"Mesh Settings"},{"location":"pointcloud-import/#build-settings","text":"This tab contains options related to import process and initial point cloud transformations. Some options under this tab will affect amount of system memory required during the import process. You will see the warning if you're exceeding your system's limits - lower one of related settings values in this case. Parameter Name Description Thread Count Amount of threads that the import process should be split across. It's not guaranteed that all of the threads will be utilized fully, but higher thread count reduces import time. Affects memory requirements. Chunk Size Maximum amount of points loaded into memory at once, per thread. This only affects import process and has no effect on output data. Affects memory requirements. Center If this option is enabled, local space origin of the output point cloud will be moved to its bounds center. Normalize If this option is enabled, all points positions will be rescaled to fit normalized coordinates. LAS RGB 8bit Workaround If this option is enabled, additional 8 bit offset is used when reading color and intensity from LAS files. Use this if your data imported from LAS files seems to have corrupted color and/or intensity values. Axes Coordinate system axes convention used in original files. Setting this to correct value ensures that output point cloud will be compatible with Unity's coordinate system.","title":"Build Settings"},{"location":"pointcloud-rendering/","text":"Point Cloud Rendering LGSVL Simulator supports scenes containing point cloud data. Point clouds can be rendered alongside other geometry like meshes or particles, are combatible with most of the features available in high definition render pipeline and can be detected by multiple sensors, including lidar. Point cloud rendering in Simulator can handle large point clouds (hundreds of millions of points) thanks to octree-based structure and selective, frustum-based culling. If you want to create your own point cloud based environment, you should start with importing your point cloud data into simulator. This step is not necessary for pre-built point cloud environments - they do not require any additional setup. Rendering Features # Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and lidar Setup # The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section. Components Overview # In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present. PointCloudManager # This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data. NodeTreeLoader # This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process. NodeTreeRenderer # This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer . Rendering Modes Overview # Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it. Settings # Loader Settings # Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled. Renderer Settings # Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them. Shared Rendering Settings # Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), Lidar (makes point cloud detectable by lidar sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size . Point/Cones Rendering Settings # Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance. Solid Rendering Mode # Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode. Culling Settings # Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if lidar is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Point cloud rendering"},{"location":"pointcloud-rendering/#rendering-features","text":"Most of the features available for point cloud renderers are optional and can be toggled based on individual use cases. Lighting, for example, can be turned off if point cloud has light data already baked in, or calculated fully based on scene settings like time of day or weather. Most notable features include: Full deferred lighting compatible with HDRP's lighting system Filling holes between points Rendering multiple point clouds at once Normals estimation Shadow casting/receiving Point streaming Point budget (for memory and render buffer) Camera frustum or distance-based culling Mesh collider generation Support for color/depth camera sensors and lidar","title":"Rendering Features"},{"location":"pointcloud-rendering/#setup","text":"The easiest way to set up point cloud rendering is to perform an import with the target scene open in the background. When importing is done, you will be prompted with a choice to automatically add all of the required components to the open scene. If accepted, point cloud with default rendering settings will immediately show up in scene view. Alternatively, a prefab located in Assets/Prefabs/PointCloudRenderer.prefab can be added to any scene for a quick setup. This requires to specify point cloud data path in NodeTreeLoader component. More complex setups with multiple point clouds or masked rendering require setting up components manually. Details can be found in Components Overview section.","title":"Setup"},{"location":"pointcloud-rendering/#components-overview","text":"In the most common scenario with a single point cloud, one of each of the components described below will be present on a scene. For more complex cases (e.g. multiple point clouds or separate color and intensity data) it's possible to add multiple instances of NodeTreeLoader and/or NodeTreeRenderer to one scene. Only one instance of PointCloudManager should ever be present.","title":"Components Overview"},{"location":"pointcloud-rendering/#pointcloudmanager","text":"This component is responsible for keeping track of shared point cloud resources and orchestrating rendering for one or more point cloud renderers. It will also automatically set up HDRP passes and enable or disable them depending on all renderers' settings. There are no settings available for this component. Only one instance should be active at a time. Disabling this component will disable rendering of all point cloud data.","title":"PointCloudManager"},{"location":"pointcloud-rendering/#nodetreeloader","text":"This component is responsible for managing single point cloud's data in memory. It will load chunks of point cloud data from mass storage based on each renderer's requests (performed on a separate thread) and will keep track of total amount of points loaded into memory. When defined point budget is exceeded, older points will be unloaded. Single instance of this component can only operate on one point cloud, but multiple renderers can use data stored in a single NodeTreeLoader . This component can also optionally load mesh colliders for a point cloud if they were generated during import process.","title":"NodeTreeLoader"},{"location":"pointcloud-rendering/#nodetreerenderer","text":"This component represents a single point cloud to render and stores all related settings. Transform to which this component is tied affects translation, rotation and scale of the point cloud. By default, point cloud rendered by a single renderer will be visible in all cameras and compatible sensors, but not in shadow caster pass. This can be controlled on a per-renderer basis. Note that this component will not actively trigger any rendering. It's used mostly as a container for data that will be used by PointCloudManager . NodeTreeRenderer is default implementation that includes camera-based culling, which will make load requests to NodeTreeLoader . Adding other culling methods can be done by extending the base class, PointCloudRenderer .","title":"NodeTreeRenderer"},{"location":"pointcloud-rendering/#rendering-modes-overview","text":"Depending on the requirements, there are multiple combinations of rendering modes available. Sensors supported in point cloud rendering are compatible with all of them. The most simple one ( Points ) will render points with size dependent on distance from the camera. This image is stable, but does not support lighting and will show holes if point cloud is not dense enough. Improved version of this mode ( Cones ) will use 3D cones instead of 2D circles, which creates better transitions between very close points, but has a slight performance cost. More advanced rendering mode, Solid , will remove points that should be invisible from observer's position and interpolate color and depth data between leftover points. It can also reconstruct normals data from depth, and subsequently calculate lighting for point cloud. Filters used here are executed in screen space, which means that camera position and rotation affect rendering output. This might introduce some artifacts around edges, especially around large depth differences. It's suggested to test available modes and choose the one best suited for given point cloud and use case. If lighting is required, solid rendering mode is the only one currently supporting it.","title":"Rendering Modes Overview"},{"location":"pointcloud-rendering/#settings","text":"","title":"Settings"},{"location":"pointcloud-rendering/#loader-settings","text":"Parameter Name Type Description Data Path string Directory in which imported point cloud data is stored. Paths starting with .../ will be treated as relative to Assets folder in the project, others are treated as absolute. Point Limit int Maximum amount of points that can be loaded into memory at once. These points are treated as cache between render buffer and hard drive. When point limit is exceeded, older points will be unloaded. Load Meshes bool If this option is enabled, collider meshes will loaded alongside the point cloud data. This requires Generate Mesh option in point cloud import settings to be enabled.","title":"Loader Settings"},{"location":"pointcloud-rendering/#renderer-settings","text":"Settings visible for renderer depend on currently selected rendering mode. Irrelevant settings will be hidden in inspector. Enabling additional features will sometimes show new settings related to them.","title":"Renderer Settings"},{"location":"pointcloud-rendering/#shared-rendering-settings","text":"Parameter Name Type Description Colorize enum Colorization mode to use for point cloud. If selected data is not present in source point cloud, result will be black. Available modes: Colors , Intensity , Rainbow Intensity and Rainbow Height . Render Mode enum Describes how points should be represented. Available modes: Points (most simple one), Cones (similar to points, with interpolation) and Solid (fills holes, required for lighting). Mask enum Enum flags used to enable or disable renderer's visibility. Point cloud will be visible only in selected passes. Available options: Camera (all color and depth cameras), Shadowcaster (enables shadow casting by point cloud), Lidar (makes point cloud detectable by lidar sensor). Shadow Point Size float Only visible if Shadowcaster is enabled under Mask . Determines size of a point for shadow casting purposes. Shadow Bias float Only visible if Shadowcaster is enabled under Mask . Determines shadow bias for each point. Value is relative to Shadow Point Size .","title":"Shared Rendering Settings"},{"location":"pointcloud-rendering/#pointcones-rendering-settings","text":"Parameter Name Type Description Constant Size bool If this option is enabled, point size will not depend on distance from camera. Pixel Size float Only visible with Constant Size enabled. Determines size of a point. Absolute Size float Only visible with Constant Size disabled. Determines size of a point at a near plane. Points further away will be smaller. Min Pixel Size float Only visible with Constant Size disabled. Determines minimum size of a point, irrelevant from distance.","title":"Point/Cones Rendering Settings"},{"location":"pointcloud-rendering/#solid-rendering-mode","text":"Cascades options for Remove Hidden and Smooth Normals stages affect filter window size for both these effects and its scaling with distance for camera. They can be visualized by toggling Show Preview option on. Parameter Name Type Description Lighting Mode enum Determines how light will affect the point cloud. Available modes: Unlit (no lighting, no shadows), Shadow Receiver (no lighting, receives shadows), Full Deferred (receives lighting and shadows) Calculate Normals bool If enabled, normals approximation stage will be performed and world-space normals will be available. Required for lighting. Smooth Normals bool If enabled, additional normals smoothing stage will be performed to reduce noise. Results will replace normals data from Calculate Normals option. FoV Reprojection bool If enabled, wider image of point cloud will be rendered internally and it will be reprojected to desired camera FOV. This option reduces artifacts near the edges of the image. Does not affect final FOV. Reprojection Ratio float Multiplier determining ratio between FOV used internally for FoV Reprojection option and original FOV of the camera. Solid rendering also has some debug options under separate tab. These are mostly directly affecting separate rendering stages, but some of them can me modified to affect final image. Parameter Name Type Description Linear depth bool If enabled, linear depth will be used throughout all compute shaders instead of depth buffer values. Force fill bool If enabled, holes below horizon line will be filled regardless of their size. Can create blurred artifacts for large holes, but removes all holes in road for low density point clouds. Fill threshold float Sets height threshold under which Force fill option will be used if it's enabled. Blend sky bool If enabled, point cloud color and intensity will be blended to sky color instead of black. Removes black color around edges, but requires additional sky pre-render for lit mode.","title":"Solid Rendering Mode"},{"location":"pointcloud-rendering/#culling-settings","text":"Parameter Name Type Description Node Tree Loader NodeTreeLoader Reference to loader that will be queried for point cache updates. Cull Camera Camera Reference to camera used for culling. If this field is left empty, main simulator camera will be used after simulation is started. Cull Mode enum Determines how points are evaluated for visibility. Available modes: Camera Frustum (both view frustum and distance from camera are considered), Distance (only distance from camera is considered - recommended if lidar is used). Point Limit int Number of points that should be pushed to render buffer. This value determines maximum amount of points that are visible at once and directly affects buffer size. Min Projection float Minimum screen projection size along single axis (in pixels) that octree node has to occupy to be traversed further. Nodes below this threshold will not be visible. Rebuild Steps int Number of frames to spend for render buffer rebuild. Previous fully completed buffer will be used until new one is ready. Values higher than 1 reduce per-frame cost of buffer rebuild, but can introduce popping.","title":"Culling Settings"},{"location":"python-api/","text":"Python API Guide Overview # LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command Line Parameters for more information. Table of Contents Overview Requirements Quickstart Core Concepts Simulation Non-realtime Simulation Agents EGO Vehicle NPC Vehicles Pedestrians Callbacks Agent Callbacks 'EgoVehicle NpcVehicle Callbacks Pedestrian Callbacks Sensors Camera Sensor Lidar Sensor IMU Sensor GPS Sensor Radar Sensor CAN bus Weather and Time of Day Control Controllable Objects Helper Functions Changelog Copyright and License Requirements top # Using Python API requires Python version 3.5 or later. Quickstart top # Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor). Simulator by default listens for connections on port 8181 on localhost. Click the Open Browser button to open the Simulator UI. After the default maps and vehicles have been downloaded, navigate to the Simulations tab. Create a new Simulation. Give it a name and check the API Only option. Click Submit . Select the newly created Simulation and click the \"Play\" button in the bottom right. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move. Core concepts top # The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values. Simulation top # To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load(scene = \"BorregasAve\", seed = 650387) Scene is a string representing the name of the Map in the Web UI. Currently available scenes: BorregasAve - small suburban map AutonomouStuff - small office park Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check the Web UI Maps tab for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution: Non-realtime Simulation top # The simulator can be run at faster-than-realtime speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time. Agents top # You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"Lincoln2017MKZ (Apollo 5.0)\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ (Apollo 5.0) vehicle from the Web UI Vehicles tab. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: Jaguar2015XE (Apollo 3.0) - Apollo 3.0 vehicle Jaguar2015XE (Apollo 5.0) - Apollo 5.0 vehicle Jaguar2015XE (Autoware) - Autoware vehicle Lexus2016RXHybrid (Autoware) - Autoware vehicle Lincoln2017MKZ (Apollo 5.0) - Apollo 5.0 vehicle Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information. EGO vehicle top # EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True) NPC vehicles top # You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC) Pedestrians top # You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN) Callbacks top # The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below. Agent Callbacks top # collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point. EgoVehicle Callbacks top # In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information. NpcVehicle Callbacks top # In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance Pedestrian Callbacks top # In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer. Sensors top # EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge Camera Sensor top # The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files. Lidar Sensor top # Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255). IMU Sensor top # You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent. GPS Sensor top # You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees Radar Sensor top # Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor. CAN bus top # Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor. Weather and Time of Day Control top # You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). Controllable Objects top # A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state Helper Functions top # Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values. Changelog top # 2020-01-30 * Extended controllable objects to support plugins - see controllable plugins 2019-09-05 * Extended DriveWaypoint to support angle, idle time and trigger distance * Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release Copyright and License top # Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Python API guide"},{"location":"python-api/#overview","text":"LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retrieve sensor configuration and data, control weather, time state, and more. The interface to listen for incoming API calls is defined in config.yml . See Configuration File and Command Line Parameters for more information.","title":"Overview"},{"location":"python-api/#requirements","text":"Using Python API requires Python version 3.5 or later.","title":"Requirements"},{"location":"python-api/#quickstart","text":"Python API is available in separate repository: https://github.com/lgsvl/PythonAPI After cloning or downloading it from the git repository follow these steps: Run the following command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor). Simulator by default listens for connections on port 8181 on localhost. Click the Open Browser button to open the Simulator UI. After the default maps and vehicles have been downloaded, navigate to the Simulations tab. Create a new Simulation. Give it a name and check the API Only option. Click Submit . Select the newly created Simulation and click the \"Play\" button in the bottom right. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the BorregasAve.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move.","title":"Quickstart"},{"location":"python-api/#core-concepts","text":"The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values.","title":"Core Concepts"},{"location":"python-api/#simulation","text":"To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator(address = \"localhost\", port = 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load(scene = \"BorregasAve\", seed = 650387) Scene is a string representing the name of the Map in the Web UI. Currently available scenes: BorregasAve - small suburban map AutonomouStuff - small office park Seed (optional) is an Integer (-2,147,483,648 - 2,147,483,647) that determines the \"random\" behavior of the NPC vehicles and rain effects. Check the Web UI Maps tab for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. To run the simulator in realtime, call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution:","title":"Simulation"},{"location":"python-api/#non-realtime-simulation","text":"The simulator can be run at faster-than-realtime speeds depending on the performance of the computer running the simulator. This is done by calling the run method with the time_scale argument: sim.run(time_limit = 6, time_scale = 2) run takes a 2nd optional argument specifying how much faster to run. In the above example, if the computer is fast enough the run call will finish in 3 seconds (6 divided by 2), but 6 virtual seconds of data would be generated. If only time_scale is specified or time_limit = 0, then simulation will run continuously at non-realtime speed. The value of time_scale can be lower than 1 which gives ability to run simulation in slower than real time.","title":"Non-realtime Simulation"},{"location":"python-api/#agents","text":"You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent(name = \"Lincoln2017MKZ (Apollo 5.0)\", \\ agent_type = lgsvl.AgentType.EGO, state = None) This will create the Lincoln2017MKZ (Apollo 5.0) vehicle from the Web UI Vehicles tab. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: Jaguar2015XE (Apollo 3.0) - Apollo 3.0 vehicle Jaguar2015XE (Apollo 5.0) - Apollo 5.0 vehicle Jaguar2015XE (Autoware) - Autoware vehicle Lexus2016RXHybrid (Autoware) - Autoware vehicle Lincoln2017MKZ (Apollo 5.0) - Apollo 5.0 vehicle Available NPC vehicles: Sedan SUV Jeep Hatchback SchoolBus BoxTruck Available pedestrian types: Bob EntrepreneurFemale Howard Johny Pamela Presley Red Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information.","title":"Agents"},{"location":"python-api/#ego-vehicle","text":"EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True)","title":"EGO Vehicle"},{"location":"python-api/#npc-vehicles","text":"You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10, lgsvl.Vector(0, 0, 0), 0, False, 0), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5, lgsvl.Vector(0, 0, 0), 0, False, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, a desired velocity in m/s, a desired angular orientation as a vector of Euler angles, an optional wait-time for the vehicle to stay idle, an bool on whether the NPC should be active while idling, and an optional trigger distance. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. The angular orientation of the NPC will be interpolated in such a manner that it will pass through the waypoint at the angle specified in the DriveWaypoint . The trigger distance, if used, provides a method to pause the NPC until an ego vehicle approaches. The NPC will begin to drive as soon as its distance to an ego vehicle drops below the value specified as trigger distance in the DriveWaypoint . You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent(\"Sedan\", lgsvl.AgentType.NPC) def on_waypoint(agent, index): print(\"waypoint {} reached\".format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. You can also spawn a pool of NPC vehicles with the same behavior as NPCs added to a non-API simulation. They will follow the map annotations, obey speed limits, obey traffic signals, and attempt to avoid accidents. These NPCs cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.NPC)","title":"NPC Vehicles"},{"location":"python-api/#pedestrians","text":"You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent(\"Bob\", lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5, 0), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10, 0), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5, 0), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates, an optional idle time that the pedestrian will spend standing in-place when it reaches the waypoint, and an optional trigger distance. You can receive information on progress by setting the on_waypoint_reached callback. You can also spawn a pool of pedestrians with the same behavior as pedestrians added to a non-API simulation. They will follow the map annotations and path find. These pedestrians cannot be directly controlled. sim.add_random_agents(lgsvl.AgentType.PEDESTRIAN)","title":"Pedestrians"},{"location":"python-api/#callbacks","text":"The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below.","title":"Callbacks"},{"location":"python-api/#agent-callbacks","text":"collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = \"STATIC OBSTACLE\" if agent1 is None else agent1.name name2 = \"STATIC OBSTACLE\" if agent2 is None else agent2.name print(\"{} collided with {} at {}\".format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point.","title":"Agent Callbacks"},{"location":"python-api/#egovehicle-callbacks","text":"In addition to Agent callbacks, EgoVehicle has one extra callback. on_custom - called when a Sensor Plugin sends a callback; accepts three arguments: (agent, kind, context) - agent instance, kind of sensor plugin as string, JSON context See Sensor Plugins for more information.","title":"'EgoVehicle"},{"location":"python-api/#npcvehicle-callbacks","text":"In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance","title":"NpcVehicle Callbacks"},{"location":"python-api/#pedestrian-callbacks","text":"In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer.","title":"Pedestrian Callbacks"},{"location":"python-api/#sensors","text":"EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus sensor Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge","title":"Sensors"},{"location":"python-api/#camera-sensor","text":"The Camera sensor has the following read only properties: frequency - rate at which images are captured & sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Main Camera\": sensor.save(\"main-camera.png\", compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files.","title":"Camera Sensor"},{"location":"python-api/#lidar-sensor","text":"Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent(\"Lincoln2017MKZ (Apollo 5.0)\", lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = \"Lidar\": sensor.save(\"lidar.pcd\") A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255).","title":"Lidar Sensor"},{"location":"python-api/#imu-sensor","text":"You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent.","title":"IMU Sensor"},{"location":"python-api/#gps-sensor","text":"You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print(\"Latitude:\", data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees","title":"GPS Sensor"},{"location":"python-api/#radar-sensor","text":"Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"Radar Sensor"},{"location":"python-api/#can-bus","text":"Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"CAN bus"},{"location":"python-api/#weather-and-time-of-day-control","text":"You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print(\"Current time of day:\", sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ).","title":"Weather and Time of Day Control"},{"location":"python-api/#controllable-objects","text":"A controllable object is an object that you can control by performing an action using Python APIs. Each controllable object has its own valid actions (e.g., green, yellow, red, trigger, wait, loop) that it can take and is controlled based on control policy , which defines rules for control actions. For example, a traffic light is a controllable object, and you can change its behavior by updating control policy: \"trigger=50;green=1;yellow=1.5;red=2;loop\" trigger=50 - Wait until an ego vehicle approaches this controllable object within 50 meters green=1 - Change current state to green and wait for 1 second yellow=1.5 - Change current state to yellow and wait for 1.5 second red=2 - Change current state to red and wait for 2 second loop - Loop over this control policy from the beginning Available controllable object types: signal cone All Controllable objects can be added or removed dynamically. When reset() is called, all Controllables are removed and the ones in the map (if any) are added back. Controllable objects can be loaded plugins at runtime. Plugin must include IControllable and be built using the Simulator build process from Assets/External/Controllables folder. To get a list of controllable objects in a scene: controllables = sim.get_controllables() For a controllable object of interest, you can get following information: signal = controllables[0] print(\"Type:\", signal.type) print(\"Transform:\", signal.transform) print(\"Current state:\", signal.current_state) print(\"Valid actions:\", signal.valid_actions) For control policy, each controllable object always has default control policy (read-only). When you load a scene for the first time or reset a scene to the initial state, a controllable object resets current control policy to default one follows it. You can get default control policy and current control policy as follows: print(\"Default control policy:\", signal.default_control_policy) print(\"Current control policy:\", signal.control_policy) To change a current control policy, you can create a new control policy and call control function as below: control_policy = \"trigger=50;green=1;yellow=1.5;red=2;loop\" signal.control(control_policy) To add a plugin controllable and set object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0,0,0) state.transform.rotation = lgsvl.Vector(0,0,0) state.velocity = lgsvl.Vector(0,10,0) state.angular_velocity = lgsvl.Vector(6.5,0,0) cone = sim.controllable_add(\"TrafficCone\", state) To get plugin controllable object state cone.object_state To set plugin controllable object state state = lgsvl.ObjectState() state.transform.position = lgsvl.Vector(0, 0, -10) cone.object_state = state","title":"Controllable Objects"},{"location":"python-api/#helper-functions","text":"Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position & rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print(\"Distance right:\", hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values.","title":"Helper Functions"},{"location":"python-api/#changelog","text":"2020-01-30 * Extended controllable objects to support plugins - see controllable plugins 2019-09-05 * Extended DriveWaypoint to support angle, idle time and trigger distance * Added controllable objects - use Simulator.get_controllables 2019-08-12 Added time_scale argument to run function for running simulation in non-realtime Added seed argument to Simulator.load for deterministic NPCs 2019-04-19 initial release","title":"Changelog"},{"location":"python-api/#copyright-and-license","text":"Copyright (c) 2019-2020 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"sensor-json-options/","text":"Sensor JSON Options This page details the different available sensors and the configuration options possible. Table of Contents Examples How to Specify a Sensor Color Camera Depth Camera Segmentation Camera Lidar 3D Ground Truth 3D Ground Truth Visualizer CAN-Bus GPS Device GPS Odometry GPS-INS Status Vehicle Control Keyboard Control Wheel Control Manual Control Cruise Control IMU 2D Ground Truth 2D Ground Truth Visualizer Radar Clock Control Calibration Transform Sensor Signal Sensor Examples top # Example JSON configurations are available here: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON Data Collection JSON Some Lidar sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128 How to Specify a Sensor top # A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis Color Camera top # This is the type of sensor that would be used for the Main Camera in Apollo. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** Cubemap Size should only be 512, 1024, or 2048. { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Depth Camera top # This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on Cubemap Size for Color Camera. { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Segmentation Camera top # This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on Cubemap Size for Color Camera. { \"type\": \"Segmentation Camera\", \"name\": \"Segmentation Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\" ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Lidar top # This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 3D Ground Truth top # This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 3D Ground Truth Visualizer top # This sensor will visualize bounding boxes on objects as detected by the AD Stack. It does not publish any data and instead subscribes to a topic from the AD Stack. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } CAN-Bus top # This sensor sends data about the vehicle chassis. The data includes: - Speed [m/s] - Throttle [%] - Braking [%] - Steering [+/- %] - Parking Brake Status [bool] - High Beam Status [bool] - Low Beam Status [bool] - Hazard Light Status [bool] - Fog Light Status [bool] - Left Turn Signal Status [bool] - Right Turn Signal Status [bool] - Wiper Status [bool] - Reverse Gear Status [bool] - Selected Gear [Int] - Engine Status [bool] - Engine RPM [RPM] - GPS Latitude [Latitude] - GPS Longitude [Longitude] - Altitude [m] - Orientation [3D Vector of Euler angles] - Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Device top # This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS Odometry top # This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } GPS-INS Status top # This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Vehicle Control top # This sensor is required for a vehicle to subscribe to the control topic of an AD Stack. { \"type\": \"Vehicle Control\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } Keyboard Control top # This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" } Wheel Control top # This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"Wheel Control\", \"name\": \"Wheel Car Control\" } Manual Control top # This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. {Deprecated} Will be removed next release. Use Keyboard Control { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" } Cruise Control top # This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"Cruise Control\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } } IMU top # This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 2D Ground Truth top # This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } 2D Ground Truth Visualizer top # This sensor will visualize bounding boxes on objects as detected by the AD Stack, it does not publish any data. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta In order for bounding boxes to align properly, parameters should match the same camera that the AD Stack is using for detection (i.e. if running Apollo, the parameters should match the sensor named \"Main Camera\"). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Radar top # This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Clock top # This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message. Only parameter to use is topic name. Users can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . { \"type\": \"Clock\", \"name\": \"ROS Clock\", \"params\": { \"Topic\": \"/clock\" } } Control Calibration top # This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } } Total Control Calibration Criteria: Transform Sensor top # This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"type\": \"Transform\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } Signal Sensor top # This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"Signal\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Sensor parameters"},{"location":"sensor-json-options/#examples","text":"Example JSON configurations are available here: Apollo 3.0 JSON Apollo 5.0 JSON Autoware JSON Data Collection JSON Some Lidar sensor JSONs: Velodyne VLP-16 Velodyne VLP-32C Velodyne VLS-128","title":"Examples"},{"location":"sensor-json-options/#how-to-specify-a-sensor","text":"A vehicle configuration is in the following format: [ SENSOR, SENSOR, SENSOR ] A SENSOR is defined in the JSON configuration in the following format: { \"type\": STRING, \"name\": STRING, \"params\": {PARAMS}, \"parent\": STRING, \"transform\": { \"x\": FLOAT, \"y\": FLOAT, \"z\": FLOAT, \"pitch\": FLOAT, \"yaw\": FLOAT, \"roll\": FLOAT, } } type is the type of sensor. name is the name of the sensor. This is how the sensor will be identified. params are the explicitly specified parameters. If a parameter is not set, the Default Value in the sensor definition will be used. ex. {\"Width\": 1920, \"Height\": 1080} There are 2 parameters that all sensors have Parameter Description Default Value Topic defines the topic that the sensor will subscribe/publish to null Frame defines the frame_id if the sensor publishes a ROS message. See ROS Header Message for more information null parent (OPTIONAL) a sensor's transform can be relative to another sensor. STRING is the name of the base sensor transform to which this sensor is relative. If omitted, the transform is relative to the origin of the vehicle. transform is the location and rotation of the sensor relative to the local position of the vehicle. The Unity left-hand coordinate system is used (+x right, +y up, +z forward, +pitch tilts the front down, +yaw rotates clockwise when viewed from above, +roll tilts the left side down). x is the position of the sensor along the x-axis y is the position of the sensor along the y-axis z is the position of the sensor along the z-axis pitch is the rotation around the x-axis yaw is the rotation around the y-axis roll is the rotation around the z-axis","title":"How to Specify a Sensor"},{"location":"sensor-json-options/#color-camera","text":"This is the type of sensor that would be used for the Main Camera in Apollo. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 JpegQuality defines the quality if the image output % Int 75 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion * List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * If Distorted is true , DistortionParameters must be an empty list or a list of FOUR floats. The values in this list should come from calibration result of real camera. Setting arbitrary values may cause undefined result. If Distorted is false , DistortionParameters , Fisheye and Xi are ignored. ** If Fisheye is true , Xi should be a value from calibration result of real camera. Setting arbitrary value may cause undefined result. If Fisheye is false , Xi is ignored. *** Cubemap Size should only be 512, 1024, or 2048. { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Color Camera"},{"location":"sensor-json-options/#depth-camera","text":"This sensor returns an image where the shades on the grey-scale correspond to the depth of objects. Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 5 1 100 JpegQuality defines the quality if the image output % Int 100 0 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on Cubemap Size for Color Camera. { \"type\": \"Depth Camera\", \"name\": \"Depth Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/depth_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Depth Camera"},{"location":"sensor-json-options/#segmentation-camera","text":"This sensor returns an image where objects are colored corresponding to their tag: Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F If a tag is included in \"InstanceSegmentationTags\", each instance of objects with that tag is colored differently, but all will have same hue (e.g. all cars will be bluish and all pedestrians will be reddish). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image output pixels Int 1920 1 1920 Height defines the height of the image output pixels Int 1080 1 1080 Frequency defines the maximum rate that messages will be published Hertz Int 15 1 100 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 1000 0.01 2000 Distorted defines if the image is distorted Bool false DistortionParameters parameters used by distortion* List of Float empty list Fisheye defines if the camera has fisheye lens Bool false Xi parameter used by fisheye distortion ** Float 0.0 Cubemap Size size of the cubemap used by fisheye distortion *** pixels Int 1024 512 2048 InstanceSegmentationTags define tags with instance segmentation List of String empty list * See notes on DistortionParameters for Color Camera. ** See notes on Xi for Color Camera. *** See notes on Cubemap Size for Color Camera. { \"type\": \"Segmentation Camera\", \"name\": \"Segmentation Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"InstanceSegmentationTags\": [ \"Car\", \"Pedestrian\" ], \"Topic\": \"/simulator/segmentation_camera\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Segmentation Camera"},{"location":"sensor-json-options/#lidar","text":"This sensor returns a point cloud after 1 revolution. Parameter Description Unit Type Default Value Minimum Maximum VerticalRayAngles defines vertical angle for each laser beam* List of Float empty list LaserCount defines how many vertically stacked laser beams there are Int 32 1 128 FieldOfView defines the vertical angle between bottom and top ray degrees Float 41.33 1 45 CenterAngle defines the center of the FieldOfView cone to the horizon (+ means below horizon) degrees Float 10 -45 45 MinDistance defines how far an object must be from the sensor for it to be detected meters Float 0.5 0.01 1000 MaxDistance defines how close an object must be to the sensor for it to be detected meters Float 100 0.01 2000 RotationFrequency defines how fast the sensor rotates Hertz Float 10 1 30 MeasurementsPerRotation defines how many measurements each beam takes per rotation Int 1500 18 6000 Compensated defines whether or not the point cloud is compensated for the movement of the vehicle Bool true PointSize defines how large of points are visualized pixels Float 2 1 10 PointColor defines the color of visualized points rgba in hex Color #FF0000FF * If VerticalRayAngles is not empty, LaserCount will be automatically set to the length of VerticalRayAngles , and FieldOfView and CenterAngle will be ignored. A sample of uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } A sample of non-uniformly distributed angles: { \"type\": \"Lidar\", \"name\": \"Lidar-NonUniform\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Lidar"},{"location":"sensor-json-options/#3d-ground-truth","text":"This sensor returns 3D ground truth data for training and creates bounding boxes around the detected objects. The color of the object corresponds to the object's type: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines the how close an object must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"3D Ground Truth\", \"name\": \"3D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/3d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"3D Ground Truth"},{"location":"sensor-json-options/#3d-ground-truth-visualizer","text":"This sensor will visualize bounding boxes on objects as detected by the AD Stack. It does not publish any data and instead subscribes to a topic from the AD Stack. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta { \"type\": \"3D Ground Truth Visualizer\", \"name\": \"3D Ground Truth Visualizer\", \"params\": { \"Topic\": \"/simulator/ground_truth/3d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.975314, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"3D Ground Truth Visualizer"},{"location":"sensor-json-options/#can-bus","text":"This sensor sends data about the vehicle chassis. The data includes: - Speed [m/s] - Throttle [%] - Braking [%] - Steering [+/- %] - Parking Brake Status [bool] - High Beam Status [bool] - Low Beam Status [bool] - Hazard Light Status [bool] - Fog Light Status [bool] - Left Turn Signal Status [bool] - Right Turn Signal Status [bool] - Wiper Status [bool] - Reverse Gear Status [bool] - Selected Gear [Int] - Engine Status [bool] - Engine RPM [RPM] - GPS Latitude [Latitude] - GPS Longitude [Longitude] - Altitude [m] - Orientation [3D Vector of Euler angles] - Velocity [3D Vector of m/s] Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 { \"type\": \"CAN-Bus\", \"name\": \"CAN Bus\", \"params\": { \"Frequency\": 10, \"Topic\": \"/canbus\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"CAN-Bus"},{"location":"sensor-json-options/#gps-device","text":"This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Device"},{"location":"sensor-json-options/#gps-odometry","text":"This sensor outputs the GPS location of the vehicle in Longitude/Latitude and Northing/Easting coordintates and the vehicle velocity. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 12.5 1 100 ChildFrame used by Autoware IgnoreMapOrigin defines whether or not the actual GPS position is returned. If true , then the Unity world position is returned (as if the MapOrigin were (0,0)) Bool false { \"type\": \"GPS Odometry\", \"name\": \"GPS Odometry\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_odometry\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS Odometry"},{"location":"sensor-json-options/#gps-ins-status","text":"This sensor outputs the status of the GPS correction due to INS. The Simulator is an ideal environment in which GPS is always corrected. Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published [Hertz] Float 12.5 1 100 { \"type\": \"GPS-INS Status\", \"name\": \"GPS INS Status\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/gps_ins_stat\", \"Frame\": \"gps\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": -1.348649, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"GPS-INS Status"},{"location":"sensor-json-options/#vehicle-control","text":"This sensor is required for a vehicle to subscribe to the control topic of an AD Stack. { \"type\": \"Vehicle Control\", \"name\": \"AD Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } }","title":"Vehicle Control"},{"location":"sensor-json-options/#keyboard-control","text":"This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. { \"type\": \"Keyboard Control\", \"name\": \"Keyboard Car Control\" }","title":"Keyboard Control"},{"location":"sensor-json-options/#wheel-control","text":"This sensor is required for a vehicle to accept Logitech G920 wheel control commands. Parameters are not required. { \"type\": \"Wheel Control\", \"name\": \"Wheel Car Control\" }","title":"Wheel Control"},{"location":"sensor-json-options/#manual-control","text":"This sensor is required for a vehicle to accept keyboard control commands. Parameters are not required. {Deprecated} Will be removed next release. Use Keyboard Control { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }","title":"Manual Control"},{"location":"sensor-json-options/#cruise-control","text":"This sensor causes the vehicle to accelerate to the desired speed and then maintain the desired speed. Parameter Description Unit Type Default Value Minimum Maximum CruiseSpeed defines the desired speed meters/second Float 0 0 200 { \"type\": \"Cruise Control\", \"name\": \"AD Car Control\", \"params\": { \"CruiseSpeed\": 10 } }","title":"Cruise Control"},{"location":"sensor-json-options/#imu","text":"This sensor output at a fixed rate of 100 Hz. IMU publishes data on topics where the 2nd topic has corrected IMU data. Parameter Description CorectedTopic defines the 2nd topic that the data is published to CorrectedFrame defines the 2nd frame for the ROS header { \"type\": \"IMU\", \"name\": \"IMU\", \"params\": { \"Topic\": \"/imu\", \"Frame\": \"imu\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"IMU"},{"location":"sensor-json-options/#2d-ground-truth","text":"This sensor outputs an image where objects are encased in a box. The color of the box depends on the type of object. Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 DetectionRange defines how close an object must be to be given a bounding box meters Float 100 0.01 2000 { \"type\": \"2D Ground Truth\", \"name\": \"2D Ground Truth\", \"params\": { \"Frequency\": 10, \"Topic\": \"/simulator/ground_truth/2d_detections\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"2D Ground Truth"},{"location":"sensor-json-options/#2d-ground-truth-visualizer","text":"This sensor will visualize bounding boxes on objects as detected by the AD Stack, it does not publish any data. The color of the boxes are: Object Color Car Green Pedestrian Yellow Bicycle Cyan Unknown Magenta In order for bounding boxes to align properly, parameters should match the same camera that the AD Stack is using for detection (i.e. if running Apollo, the parameters should match the sensor named \"Main Camera\"). Parameter Description Unit Type Default Value Minimum Maximum Width defines the width of the image pixels Int 1920 1 1920 Height defines the height of the iamge pixels Int 1080 1 1080 FieldOfView defines the vertical angle that the camera sees degrees Float 60 1 90 MinDistance defines how far an object must be from the sensor to be in the image meters Float 0.1 0.01 1000 MaxDistance defines how close an object must be to the sensor to be in the image meters Float 1000 0.01 2000 { \"type\": \"2D Ground Truth Visualizer\", \"name\": \"2D Ground Truth Visualizer\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/ground_truth/2d_visualize\" }, \"transform\": { \"x\": 0, \"y\": 1.7, \"z\": -0.2, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"2D Ground Truth Visualizer"},{"location":"sensor-json-options/#radar","text":"This sensor outputs the objects detected by the radar. Detected objects are visualized with a box colored by their type: Type Color Car Green Agent Magenta Bicycle Cyan Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 13.4 1 100 { \"type\": \"Radar\", \"name\": \"Radar\", \"params\": { \"Frequency\": 13.4, \"Topic\": \"/radar\" }, \"transform\": { \"x\": 0, \"y\": 0.689, \"z\": 2.272, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Radar"},{"location":"sensor-json-options/#clock","text":"This sensor outputs simulated time to ROS as rosgraph_msgs/Clock message. Only parameter to use is topic name. Users can add <param name=\"/use_sim_time\" value=\"true\"> to their ROS launch file, or use rosparam set /use_sim_time true in command line, to have a ROS node use simulation time according to the /clock topic. For more details please refer to this page . { \"type\": \"Clock\", \"name\": \"ROS Clock\", \"params\": { \"Topic\": \"/clock\" } }","title":"Clock"},{"location":"sensor-json-options/#control-calibration","text":"This sensor outputs control calibration criteria collected by AD Stacks (Apollo, Autoware). It generates steering, throttle or brakes with gear commands between minimum and maximum of velocity during duration. Parameter Description Unit Type Minimum Maximum min_velocity defines the minimum velocity when criterion is executed meters/second Float 0 50.0 max_velocity defines the maximum velocity when criterion is executed meters/second Float 0 50.0 throttle defines the throttle which makes acceleration Percent Float 0 100.0 brakes defines the brakes which make deceleration Percent Float 0 100.0 steering defines ego vehicle's steering Percent Float -100.0 100.0 gear defines ego vehicle's direction (forward or reverse) String duration defines criterion's execution time second Float 0 { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 } ] } } Total Control Calibration Criteria:","title":"Control Calibration"},{"location":"sensor-json-options/#transform-sensor","text":"This sensor is specifically used to parent other sensors. For example, if there is a cluster of sensors a Transform sensor can be added at the location of the cluster and then the individual sensors can have a transform that is relative to the location of the Transform sensor`. Example usage { \"type\": \"Transform\", \"name\": \"Cluster Reference\", \"transform\": { \"x\": 0.75, \"y\": 1.7, \"z\": 1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Color Camera\", \"name\": \"Main Camera\", \"params\": { \"Width\": 1920, \"Height\": 1080, \"Frequency\": 15, \"JpegQuality\": 75, \"FieldOfView\": 50, \"MinDistance\": 0.1, \"MaxDistance\": 1000, \"Topic\": \"/simulator/main_camera\", \"Frame\": \"camera\", \"Distorted\": true, \"DistortionParameters\": [ -0.25349, 0.11868, 0, 0 ] }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0.1, \"y\": 0, \"z\": -0.1, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar-Uniform\", \"params\": { \"LaserCount\": 32, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"parent\": \"Cluster Reference\", \"transform\": { \"x\": 0, \"y\": 0.2, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Transform Sensor"},{"location":"sensor-json-options/#signal-sensor","text":"This sensor returns ground truth data for traffic light signals connected to the current lane of ego vehicle and creates bounding boxes around the detected signals. The color of the bounding box corresponds to the signal's type: Bounding Box Signal Green Green Yellow Yellow Red Red Parameter Description Unit Type Default Value Minimum Maximum Frequency defines the maximum rate that messages will be published Hertz Float 10 1 100 MaxDistance defines how close a traffic light must be to the sensor to be detected meters Float 100 1 1000 { \"type\": \"Signal\", \"name\": \"Signal Sensor\", \"params\": { \"Frequency\": 10, \"MaxDistance\": 100, \"Topic\": \"/simulator/ground_truth/signals\" }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }","title":"Signal Sensor"},{"location":"sensor-plugins/","text":"Sensor Plugins Sensor plugins are custom sensors that can be added to a vehicle configuration. Sensor plugins must be built by the simulator and the resultant bundle named sensor_XXX must be placed in the AssetBundles/Sensors folder. If running the binary, this folder is included in the downloaded .zip. If running in Editor, the sensor will be built into the folder directly. This must be done before running the simulator (running the executable or pressing Play in the Editor). The sensor can be added to a vehicle configuration just like other sensors, see here Building sensor plugins to bundle is done as below 1. Open Simulator -> Build... menu item 2. Select sensor plugins in \"Sensor\" section of build window 3. Build plugins with \"Build\" button To make sensor plugin, create folder in Assets/External/Sensors , for example Assets/External/Sensors/CustomCameraSensor . Inside this folder you must place sensor prefab with same name ( CustomCameraSensor.prefab ) that will be used by simulator to instantiate at runtime. This prefab must have the sensor script added to the root of the prefab. To create a prefab: Right-click in the scene hierarchy and select Create Empty Change the name to the name of the sensor (e.g. CustomCameraSensor ) In the Inspector for this object, select Add Component Search for the sensor script Drag this object from the scene hierarchy into the project folder Additionally you can place C# scripts which will be compiled & bundled with prefab, as well as any additional Unity resources (shaders, materials, textures, etc...). Sensor plugins must have SensorType attribute which specifies the kind of sensor being implemented as well as the type of data that the sensor sends over the bridge. In addition, it must have SensorBase as the base class and must implement the OnBridgeSetup , OnVisualize , and OnVisualizeToggle methods. Sensors can optionally include CheckVisible method to prevent NPC or Pedestrians from spawning in bounds of the sensor. See the below codeblock from the ColorCamera sensor: namespace Simulator.Sensors { // The SensorType's name will match the `type` when defining a sensor in the JSON configuration of a vehicle // The requiredType list is required if data will be sent over the bridge. It can otherwise be empty. // Publishable data types are: // CanBusData, CLockData, Detected2DObjectData, Detected3DObjectData, DetectedRadarObjectData, // GpsData, ImageData, ImuData, PointCloudData, SignalData, VehicleControlData [SensorType(\"Custom Color Camera\", new[] { typeof(ImageData)})] // Inherits Monobehavior // SensorBase also defines the parameters Name, Topic, and Frame public partial class CustomCameraSensor : SensorBase { private Camera Camera; IBridge Bridge; IWriter<ImageData> Writer; // These public variables can be set in the JSON configuration [SensorParameter] [Range(1, 128)] public int JpegQuality = 75; //Sets up the bridge to send this sensor's data public override void OnBridgeSetup(IBridge bridge) { Bridge = bridge; Writer = bridge.AddWriter<ImageData>(Topic); } // Defines how the sensor data will be visualized in the simulator public override void OnVisualize(Visualizer visualizer) { Debug.Assert(visualizer != null); visualizer.UpdateRenderTexture(Camera.activeTexture, Camera.aspect); } // Called when user toggles visibility of sensor visualization // This function needs to be implemented, but otherwise can be empty public override void OnVisualizeToggle(bool state) { } // Called when NPC and Pedestrian managers need to check if visible by sensor // camera or bounds before placing object in scene public override void CheckVisible(Bounds bounds) { var activeCameraPlanes = GeometryUtility.CalculateFrustumPlanes(Camera); return GeometryUtility.TestPlanesAABB(activeCameraPlanes, bounds); } } } SensorBase in inherited from Unity's Monobehavior so any of the Messages can be used to control how and when the sensor collects data. Open-source examples are available: Comfort Sensor","title":"Introduction"},{"location":"sensor-visualizers/","text":"Sensor Visualizers When in a non-Headless Simulation, sensor visualizers can be toggled from the menu. To visualize a sensor, click the \"eye\" next to the sensor name. Sensors are identified by the name parameter from the JSON configuration. For full details on the possible JSON parameters see Sensor Parameters Not all sensors have visualizations available, only sensors who have will show their visualizations. Table of Contents Cameras Color Camera Depth Camera Semantic Camera 2D Ground Truth Lidar Radar 3D Ground Truth Cameras top # When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" again. Color Camera top # Visualized Color camera shows the same things that are visible from the normal follow and free cameras, but from the perpsective defined in the JSON configuration. Depth Camera top # Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object. Semantic Camera top # Visualized Semantic camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F 2D Ground Truth top # Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box. Lidar top # Visualized Lidar shows the point cloud that is detected. Radar top # Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs ni a green box, bicycles in a cyan box, and other EGOs in a magenta box. 3D Ground Truth top # Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"Sensor visualization"},{"location":"sensor-visualizers/#cameras","text":"When a camera is visualized, the image the sensor see is visualized in a window. This window can be resized by clicking-and-dragging the icon in the bottom right corner and can be made full-screen with the box icon in the top right corner. The window can be moved by clicking-and-dragging the top bar. To close the window, either click the X or click the \"eye\" again.","title":"Cameras"},{"location":"sensor-visualizers/#color-camera","text":"Visualized Color camera shows the same things that are visible from the normal follow and free cameras, but from the perpsective defined in the JSON configuration.","title":"Color Camera"},{"location":"sensor-visualizers/#depth-camera","text":"Visualized Depth camera shows objects colored on a grayscale based on the distance between the camera and the object.","title":"Depth Camera"},{"location":"sensor-visualizers/#semantic-camera","text":"Visualized Semantic camera shows objects colored according to the object tag. Tag Color Hex Value Car Blue #120E97 Road Purple #7A3F83 Sidewalk Orange #BA8350 Vegetation Green #71C02F Obstacle White #FFFFFF TrafficLight Yellow #FFFF00 Building Turquoise #238688 Sign Dark Yellow #C0C000 Shoulder Pink #FF00FF Pedestrian Red #FF0000 Curb Dark Purple #4A254F","title":"Semantic Camera"},{"location":"sensor-visualizers/#2d-ground-truth","text":"Visualized 2D Ground Truth shows the same things as a color camera except pedestrians are enclosed in a yellow wire box and NPCs are enclosed in a green wire box.","title":"2D Ground Truth"},{"location":"sensor-visualizers/#lidar","text":"Visualized Lidar shows the point cloud that is detected.","title":"Lidar"},{"location":"sensor-visualizers/#radar","text":"Visualized Radar shows the radar cones and creates wireframe boxes enclosing NPCs ni a green box, bicycles in a cyan box, and other EGOs in a magenta box.","title":"Radar"},{"location":"sensor-visualizers/#3d-ground-truth","text":"Visualized 3D Ground Truth creates wireframe boxes enclosing pedestrians in a yellow box and NPCs in a green box.","title":"3D Ground Truth"},{"location":"sensors-distribution/","text":"Sensors Distribution Distribution sensors between different machines are the advantage of the cluster simulation. Sensors are not synchronized between simulation, one sensor can be simulated only on one machine, but one machine can still simulate multiple sensors. Sensor Setup top # Sensors distributed to the clients will not be simulated on the master, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the master) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the DistributionType property with LowLoad , HighLoad , UltraHighLoad value according to its performance overhead. Sensors Load Balancing top # The current load balancing algorithm divides sensors into groups by their type. Master distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine. UltraHighLoad sensors are assigned first and adds 1.0 overhead to a machine. Sensors like LIDAR , which parses the camera images and performs complex maths operations, should be classified under this type. HighLoad sensors are assigned next and each sensor adds 0.1 overhead to a machine. Sensors like RadarSensor , which performs complex maths operations, should be classified under this type. LowLoad sensors are assigned last and each sensor adds 0.05 overhead to a machine. Sensors like GpsSensor , which performs simple maths operations, should be classified under this type. DoNotDistribute sensors are assigned only to the master machine. Sensors like VehicleControlSensor , which controls objects in a simulation, have to be classified under this type. As the master machine requires more resources UltraHighLoad sensors that can be distributed will never be assigned to the master, additionally master starts the algorithm with 0.15 overhead.","title":"Sensors Distribution [](#top)"},{"location":"sensors-distribution/#sensor-setup","text":"Sensors distributed to the clients will not be simulated on the master, those sensors will not affect the simulation (for example manual control sensor has to be simulated on the master) and API requests callbacks will be delayed. Due to these restrictions distribution is disabled by default and enabling it requires additional setup. Every sensor which can be distributed to clients has to override the DistributionType property with LowLoad , HighLoad , UltraHighLoad value according to its performance overhead.","title":"Sensor Setup"},{"location":"sensors-distribution/#sensors-load-balancing","text":"The current load balancing algorithm divides sensors into groups by their type. Master distributes sensors by counting overhead sum and assigning each next sensor to the least overloaded machine. UltraHighLoad sensors are assigned first and adds 1.0 overhead to a machine. Sensors like LIDAR , which parses the camera images and performs complex maths operations, should be classified under this type. HighLoad sensors are assigned next and each sensor adds 0.1 overhead to a machine. Sensors like RadarSensor , which performs complex maths operations, should be classified under this type. LowLoad sensors are assigned last and each sensor adds 0.05 overhead to a machine. Sensors like GpsSensor , which performs simple maths operations, should be classified under this type. DoNotDistribute sensors are assigned only to the master machine. Sensors like VehicleControlSensor , which controls objects in a simulation, have to be classified under this type. As the master machine requires more resources UltraHighLoad sensors that can be distributed will never be assigned to the master, additionally master starts the algorithm with 0.15 overhead.","title":"Sensors Load Balancing"},{"location":"simulation-menu/","text":"Simulation Menu When in a non-Headless Simulation, a menu can be accessed by clicking on the \"hamburger\" menu icon in the bottom left. In an Interactive Simulation, the \"Play\" and \"Pause\" buttons are found to the right of the menu icon. Table of Contents Info Menu Controls Menu Interactive Menu Sensors Menu Bridge Menu Camera Button Vehicle Selection Info Menu top # This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu. Controls Menu top # This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. See Keyboard Shortcuts for more details. Interactive Menu top # This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the Simulation while the Simulation is playing. Sensors Menu top # This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized. See Sensor Visualization for more details. Bridge Menu top # This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details. Camera Button top # The camera icon in the bottom right indicates if the view is currently a follow camera or free-roam camera. A follow camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. A free-roam camera can be moved freely around the map using all of the camera controls. Clicking the camera button toggles between the 2 camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle. Vehicle Selection top # The vehicle listed in the bottom right is the current active vehicle. This vehicle is affected by keyboard input. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Menu items"},{"location":"simulation-menu/#info-menu","text":"This menu is accessed from the i button. It lists the build info as well as any errors, logs, or warnings that are created in the current simulation. To clear all of these messages, click the \"Trash can\" icon in the bottom right of the menu.","title":"Info Menu"},{"location":"simulation-menu/#controls-menu","text":"This menu is accessed from the \"controller\" button. It lists all the keyboard commands in the simulation. See Keyboard Shortcuts for more details.","title":"Controls Menu"},{"location":"simulation-menu/#interactive-menu","text":"This menu is only available in an Interactive Simulation and is accessed from the \"sliders\" button. It contains tools to change the environment of the Simulation while the Simulation is playing.","title":"Interactive Menu"},{"location":"simulation-menu/#sensors-menu","text":"This menu is accessed from the \"eye\" button. It lists all sensors on the selected vehicle and allows for the sensors to be visualized. See Sensor Visualization for more details.","title":"Sensors Menu"},{"location":"simulation-menu/#bridge-menu","text":"This menu is accessed from the \"plug\" button. It lists information on the bridge status as well as all published and subscribed topics. See Bridge Topics for more details.","title":"Bridge Menu"},{"location":"simulation-menu/#camera-button","text":"The camera icon in the bottom right indicates if the view is currently a follow camera or free-roam camera. A follow camera remains centered on the selected vehicle. W and S zoom and Mouse RightClick rotates the view around the center of the car. If the view has not been manually rotated, the camera will stay behind the vehicle. A free-roam camera can be moved freely around the map using all of the camera controls. Clicking the camera button toggles between the 2 camera modes. When switching back to the follow camera, the camera will automatically be positioned behind the active vehicle.","title":"Camera Button"},{"location":"simulation-menu/#vehicle-selection","text":"The vehicle listed in the bottom right is the current active vehicle. This vehicle is affected by keyboard input. Selecting a different vehicle will change the view to the follow camera of the selected vehicle. The number preceding each vehicle corresponds to the number key to select the vehicle.","title":"Vehicle Selection"},{"location":"simulations-tab/","text":"Web UI Simulations Tab Explanation A Simulation can be in the following states. A Simulation will have a Valid status if it can be run A Simulation can become Invalid for several reasons: A Map or Vehicle has become Invalid since the Simulation was created A Vehicle with a bridge is missing a Bridge Connection String How to Add/Edit a Simulation # Click the Add new button or the pencil icon The dialogue that opens has 4 tabs which change the parameters of the Simulation: General Simulation Name : The name of the Simulation Select Cluster : From the dropdown, select the cluster of computers that will run the Simulation API Only : Check this if the Simulation will be controlled through the Python API. Checking this will disable most other options as they will be set through the API Headless Mode : Check this if it is not necessary to render the Simulator in the main window. Checking this will improve performance. Map & Vehicles Interactive Mode : Check this to enable Simulation controls Select Map : From the dropdown, choose the map that will be used Select Vehicle : From the dropdown, choose the vehicle that will be spawned Bridge Connection String : If the chosen vehicle has a Bridge Type, an IP:port must be provided to the bridge host + : Adds an additional vehicle. Vehicles will spawn in Spawn Info positions of the map in order Traffic Use Predefined Seed : Check this and enter a seed [int] which will be used deterministically control NPCs Enable NPC : Check this to have NPC vehicles spawn at the beginning of the Simulation Enable Pedestrians : Check this to have Pedestrians spawn at the beginning of the Simulation Weather Time of Day : Set the time of day for the Simulation Rain : [0-1] set how much rain should fall Wetness : [0-1] set how wet the roads should be Fog : [0-1] set thick fog there should be Cloudiness : [0-1] set how much cloud cover thee should be","title":"Simulations"},{"location":"simulations-tab/#how-to-addedit-a-simulation","text":"Click the Add new button or the pencil icon The dialogue that opens has 4 tabs which change the parameters of the Simulation: General Simulation Name : The name of the Simulation Select Cluster : From the dropdown, select the cluster of computers that will run the Simulation API Only : Check this if the Simulation will be controlled through the Python API. Checking this will disable most other options as they will be set through the API Headless Mode : Check this if it is not necessary to render the Simulator in the main window. Checking this will improve performance. Map & Vehicles Interactive Mode : Check this to enable Simulation controls Select Map : From the dropdown, choose the map that will be used Select Vehicle : From the dropdown, choose the vehicle that will be spawned Bridge Connection String : If the chosen vehicle has a Bridge Type, an IP:port must be provided to the bridge host + : Adds an additional vehicle. Vehicles will spawn in Spawn Info positions of the map in order Traffic Use Predefined Seed : Check this and enter a seed [int] which will be used deterministically control NPCs Enable NPC : Check this to have NPC vehicles spawn at the beginning of the Simulation Enable Pedestrians : Check this to have Pedestrians spawn at the beginning of the Simulation Weather Time of Day : Set the time of day for the Simulation Rain : [0-1] set how much rain should fall Wetness : [0-1] set how wet the roads should be Fog : [0-1] set thick fog there should be Cloudiness : [0-1] set how much cloud cover thee should be","title":"How to Add/Edit a Simulation"},{"location":"total-control-calibration-criteria/","text":"Total Control Calibration Criteria back This page has control calibration criteria JSON. { \"type\": \"Control Calibration\", \"name\": \"Control Calibration\", \"params\": { \"states\": [{ \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 24, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 20, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 0, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 20.0, \"max_velocity\": 40.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -40, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 27, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 23, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": 70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 20, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 28, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 32, \"brakes\": 0, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 18, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 23, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 10.0, \"max_velocity\": 20.0, \"throttle\": 0, \"brakes\": 28, \"steering\": -70, \"gear\": \"forward\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 0, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 18, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -10, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -20, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -45, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -50, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 19, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": 70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 15.0, \"brakes\": 14.0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 19, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 22, \"brakes\": 0, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 15, \"steering\": -65, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 10.0, \"throttle\": 0, \"brakes\": 22, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 }, { \"min_velocity\": 0.2, \"max_velocity\": 2.0, \"throttle\": 0, \"brakes\": 27, \"steering\": -70, \"gear\": \"reverse\", \"duration\": 4 } ] } }","title":"Total Control Calibration Criteria"},{"location":"vehicles-tab/","text":"Web UI Vehicles Tab Explanation A Vehicle can be in the following states. A Vehicle with a local URL or if it has already been downloaded will have a Valid status. If the URL to the Vehicle assetbundle is not local and the assetbundle is not in the local database, then the assetbundle needs to be downloaded. Currently only 1 assetbundle is downloaded at a time. If an assetbundle is downloading, the Vehicle will show a GREY dot and the status will be Downloading with the download percentage. If another assetbundle is downloading, the icon will be ORANGE and the status will be Downloading without a percentage. A downloading Vehicle can be interrupted by pressing the stop button. If the Vehicle is not usable in a Simulation it will have an Invalid status. This can be because the local assetbundle is not usable or the download was interrupted. Where to find Vehicles top # Vehicle assetbundles are available from our content website . When adding a vehicle, the link to the appropriate assetbundle can be entered as the URL or the assetbundle can be downloaded manually and the local path can be entered. The calibration files for the vehicles are available in the same page. Please see the relevant doc for instructions on how to add a vehicle to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware Example JSON configurations can be found on these pages: Apollo 5.0 JSON Apollo 3.0 JSON Autoware JSON How to add a Vehicle top # Click the Add new button In the dialogue that opens, enter the name of the vehicle and the URL to the assetbundle. This can be a URL to a location in the cloud (e.g. the link to the assetbundle on the content website ) or to a location on a local drive (the absolute path to the vehicle_XXX file). If the URL is not local, the assetbundle will be downloaded to the local database. How to Edit a Vehicle top # Click the pencil icon In the dialogue that opens, the name of the vehicle can be changed and the URL to the assetbundle. If the URL is changed, the assetbundle in the database will be updated (downloaded if necessary) How to Change the Configuration of a Vehicle top # Click the wrench icon In the dialogue that opens, the bridge type of the vehicle and the JSON configuration of the vehicle can be entered A JSON beautifier is recommended to make the configuration more readable The bridge type determines how the sensor data will be formatted and sent to an AD stack. All bridge types other than No bridge will require a Bridge Connection String when adding a vehicle to a simulation. This string includes the IP of the AD Stack and the open port (ex. 192.168.1.100:9090 ) The JSON determines what sensors are on the vehicle, where they are located, what topic they will publish data under, and what control inputs the vehicle accepts See below for an example JSON configuration See Sensor Parameters for full defintions of all availble sensors and how to add them to a vehicle. Bridge Types top # No bridge : This is bridge available by default. Does not require any additional information while setting up Simulation. Used when there is no need to connect to an AD Stack. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). ROS1 Bridge requires IP address and port number while setting up Simulation Configuration. ROS Apollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires IP address and port number while setting up Simulation Configuration. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires IP address, port number while setting up Simulation Configuration. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge requires IP address, port number while setting up Simulation Configuration. Example JSON top # This is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic, a LIDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic, a Manual Control input which allows the keyboard input to control the car, and a Vehicle Control input which subscribes to the Autoware AD Stack control commands. [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Vehicles"},{"location":"vehicles-tab/#where-to-find-vehicles","text":"Vehicle assetbundles are available from our content website . When adding a vehicle, the link to the appropriate assetbundle can be entered as the URL or the assetbundle can be downloaded manually and the local path can be entered. The calibration files for the vehicles are available in the same page. Please see the relevant doc for instructions on how to add a vehicle to an AD Stack: Apollo 5.0 Apollo 3.0 Autoware Example JSON configurations can be found on these pages: Apollo 5.0 JSON Apollo 3.0 JSON Autoware JSON","title":"Where to find Vehicles"},{"location":"vehicles-tab/#how-to-add-a-vehicle","text":"Click the Add new button In the dialogue that opens, enter the name of the vehicle and the URL to the assetbundle. This can be a URL to a location in the cloud (e.g. the link to the assetbundle on the content website ) or to a location on a local drive (the absolute path to the vehicle_XXX file). If the URL is not local, the assetbundle will be downloaded to the local database.","title":"How to add a Vehicle"},{"location":"vehicles-tab/#how-to-edit-a-vehicle","text":"Click the pencil icon In the dialogue that opens, the name of the vehicle can be changed and the URL to the assetbundle. If the URL is changed, the assetbundle in the database will be updated (downloaded if necessary)","title":"How to Edit a Vehicle"},{"location":"vehicles-tab/#how-to-change-the-configuration-of-a-vehicle","text":"Click the wrench icon In the dialogue that opens, the bridge type of the vehicle and the JSON configuration of the vehicle can be entered A JSON beautifier is recommended to make the configuration more readable The bridge type determines how the sensor data will be formatted and sent to an AD stack. All bridge types other than No bridge will require a Bridge Connection String when adding a vehicle to a simulation. This string includes the IP of the AD Stack and the open port (ex. 192.168.1.100:9090 ) The JSON determines what sensors are on the vehicle, where they are located, what topic they will publish data under, and what control inputs the vehicle accepts See below for an example JSON configuration See Sensor Parameters for full defintions of all availble sensors and how to add them to a vehicle.","title":"How to Change the Configuration of a Vehicle"},{"location":"vehicles-tab/#bridge-types","text":"No bridge : This is bridge available by default. Does not require any additional information while setting up Simulation. Used when there is no need to connect to an AD Stack. ROS : This bridge allows connecting to ROS1 based AV stacks. (like Autoware). ROS1 Bridge requires IP address and port number while setting up Simulation Configuration. ROS Apollo : This bridge allows connecting to ROS1 based AV stacks which requires protobuf message format. (like Apollo 3.0). ROS1 Apollo Bridge requires IP address and port number while setting up Simulation Configuration. ROS2 : This bridge allows connecting to ROS2 based AV stacks. ROS2 Bridge requires IP address, port number while setting up Simulation Configuration. CyberRT : This bridge allows connections to Apollo 5.0. CyberRT Bridge requires IP address, port number while setting up Simulation Configuration.","title":"Bridge Types"},{"location":"vehicles-tab/#example-json","text":"This is a shortened version of the JSON configuration on the Jaguar2015XE (Autoware) default vehicle. It uses a ROS bridge type. The JSON includes a GPS sensor in the center of the vehicle that publishes data on the \"/nmea_sentence\" topic, a LIDAR sensor 2.312m above the center of the vehicle that publishes data on the \"/points_raw\" topic, a Manual Control input which allows the keyboard input to control the car, and a Vehicle Control input which subscribes to the Autoware AD Stack control commands. [ { \"type\": \"GPS Device\", \"name\": \"GPS\", \"params\": { \"Frequency\": 12.5, \"Topic\": \"/nmea_sentence\", \"Frame\": \"gps\", \"IgnoreMapOrigin\": true }, \"transform\": { \"x\": 0, \"y\": 0, \"z\": 0, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Lidar\", \"name\": \"Lidar\", \"params\": { \"LaserCount\": 32, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 360, \"FieldOfView\": 41.33, \"CenterAngle\": 10, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/points_raw\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } }, { \"type\": \"Manual Control\", \"name\": \"Manual Car Control\" }, { \"type\": \"Vehicle Control\", \"name\": \"Autoware Car Control\", \"params\": { \"Topic\": \"/vehicle_cmd\" } } ]","title":"Example JSON"},{"location":"velodyne-json-examples/","text":"Velodyne VLP-16 # [ { \"type\": \"Lidar\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLP-32C # [ { \"type\": \"Lidar\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ] Velodyne VLS-128 # [ { \"type\": \"Lidar\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne json examples"},{"location":"velodyne-json-examples/#velodyne-vlp-16","text":"[ { \"type\": \"Lidar\", \"name\": \"Velodyne VLP-16\", \"params\": { \"LaserCount\": 16, \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"FieldOfView\": 30, \"CenterAngle\": 0, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-16"},{"location":"velodyne-json-examples/#velodyne-vlp-32c","text":"[ { \"type\": \"Lidar\", \"name\": \"Velodyne VLP-32C\", \"params\": { \"VerticalRayAngles\": [ -25.0, -1.0, -1.667, -15.639, -11.31, 0.0, -0.667, -8.843, -7.254, 0.333, -0.333, -6.148, -5.333, 1.333, 0.667, -4.0, -4.667, 1.667, 1.0, -3.667, -3.333, 3.333, 2.333, -2.667, -3.0, 7.0, 4.667, -2.333, -2.0, 15.0, 10.333, -1.333 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLP-32C"},{"location":"velodyne-json-examples/#velodyne-vls-128","text":"[ { \"type\": \"Lidar\", \"name\": \"Velodyne VLS-128\", \"params\": { \"VerticalRayAngles\": [ -11.742, -1.99, 3.4, -5.29, -0.78, 4.61, -4.08, 1.31, -6.5, -1.11, 4.28, -4.41, 0.1, 6.48, -3.2, 2.19, -3.86, 1.53, -9.244, -1.77, 2.74, -5.95, -0.56, 4.83, -2.98, 2.41, -6.28, -0.89, 3.62, -5.07, 0.32, 7.58, -0.34, 5.18, -3.64, 1.75, -25, -2.43, 2.96, -5.73, 0.54, 9.7, -2.76, 2.63, -7.65, -1.55, 3.84, -4.85, 3.188, -5.51, -0.12, 5.73, -4.3, 1.09, -16.042, -2.21, 4.06, -4.63, 0.76, 15, -3.42, 1.97, -6.85, -1.33, -5.62, -0.23, 5.43, -3.53, 0.98, -19.582, -2.32, 3.07, -4.74, 0.65, 11.75, -2.65, 1.86, -7.15, -1.44, 3.95, -2.1, 3.29, -5.4, -0.01, 4.5, -4.19, 1.2, -13.565, -1.22, 4.17, -4.52, 0.87, 6.08, -3.31, 2.08, -6.65, 1.42, -10.346, -1.88, 3.51, -6.06, -0.67, 4.72, -3.97, 2.3, -6.39, -1, 4.39, -5.18, 0.21, 6.98, -3.09, 4.98, -3.75, 1.64, -8.352, -2.54, 2.85, -5.84, -0.45, 8.43, -2.87, 2.52, -6.17, -1.66, 3.73, -4.96, 0.43 ], \"MinDistance\": 0.5, \"MaxDistance\": 100, \"RotationFrequency\": 10, \"MeasurementsPerRotation\": 1800, \"Compensated\": true, \"PointColor\": \"#ff000000\", \"Topic\": \"/point_cloud\", \"Frame\": \"velodyne\" }, \"transform\": { \"x\": 0, \"y\": 2.312, \"z\": -0.3679201, \"pitch\": 0, \"yaw\": 0, \"roll\": 0 } } ]","title":"Velodyne VLS-128"}]}